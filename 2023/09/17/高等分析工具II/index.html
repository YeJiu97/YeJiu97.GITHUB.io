<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="高等数据分析工具II的笔记汇总">
<meta property="og:type" content="article">
<meta property="og:title" content="高等分析工具II">
<meta property="og:url" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/index.html">
<meta property="og:site_name" content="夜久">
<meta property="og:description" content="高等数据分析工具II的笔记汇总">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724183847025.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184037396.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184156337.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184208201.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113221770-1691217347164-27.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113506205-1691217347165-28.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113607209-1691217347165-29.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731114632947-1691217347165-30.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731114743209-1691217347165-31.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201049365-1691217347165-32.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201253316-1691217347166-33.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731192744584-1691217347166-34.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731194218933-1691217347166-35.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201311687-1691217347166-36.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201700558-1691217347166-37.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731161357855.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731161434097.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162014045.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162059203.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162230334.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162242550.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162322829.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162336316.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162358384.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162854763.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731163520091.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731163812853.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731164221343.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731164239239.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918181604329.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918181938152.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918182117265.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918183937958.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918183819625.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918200814802.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918220300222.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222410936.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222439230.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222721893.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918223847500.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918223954876.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918230719090.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918232441979.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918232540916.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230919000312509.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230919235412150.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920000518616.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920001511743.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920000941381.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002446331.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002649499.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002703834.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002713916.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002727198.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920003453577.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920005430457.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920010614362.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920010656984.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185854370.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185904606.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185918918.png">
<meta property="og:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185933798.png">
<meta property="article:published_time" content="2023-09-17T09:26:07.000Z">
<meta property="article:modified_time" content="2023-09-20T09:29:41.203Z">
<meta property="article:author" content="Ye Jiu">
<meta property="article:tag" content="数据科学">
<meta property="article:tag" content="分析工具">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724183847025.png">


<link rel="canonical" href="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/","path":"2023/09/17/高等分析工具II/","title":"高等分析工具II"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>高等分析工具II | 夜久</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">夜久</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/848368468&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/terence-vassallo-415912750" title="Ocelotter" target="_blank" style="color: #cccccc; text-decoration: none;">Ocelotter</a> · <a href="https://soundcloud.com/terence-vassallo-415912750/euphoria" title="Euphoria - 楽園の扉 中日字幕" target="_blank" style="color: #cccccc; text-decoration: none;">Euphoria - 楽園の扉 中日字幕</a></div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535990&auto=1&height=66"></iframe>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#01-%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="nav-number">1.</span> <span class="nav-text">01-关于大数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Big-Data"><span class="nav-number">1.1.</span> <span class="nav-text">Big Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Science"><span class="nav-number">1.2.</span> <span class="nav-text">Data Science</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Applications"><span class="nav-number">1.3.</span> <span class="nav-text">Applications</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#02-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">2.</span> <span class="nav-text">02-朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">2.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80"><span class="nav-number">2.2.</span> <span class="nav-text">概率基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">2.2.1.</span> <span class="nav-text">基础概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-number">2.2.2.</span> <span class="nav-text">贝叶斯定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">2.3.</span> <span class="nav-text">概率分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E7%B1%BB%E5%88%AB"><span class="nav-number">2.3.1.</span> <span class="nav-text">两种类别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MAP%EF%BC%88Maximum-A-Posteriori%EF%BC%89%E5%88%86%E7%B1%BB%E8%A7%84%E5%88%99"><span class="nav-number">2.3.2.</span> <span class="nav-text">MAP（Maximum A Posteriori）分类规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Naive-Bayes"><span class="nav-number">2.3.3.</span> <span class="nav-text">朴素贝叶斯 Naive Bayes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.3.4.</span> <span class="nav-text">朴素贝叶斯的代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">2.3.5.</span> <span class="nav-text">优缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Practical-Weka"><span class="nav-number">2.4.</span> <span class="nav-text">Practical - Weka</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFweka"><span class="nav-number">2.4.1.</span> <span class="nav-text">什么是weka</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%AE%89%E8%A3%85weka"><span class="nav-number">2.4.2.</span> <span class="nav-text">下载和安装weka</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AAWeka%E7%A8%8B%E5%BA%8F"><span class="nav-number">2.4.3.</span> <span class="nav-text">第一个Weka程序</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#03-%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">03-贝叶斯网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">3.1.</span> <span class="nav-text">贝叶斯网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">3.1.1.</span> <span class="nav-text">什么是贝叶斯网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E5%88%97%E5%AD%90%E2%91%A0"><span class="nav-number">3.1.2.</span> <span class="nav-text">贝叶斯网络列子①</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%BE%8B%E5%AD%90%E2%91%A1"><span class="nav-number">3.1.3.</span> <span class="nav-text">贝叶斯网络例子②</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">3.1.4.</span> <span class="nav-text">条件独立性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%9D%A1%E4%BB%B6"><span class="nav-number">3.1.5.</span> <span class="nav-text">马尔科夫条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">3.1.6.</span> <span class="nav-text">联合概率分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E6%8E%A8%E7%90%86"><span class="nav-number">3.1.7.</span> <span class="nav-text">贝叶斯网络推理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Practical-%E8%B4%9D%E5%8F%B6%E6%96%AF-gRain"><span class="nav-number">3.2.</span> <span class="nav-text">Practical - 贝叶斯 gRain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%85%E5%A4%87%E7%9A%84package%E5%92%8C%E4%BE%9D%E8%B5%96"><span class="nav-number">3.2.1.</span> <span class="nav-text">必备的package和依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%A7%8B%E8%BF%9B%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-number">3.2.2.</span> <span class="nav-text">开始进行计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.3.</span> <span class="nav-text">贝叶斯网络学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%83%BD%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E5%85%B3%E7%B3%BB"><span class="nav-number">3.3.1.</span> <span class="nav-text">可能的贝叶斯网络关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E8%AF%84%E5%88%86"><span class="nav-number">3.3.2.</span> <span class="nav-text">贝叶斯网络评分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-number">3.3.3.</span> <span class="nav-text">两种方法的比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-number">3.3.4.</span> <span class="nav-text">相关性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PC%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.5.</span> <span class="nav-text">PC算法</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Jiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Jiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夜久">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="高等分析工具II | 夜久">
      <meta itemprop="description" content="高等数据分析工具II的笔记汇总">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          高等分析工具II
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-17 18:56:07" itemprop="dateCreated datePublished" datetime="2023-09-17T18:56:07+09:30">2023-09-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-20 18:59:41" itemprop="dateModified" datetime="2023-09-20T18:59:41+09:30">2023-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">数据科学</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">高等数据分析工具II的笔记汇总</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="01-关于大数据"><a href="#01-关于大数据" class="headerlink" title="01-关于大数据"></a>01-关于大数据</h1><h2 id="Big-Data"><a href="#Big-Data" class="headerlink" title="Big Data"></a>Big Data</h2><p>Why Big Data:</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724183847025.png" alt="image-20230724183847025"></p>
<p>What is Big Data:</p>
<blockquote>
<p>•‘Big Data’ is similar to ‘small data’, but <strong>bigger</strong> in size</p>
</blockquote>
<p>3 Vs:</p>
<ol>
<li>Volume–challenging to load and process (how to index, retrieve)</li>
<li>Variety–different data types and degree of structure (how to query semi-structured data)</li>
<li>Velocity–real-time processing influenced by rate of data arrival</li>
</ol>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184037396.png" alt="image-20230724184037396"></p>
<p>10 Vs:</p>
<ol>
<li>体积：吨字节</li>
<li>多样性：维度</li>
<li>速度：高数据速率</li>
<li>真实性：必要且充分的数据</li>
<li>有效性：数据质量</li>
<li>价值：商业价值、投资回报率</li>
<li>可变性：动态</li>
<li>地点：地点</li>
<li>词汇：语义</li>
<li>模糊性：混乱</li>
</ol>
<h2 id="Data-Science"><a href="#Data-Science" class="headerlink" title="Data Science"></a>Data Science</h2><p>Data Science Skills:</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184156337.png" alt="image-20230724184156337"></p>
<p>Data Scientist:</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230724184208201.png" alt="image-20230724184208201"></p>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p><strong>behaviour Analytics</strong></p>
<p>Behavioral Analytics: Definition: Behavioral analytics is the process of collecting, measuring, and analyzing user behavior data to gain insights into how individuals interact with digital platforms, products, or services. It helps organizations understand user preferences, patterns, and actions, which can be used to optimize user experience and improve business outcomes.</p>
<p>Example: An e-commerce website tracks user behavior, such as pages visited, products viewed, time spent on each page, and items added to the cart. By analyzing this data, the website can identify popular products, detect drop-off points in the user journey, and personalize recommendations to improve conversions and overall user satisfaction.</p>
<p><strong>Optimize Funnel Conversation</strong></p>
<p>Optimize Funnel Conversion: Definition: Funnel conversion optimization involves analyzing and enhancing the conversion rates at different stages of a sales or marketing funnel. The goal is to identify and remove any barriers or inefficiencies in the user journey to increase the percentage of users who complete desired actions, such as making a purchase or signing up for a service.</p>
<p>Example: A software company examines the conversion rates of its sign-up process for a free trial of their product. By using A&#x2F;B testing, they discover that simplifying the registration form and providing a clearer call-to-action significantly increases the number of trial users who convert to paying customers.</p>
<p><strong>Customer Segmentation</strong></p>
<p>Customer Segmentation: Definition: Customer segmentation is the practice of dividing a customer base into distinct groups based on specific characteristics or behaviors. This allows businesses to tailor their marketing, communication, and product offerings to better meet the needs and preferences of each segment.</p>
<p>Example: A fitness app segments its users based on their fitness goals, such as weight loss, muscle gain, or general fitness. The app then sends targeted content and workout plans to each segment, increasing user engagement and retention.</p>
<p><strong>Predictive Support</strong></p>
<p>Predictive Support: Definition: Predictive support, also known as predictive customer service, involves using data analysis and machine learning algorithms to anticipate customer needs and proactively address potential issues before they occur. This approach aims to enhance customer satisfaction and reduce support costs.</p>
<p>Example: An internet service provider monitors network performance data and user behavior to predict potential connectivity problems for specific users. When the system detects a likely issue, it automatically sends a notification to the customer and initiates troubleshooting steps, preventing a full service disruption and reducing the number of customer support calls.</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p><strong>行为分析</strong></p>
<p>行为分析：定义：行为分析是收集、测量和分析用户行为数据的过程，以获得有关个人如何与数字平台、产品或服务进行交互的见解。它有助于组织了解用户的偏好、模式和行动，这可以用于优化用户体验和改善业务结果。</p>
<p>例如：电子商务网站跟踪用户行为，如访问的页面、查看的产品、在每个页面上花费的时间和添加到购物车的物品。通过分析这些数据，网站可以确定热门产品，在用户旅程中检测到流失点，并个性化推荐以提高转化率和整体用户满意度。</p>
<p><strong>优化漏斗转化率</strong></p>
<p>优化漏斗转化率：定义：漏斗转化率优化涉及分析和增强销售或营销漏斗不同阶段的转化率。目标是识别和消除用户旅程中的任何障碍或低效率，以增加完成所需操作（例如购买或注册服务）的用户百分比。</p>
<p>例如：一家软件公司检查其免费试用产品的注册流程的转化率。通过使用A&#x2F;B测试，他们发现简化注册表格并提供更清晰的呼吁策略显著增加了转化为付费客户的试用用户数量。</p>
<p><strong>客户细分</strong></p>
<p>客户细分：定义：客户细分是将客户群体根据特定特征或行为分成不同的群体的做法。这使企业能够根据每个细分的需求和偏好来定制其营销、沟通和产品提供。</p>
<p>例如：健身应用程序根据其用户的健身目标（如减肥、增肌或普通健身）对其进行细分。然后，该应用程序向每个细分发送定向内容和训练计划，提高用户参与度和保留度。</p>
<p><strong>预测性支持</strong></p>
<p>预测性支持：定义：预测性支持，也称为预测性客户服务，涉及使用数据分析和机器学习算法来预测客户需求并在可能出现问题之前主动解决。这种方法旨在增强客户满意度并降低支持成本。</p>
<p>例如：一家互联网服务提供商监控网络性能数据和用户行为，以预测特定用户的可能连接问题。当系统检测到可能存在问题时，它会自动向客户发送通知并启动故障排除步骤，防止全面服务中断并减少客户支持电话的数量。</p>
<h1 id="02-朴素贝叶斯"><a href="#02-朴素贝叶斯" class="headerlink" title="02-朴素贝叶斯"></a>02-朴素贝叶斯</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>三种分类方法：</p>
<ol>
<li>直接建模分类规则，例子：k-NN，决策树，感知器，支持向量机（SVM）</li>
<li>建模给定输入数据的类别成员概率，例子：带有交叉熵损失的感知机</li>
<li>对每个类别内的数据进行概率建模，例子：朴素贝叶斯，基于模型的分类器</li>
</ol>
<p>1和2是判别式分类的例子，3是生成式分类的例子，2和3都是概率分类的例子。</p>
<p><strong>判别式分类模型</strong>（discriminative classification）是直接学习输入数据与其对应的输出标签之间的映射关系，即从输入到输出的直接映射。在判别式分类中，我们关注的是找到一个决策边界，能够将不同类别的样本分开。例如，k-NN、决策树、感知机和支持向量机（SVM）都属于判别式分类算法。</p>
<p><strong>生成式分类</strong>（generative classification）也可以建模类别成员概率，但是它更关注对每个样本的分类结果，即给定输入数据，输出其属于各个类别的概率。这样，我们可以在分类问题中获得更多信息，并进行更精细的分类决策。例如，感知机使用交叉熵损失来优化分类结果，从而间接地对类别成员概率进行建模。</p>
<p><strong>生成式分类</strong>（probabilistic classification）是一种通过建模每个类别内数据的分布来进行分类的方法。它考虑了样本生成的过程，即给定输入数据，估计其来自每个类别的概率，并基于概率来进行分类决策。朴素贝叶斯和其他基于模型的分类器属于生成式分类算法。生成式分类可以用于生成新的样本，因为它对数据分布有较好的建模。</p>
<h2 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h2><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><p><strong>先验概率 (Prior Probability)</strong>:</p>
<p>先验概率是指在考虑任何新的证据或信息之前，根据以往的经验或已有的知识，对一个事件或假设的概率进行的估计。</p>
<p>公式：$$ P(A) $$</p>
<p>解释：$$ P(A) $$ 是指事件 A 发生的概率，它是在考虑任何新的信息之前，基于先前观察或经验得出的概率。</p>
<p><strong>条件概率 (Conditional Probability)</strong>:</p>
<p>条件概率是指在给定其他相关事件发生的情况下，某个事件发生的概率。在条件概率中，我们考虑了某些已知条件对事件发生概率的影响。</p>
<p>公式：$$ P(A|B) &#x3D; \frac{P(A \cap B)}{P(B)} $$</p>
<p>解释：$$ P(A|B) $$ 表示在事件 B 已经发生的条件下，事件 A 发生的概率。条件概率是通过事件 A 与事件 B 共同发生的概率除以事件 B 发生的概率得到的。</p>
<p><strong>联合概率 (Joint Probability)</strong>:</p>
<p>联合概率是指两个或多个事件同时发生的概率，它表示了这些事件共同发生的可能性。</p>
<p>公式：$$ P(A \cap B) $$</p>
<p>解释：$$ P(A \cap B) $$ 表示事件 A 和事件 B 同时发生的概率，即两个事件共同发生的可能性。</p>
<p><strong>关系: 独立性 (Independence)</strong>:</p>
<p>两个事件 A 和 B 是相互独立的，当且仅当它们的联合概率等于它们各自的概率的乘积。</p>
<p>公式：$$ P(A \cap B) &#x3D; P(A) \cdot P(B) $$</p>
<p>解释：当事件 A 和事件 B 是相互独立的时候，它们的联合概率等于事件 A 发生的概率乘以事件 B 发生的概率。换句话说，在独立事件中，一个事件的发生不会影响另一个事件的发生。</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p><strong>贝叶斯定理（Bayes Rule）</strong>，也称为贝叶斯公式，是概率论中一种用于计算条件概率的重要规则。它基于条件概率的定义，通过已知的先验概率和新的证据或观测来更新事件的后验概率。</p>
<p><strong>定义</strong>：</p>
<p>给定两个事件 A 和 B，贝叶斯定理描述了在已知事件 B 发生的条件下，事件 A 发生的概率。</p>
<p><strong>公式</strong>：</p>
<p>贝叶斯定理的公式如下：</p>
<p>$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>
<p>其中，</p>
<ul>
<li>$P(A|B)$ 表示在事件 B 已经发生的条件下，事件 A 发生的概率，称为后验概率（Posterior Probability）。</li>
<li>$P(B|A)$ 表示在事件 A 已经发生的条件下，事件 B 发生的概率，称为条件概率（Conditional Probability）。</li>
<li>$P(A)$ 和 $P(B)$ 分别是事件 A 和事件 B 的先验概率（Prior Probability）。</li>
</ul>
<p><strong>推算过程</strong>：</p>
<p>要计算贝叶斯定理中的后验概率，可以按照以下步骤进行推算：</p>
<ol>
<li><p>先计算条件概率 $P(B|A)$：这是在事件 A 已经发生的条件下，事件 B 发生的概率。根据问题的具体情况进行计算。</p>
</li>
<li><p>计算事件 A 和事件 B 的先验概率 $P(A)$ 和 $P(B)$：这些概率通常是根据以往的经验或其他信息得出的。</p>
</li>
<li><p>将上述值代入贝叶斯定理的公式：$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>
</li>
<li><p>计算得到的 $P(A|B)$ 就是在已知事件 B 发生的条件下，事件 A 发生的后验概率。</p>
</li>
</ol>
<p>贝叶斯定理在统计学、机器学习、人工智能等领域有着广泛的应用，特别是在概率推理、分类问题和信息融合等方面。通过利用贝叶斯定理，我们可以根据已有的知识和新的证据来更新对事件发生概率的估计，从而做出更准确的预测和决策。</p>
<p>图示：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113221770-1691217347164-27.png" alt="image-20230731113221770"></p>
<p><strong>补充：后验概率。</strong></p>
<p>后验概率（Posterior Probability）是概率论中的一个概念，它指的是在考虑了新的观测数据或证据之后，对一个事件发生概率进行更新得到的概率。</p>
<p>在贝叶斯统计学中，后验概率是指在给定先验概率和观测数据的条件下，对事件或假设的概率进行修正的结果。它反映了在已知一些信息后，我们对事件发生概率的最新估计。</p>
<p>公式表示为：$$ P(A|B) &#x3D; \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>
<p>其中，</p>
<ul>
<li>$P(A|B)$ 表示在事件 B 已经发生的条件下，事件 A 发生的后验概率。</li>
<li>$P(B|A)$ 是在事件 A 已经发生的条件下，事件 B 发生的条件概率。</li>
<li>$P(A)$ 和 $P(B)$ 分别是事件 A 和事件 B 的先验概率。</li>
</ul>
<p>在贝叶斯定理中，我们根据已知的先验概率 $P(A)$ 和条件概率 $P(B|A)$，以及观测数据 $B$ 来计算后验概率 $P(A|B)$。通过这个计算，我们可以根据新的观测数据更新对事件发生概率的估计，使我们的推断更加准确。</p>
<p>后验概率在许多领域都有广泛应用，特别是在统计学、机器学习、模式识别和人工智能等领域。它在决策分析、分类问题、参数估计和贝叶斯推断等方面都扮演着重要角色。</p>
<p>拓展阅读：<a target="_blank" rel="noopener" href="http://www.snailtoday.com/archives/20596">http://www.snailtoday.com/archives/20596</a></p>
<h2 id="概率分类器"><a href="#概率分类器" class="headerlink" title="概率分类器"></a>概率分类器</h2><h3 id="两种类别"><a href="#两种类别" class="headerlink" title="两种类别"></a>两种类别</h3><p><strong>判别式分类器（discriminative model）：</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113506205-1691217347165-28.png" alt="image-20230731113506205"></p>
<p><strong>生成式分类器（Generative Model）：</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731113607209-1691217347165-29.png" alt="image-20230731113607209"></p>
<h3 id="MAP（Maximum-A-Posteriori）分类规则"><a href="#MAP（Maximum-A-Posteriori）分类规则" class="headerlink" title="MAP（Maximum A Posteriori）分类规则"></a>MAP（Maximum A Posteriori）分类规则</h3><p>MAP（Maximum A Posteriori）分类规则是一种基于贝叶斯定理的分类方法，用于在给定观测数据的条件下，选择具有最大后验概率的类别。在MAP分类中，我们利用已知的类别先验概率和观测数据的条件概率，来推断出最可能的类别。</p>
<p><strong>公式</strong>：</p>
<p>假设我们有两个类别 $C_1$ 和 $C_2$，以及观测数据 $x$。MAP分类规则可以表示为：</p>
<ul>
<li>如果 $P(C_1|x) &gt; P(C_2|x)$，则将 $x$ 分类为 $C_1$。</li>
<li>如果 $P(C_2|x) &gt; P(C_1|x)$，则将 $x$ 分类为 $C_2$。</li>
</ul>
<p>其中，</p>
<ul>
<li>$P(C_i|x)$ 表示在给定观测数据 $x$ 的条件下，事件属于类别 $C_i$ 的后验概率。</li>
</ul>
<p><strong>推导</strong>：</p>
<p>根据贝叶斯定理，后验概率可以表示为：</p>
<p>$$ P(C_i|x) &#x3D; \frac{P(x|C_i) \cdot P(C_i)}{P(x)} $$</p>
<p>其中，</p>
<ul>
<li>$P(x|C_i)$ 表示在事件属于类别 $C_i$ 的条件下，观测数据 $x$ 的概率，也称为似然概率（Likelihood）。</li>
<li>$P(C_i)$ 表示类别 $C_i$ 的先验概率。</li>
<li>$P(x)$ 是观测数据 $x$ 的边缘概率。</li>
</ul>
<p>由于 $P(x)$ 对于所有类别是相同的，因此我们可以忽略它，并只考虑后验概率的相对大小。</p>
<p>那么，MAP分类规则就变成了：</p>
<ul>
<li>如果 $P(x|C_1) \cdot P(C_1) &gt; P(x|C_2) \cdot P(C_2)$，则将 $x$ 分类为 $C_1$。</li>
<li>如果 $P(x|C_2) \cdot P(C_2) &gt; P(x|C_1) \cdot P(C_1)$，则将 $x$ 分类为 $C_2$。</li>
</ul>
<p><strong>例子</strong>：</p>
<p>假设我们有两个类别 $C_1$ 和 $C_2$，以及一个二维观测数据 $x &#x3D; (x_1, x_2)$。现在我们已知类别的先验概率 $P(C_1)$ 和 $P(C_2)$，以及在各个类别下观测数据的条件概率 $P(x|C_1)$ 和 $P(x|C_2)$。我们希望根据观测数据 $x$ 来判断它属于哪个类别。</p>
<p>根据MAP分类规则，我们可以计算后验概率并比较它们的大小，从而得出分类结果。假设计算结果为 $P(C_1|x)$ 和 $P(C_2|x)$，如果 $P(C_1|x) &gt; P(C_2|x)$，则将 $x$ 分类为 $C_1$；否则，将 $x$ 分类为 $C_2$。这样就完成了对观测数据的分类。</p>
<p><strong>似然概率</strong></p>
<p>似然概率（Likelihood Probability）是概率论中的一个概念，它表示在给定模型的参数条件下，观测数据发生的概率。换句话说，似然概率是关于参数的函数，它描述了参数取值的可能性，使得观测数据出现的概率最大。</p>
<p><strong>定义</strong>：</p>
<p>假设我们有一个概率模型，其中包含参数 θ。观测数据记为 X。似然概率（likelihood probability） L(θ|X) 是在给定参数 θ 的条件下，观测数据 X 出现的概率。通常表示为：</p>
<p>$$ L(θ|X) &#x3D; P(X|θ) $$</p>
<p>其中，</p>
<ul>
<li>$ L(θ|X) $ 表示在给定参数 θ 的条件下，观测数据 X 出现的似然概率。</li>
<li>$ P(X|θ) $ 表示在参数 θ 的条件下，观测数据 X 出现的概率。</li>
</ul>
<p><strong>例子</strong>：</p>
<p>假设我们有一个硬币，并且想知道这个硬币投掷时正面朝上的概率。我们假设硬币是一个公平硬币，也就是说正反两面朝上的概率都是相等的，即 $ P(正面) &#x3D; P(反面) &#x3D; 0.5 $。</p>
<p>现在，我们进行了一系列投掷实验，结果如下：正面、反面、正面、正面、反面。</p>
<p>我们的目标是使用这些观测数据来估计硬币投掷时正面朝上的概率。</p>
<p>在这个例子中，似然概率表示给定某个正面朝上的概率（参数 θ），观测数据出现的概率。假设参数 θ 表示正面朝上的概率，那么观测数据 X 可以用二进制表示为 [1, 0, 1, 1, 0]，其中 1 表示正面，0 表示反面。</p>
<p>如果我们假设硬币投掷时正面朝上的概率是 0.5（即 θ &#x3D; 0.5），那么似然概率可以表示为：</p>
<p>$$ L(θ&#x3D;0.5|X&#x3D;[1, 0, 1, 1, 0]) &#x3D; P(X&#x3D;[1, 0, 1, 1, 0]|θ&#x3D;0.5) $$</p>
<p>在这里，观测数据 X 出现的概率是由硬币投掷模型和参数 θ&#x3D;0.5 决定的。我们可以使用二项分布来建模硬币投掷实验，并计算似然概率。根据计算结果，我们可以比较不同参数值下的似然概率，找到使观测数据出现概率最大的参数值，即得到最大似然估计。在本例中，因为硬币是公平硬币，最大似然估计为 θ&#x3D;0.5，即硬币投掷时正面朝上和反面朝上的概率均为 0.5。</p>
<p><strong>Python语言的实现</strong></p>
<p>假设我们有两个类别 C1 和 C2，以及观测数据 x，我们需要根据已知的类别先验概率 P(C1) 和 P(C2)，以及在各个类别下观测数据的条件概率 P(x|C1) 和 P(x|C2) 来进行分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">map_classification</span>(<span class="params">P_C1, P_C2, P_x_given_C1, P_x_given_C2</span>):</span><br><span class="line">    <span class="comment"># 计算后验概率 P(C1|x) 和 P(C2|x)</span></span><br><span class="line">    P_C1_given_x = (P_x_given_C1 * P_C1) / (P_x_given_C1 * P_C1 + P_x_given_C2 * P_C2)</span><br><span class="line">    P_C2_given_x = (P_x_given_C2 * P_C2) / (P_x_given_C1 * P_C1 + P_x_given_C2 * P_C2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据后验概率进行分类</span></span><br><span class="line">    <span class="keyword">if</span> P_C1_given_x &gt; P_C2_given_x:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;C1&quot;</span>  <span class="comment"># 将 x 分类为 C1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;C2&quot;</span>  <span class="comment"># 将 x 分类为 C2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">P_C1 = <span class="number">0.4</span>  <span class="comment"># 类别 C1 的先验概率</span></span><br><span class="line">P_C2 = <span class="number">0.6</span>  <span class="comment"># 类别 C2 的先验概率</span></span><br><span class="line">P_x_given_C1 = <span class="number">0.7</span>  <span class="comment"># 在类别 C1 下观测数据 x 的条件概率</span></span><br><span class="line">P_x_given_C2 = <span class="number">0.3</span>  <span class="comment"># 在类别 C2 下观测数据 x 的条件概率</span></span><br><span class="line">observed_data = <span class="number">0.8</span>  <span class="comment"># 观测数据 x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 MAP 分类器进行分类</span></span><br><span class="line">result = map_classification(P_C1, P_C2, P_x_given_C1, P_x_given_C2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出分类结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;观测数据 x 被分类为:&quot;</span>, result)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，我们使用了一个简单的二分类问题，并通过计算后验概率来将观测数据 x 分类为 C1 或 C2。实际应用中，需要根据具体的问题设置先验概率和条件概率，以及相应的观测数据。</p>
<p>课件中的公式：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731114632947-1691217347165-30.png" alt="image-20230731114632947"></p>
<p>在模式识别和分类问题中，通常使用 c* 和 c 来表示不同的类别。</p>
<ul>
<li><p>c* ：通常用 c* 表示真实的或标准的类别。在监督学习中，我们经常有一组被标记好的数据，其中每个样本都有一个正确的类别标签。c* 就表示这些正确的类别标签，也称为真实类别或标准类别。在测试阶段，我们可以将新的未知样本与 c* 进行比较，从而评估分类算法的性能。</p>
</li>
<li><p>c：通常用 c 表示分类算法预测的类别。在监督学习中，我们的目标是根据已知的数据学习一个模型，然后用这个模型对未知数据进行分类。c 就表示分类器预测的类别标签，也称为预测类别或估计类别。</p>
</li>
</ul>
<p>例如，在一个手写数字识别问题中，我们可能有一组标记好的手写数字图像数据，其中每个图像都有对应的真实数字标签（c* ），而我们的任务是使用分类算法对新的手写数字图像进行识别，并预测它们属于哪个数字类别（c）。在这种情况下，c* 表示真实的数字类别，c 表示分类器预测的数字类别。我们可以根据 c* 和 c 来评估分类算法的准确性和性能。</p>
<p>使用MAP RULE的生成式分类器：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731114743209-1691217347165-31.png" alt="image-20230731114743209"></p>
<p>这是用于生成式分类的MAP（Maximum A Posteriori）分类规则的表达式。在这个公式中，我们希望计算在给定观测数据 𝐗&#x3D;𝐱 的条件下，事件属于每个类别 $𝑐_𝑖$ 的后验概率。</p>
<ul>
<li><p>$𝑃(𝐶&#x3D;𝑐_𝑖 |𝐗&#x3D;𝐱)$：表示在给定观测数据 $𝐗&#x3D;𝐱$ 的条件下，事件属于类别 $𝑐_𝑖$ 的后验概率，也就是我们要计算的分类结果。</p>
</li>
<li><p>$𝑃(𝐗&#x3D;𝐱|𝐶&#x3D;𝑐_𝑖)$：表示在事件属于类别 $𝑐_𝑖$ 的条件下，观测数据 $𝐗&#x3D;𝐱$ 的概率，也称为似然概率。</p>
</li>
<li><p>$𝑃(𝐶&#x3D;𝑐_𝑖)$：表示类别 $𝑐_𝑖$ 的先验概率，也就是在没有观测数据时，我们对事件属于类别 $𝑐_𝑖$ 的概率的初始估计。</p>
</li>
<li><p>$𝑃(𝐗&#x3D;𝐱)$：表示观测数据 $𝐗&#x3D;𝐱$ 的边缘概率，即在所有类别的情况下观测数据 $𝐗&#x3D;𝐱$ 发生的概率。</p>
</li>
</ul>
<p>公式的含义是，在生成式分类中，我们首先计算观测数据 $𝐗&#x3D;𝐱$ 对每个类别 $𝑐_𝑖$ 的似然概率和类别先验概率的乘积，然后再进行归一化（即除以观测数据的边缘概率）以得到后验概率的相对大小。最终，选择具有最大后验概率的类别作为分类结果。</p>
<p>由于在分类问题中，观测数据的边缘概率对于所有类别是相同的，因此在上述公式中可以省略边缘概率的计算，而直接比较各类别的似然概率和先验概率乘积的大小，从而得到后验概率的相对大小，然后进行分类决策。</p>
<h3 id="朴素贝叶斯-Naive-Bayes"><a href="#朴素贝叶斯-Naive-Bayes" class="headerlink" title="朴素贝叶斯 Naive Bayes"></a>朴素贝叶斯 Naive Bayes</h3><p>贝叶斯：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201049365-1691217347165-32.png" alt="image-20230731201049365"></p>
<p>思路：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201253316-1691217347166-33.png" alt="image-20230731201253316"></p>
<p>假设Y有0和1两个取值，那么计算出$X&#x3D;(x_1, x_2, …, x_n)$的情况之下，Y&#x3D;0和Y&#x3D;1的概率，然后将其分配到概率值更高的Y的类别中。</p>
<p>假设有数据集如下所示：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731192744584-1691217347166-34.png" alt="image-20230731192744584"></p>
<p>现在有一个新的数据，X&#x3D;(0, 2)，要求找到正确的Y的标签。</p>
<p>那么按照之前的逻辑，就是计算 $P(Y&#x3D;0|X &#x3D; (0, 2))$  和 $P(Y&#x3D;1|X&#x3D;(0,2))$的概率，然后比较大小，最终将这个新的数据分配到概率较大的类别中。</p>
<p>首先进行计算：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731194218933-1691217347166-35.png" alt="image-20230731194218933"></p>
<p>因为分母都是$P(X&#x3D;(0, 2))$，所以分母对于最终的概率大小不存在着影响。如此只需要计算$P(X&#x3D;(0,2) | Y &#x3D; 0 或 Y &#x3D; 1)$ 和 $P(Y&#x3D;0 或 Y &#x3D; 1)$的乘积就可以了。最终这个数据被分配到$Y&#x3D;1$这个类别中。</p>
<p><strong>为什么采用朴素贝叶斯？</strong>因为我们需要在数据集中找到X&#x3D;(0,2)的特定组合，在存在着多个特征和大量的数据的情况之下，这是极其繁琐和困难的事情。比如说如果有十个特征，每个特征有三种可能性，那么排列的总数为：<br>$$<br>\text{总的排列组合数} &#x3D; 3^{10} &#x3D; 59,049<br>$$</p>
<p>因此，在这种情况下，一共有59,049种排列组合方式。</p>
<p>如果使用朴素贝叶斯，那么可以直接将X1和X2视作是独立的变量，计算公式就可以简化为：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201311687-1691217347166-36.png" alt="image-20230731201311687"></p>
<p>因为独立的两个变量的概率是可以直接相乘的。</p>
<p>简单的计算可得：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731201700558-1691217347166-37.png" alt="image-20230731201700558"></p>
<p>推导如下所示：</p>
<p>在概率论中，当两个事件A和B是独立事件时，它们的联合概率等于各自的概率的乘积。这就是独立事件的定义。</p>
<p>如果事件A和事件B是独立的，那么有：</p>
<p>$$P(A \cap B) &#x3D; P(A) \cdot P(B)$$</p>
<p>其中，</p>
<ul>
<li>$P(A \cap B)$ 表示事件A和事件B同时发生的概率，即它们的交集的概率。</li>
<li>$P(A)$ 表示事件A发生的概率。</li>
<li>$P(B)$ 表示事件B发生的概率。</li>
</ul>
<p>当事件A和事件B是独立的，我们有以下定义和性质：</p>
<ol>
<li><p>定义：两个事件A和B是独立的，当且仅当满足以下条件之一：</p>
<ul>
<li>$P(A \mid B) &#x3D; P(A)$，表示在已知事件B发生的条件下，事件A发生的概率与事件A发生的概率相等；</li>
<li>$P(B \mid A) &#x3D; P(B)$，表示在已知事件A发生的条件下，事件B发生的概率与事件B发生的概率相等。</li>
</ul>
</li>
<li><p>性质：如果事件A和事件B是独立的，则有 $P(A \cap B) &#x3D; P(A) \cdot P(B)$。</p>
</li>
</ol>
<p>证明：</p>
<p>根据独立事件的定义，我们知道：</p>
<p>$$P(A \mid B) &#x3D; P(A)$$</p>
<p>根据条件概率的定义，我们有：</p>
<p>$$P(A \mid B) &#x3D; \frac{P(A \cap B)}{P(B)}$$</p>
<p>将这两个等式结合起来，我们有：</p>
<p>$$P(A) &#x3D; \frac{P(A \cap B)}{P(B)}$$</p>
<p>然后，我们可以通过移项，得到：</p>
<p>$$P(A \cap B) &#x3D; P(A) \cdot P(B)$$</p>
<p>这就完成了对于 $P(A \cap B) &#x3D; P(A) \cdot P(B)$ 的证明。</p>
<p>因此，当事件A和事件B是独立的时候，它们的联合概率等于各自的概率的乘积。这是独立事件的一个重要性质。</p>
<p>当事件A和事件B是独立事件时，它们的发生不会相互影响。这意味着在给定另一个事件的发生情况下，A和B仍然是独立的。因此，事件A和事件B同时发生的概率就等于它们各自发生的概率的乘积。</p>
<p>请注意，当事件A和事件B不是独立事件时，它们的联合概率通常不等于各自概率的乘积，而需要使用条件概率来计算。</p>
<p><strong>朴素贝叶斯（Naive Bayes）</strong> 是一种简单且高效的概率分类算法，常用于文本分类和垃圾邮件过滤等任务。它基于贝叶斯定理和“朴素”假设，即认为特征之间是条件独立的。</p>
<p><strong>定义</strong>：</p>
<p>朴素贝叶斯分类是一种生成式分类方法，它假设给定类别标签，不同特征之间是条件独立的。这个假设使得朴素贝叶斯分类算法非常高效，并在许多实际问题中表现良好。</p>
<p><strong>公式</strong>：</p>
<p>假设我们有一个类别标签 C 和一组特征变量 X&#x3D;{X1, X2, …, Xn}。朴素贝叶斯分类器通过贝叶斯定理来计算后验概率，并选择具有最大后验概率的类别作为分类结果。</p>
<p>朴素贝叶斯分类器的计算公式为：</p>
<p>$$ P(C|X) &#x3D; \frac{P(X|C) \cdot P(C)}{P(X)} $$</p>
<p>其中，</p>
<ul>
<li>$ P(C|X) $ 表示在给定特征变量 X 的条件下，类别标签 C 的后验概率。</li>
<li>$ P(X|C) $ 表示在类别标签 C 的条件下，特征变量 X 的似然概率。</li>
<li>$ P(C) $ 是类别标签 C 的先验概率。</li>
<li>$ P(X) $ 是观测数据 X 的边缘概率。</li>
</ul>
<p>由于在朴素贝叶斯中，特征变量之间被假设为条件独立的，因此可以将似然概率表示为各个特征变量的条件概率的乘积：</p>
<p>$$ P(X|C) &#x3D; P(X1|C) \cdot P(X2|C) \cdot … \cdot P(Xn|C) $$</p>
<p><strong>例子</strong>：</p>
<p>假设我们有一个文本分类问题，需要根据电子邮件的文本内容将邮件分类为“垃圾邮件”或“非垃圾邮件”。我们的特征变量 X 是由邮件中出现的单词组成的集合，类别标签 C 可以是“垃圾邮件”或“非垃圾邮件”。</p>
<p>在朴素贝叶斯分类中，我们首先计算观测数据 X（即邮件中出现的单词集合）在每个类别下的似然概率 $P(X|C)$。然后，我们根据先验概率 $P(C)$ 和边缘概率 $P(X)$，使用贝叶斯定理计算后验概率 $P(C|X)$。最终，我们选择具有最大后验概率的类别作为邮件的分类结果。</p>
<p>例如，如果一封邮件中出现了单词“折扣”、“免费”和“优惠”，我们可以计算这封邮件在“垃圾邮件”类别和“非垃圾邮件”类别下的似然概率，并使用贝叶斯定理来计算后验概率，从而将这封邮件分类为“垃圾邮件”或“非垃圾邮件”。</p>
<p>课件中的公式，<strong>假设所有输入特征都是条件独立的：</strong><br>$$<br>P(X_1, X_2, …, X_n | C) &#x3D; P(X_1 | X_2, …, X_n, C) \cdot P(X_2, …, X_n | C) \<br>&#x3D; P(X_1 | C) \cdot P(X_2, …, X_n | C) \<br>&#x3D; P(X_1 | C) \cdot P(X_2 | C) \cdot … \cdot P(X_n | C)<br>$$<br>这个公式是根据条件概率的定义，利用概率的链式法则来推导。它表示在给定类别标签 C 的条件下，多个特征变量 X1, X2, …, Xn 同时发生的概率可以分解为各个特征变量在给定类别标签 C 的条件下的概率的乘积。在朴素贝叶斯分类中，我们使用这个条件独立性假设，将多个特征变量之间的联合概率分解为各个特征变量在给定类别标签 C 的条件下的概率的乘积，从而简化了计算。</p>
<p><strong>更加具体的解释</strong>：</p>
<p>上述公式是朴素贝叶斯分类中使用条件独立性假设的推导过程。它基于概率的链式法则，将多个特征变量在给定类别标签 C 的条件下的联合概率拆解成各个特征变量在给定类别标签 C 的条件下的概率的乘积。这样的假设使得朴素贝叶斯分类器计算高效，并在实际应用中表现优秀。</p>
<p><strong>例子</strong>：</p>
<p>假设我们有一个文本分类问题，需要将电子邮件分类为“垃圾邮件”或“非垃圾邮件”。特征变量 X 是由邮件中出现的单词组成的集合，类别标签 C 可以是“垃圾邮件”或“非垃圾邮件”。</p>
<p>为了更好地理解上述公式，我们将其应用于一个简单的文本分类示例：</p>
<p>假设我们有一个训练数据集，其中包含以下三封邮件：</p>
<ol>
<li>邮件1：内容是 “优惠 折扣”，属于 “垃圾邮件” 类别。</li>
<li>邮件2：内容是 “商品 优惠”，属于 “垃圾邮件” 类别。</li>
<li>邮件3：内容是 “新闻 报告”，属于 “非垃圾邮件” 类别。</li>
</ol>
<p>现在，我们想对一封新的邮件进行分类，内容是 “商品 优惠 折扣”。我们需要计算在给定这封邮件的内容的条件下，属于不同类别的后验概率。</p>
<p>首先，我们使用训练数据集来计算先验概率 $P(C)$ 和各个单词在给定类别的条件下的概率 $P(X_i | C)$。在这个例子中，我们有两个类别，因此需要计算两个先验概率 $P(垃圾邮件)$ 和 $P(非垃圾邮件)$，以及每个单词在两个类别下的条件概率，如 $P(优惠 | 垃圾邮件)$、$P(折扣 | 垃圾邮件)$、$P(商品 | 垃圾邮件)$ 和 $P(新闻 | 非垃圾邮件)$。</p>
<p>然后，我们使用上述先验概率和条件概率，根据上述公式计算后验概率 $P(垃圾邮件 | 商品 优惠 折扣)$ 和 $P(非垃圾邮件 | 商品 优惠 折扣)$。通过比较这两个后验概率的大小，我们可以判断该封邮件属于哪个类别，从而完成文本分类。</p>
<p>在处理<strong>离散值特征（Discrete-Valued Features）</strong>的情况下，Naïve Bayes 算法<strong>假设各个特征之间是条件独立的</strong>，并基于贝叶斯定理来计算后验概率从而进行分类。</p>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><p>数据准备：收集标记好的训练数据，其中每个样本都有一个类别标签和多个离散值特征。对于每个特征，我们需要计算在给定类别的条件下，该特征取每个可能取值的概率。</p>
</li>
<li><p>计算先验概率：根据训练数据集中每个类别出现的频次，计算各个类别的先验概率 $P(C)$。</p>
</li>
<li><p>计算条件概率：对于每个特征 $X_i$，计算在给定类别的条件下，该特征取每个可能取值的概率 $P(X_i | C)$。为了避免零概率问题，通常会使用拉普拉斯平滑或其他平滑技术来处理未在训练数据中出现的特征值。</p>
</li>
<li><p>分类决策：对于一个新的样本数据，计算该样本属于各个类别的后验概率 $P(C | X_1, X_2, …, X_n)$，其中 $X_1, X_2, …, X_n$ 是该样本的特征值。通过比较各个类别的后验概率，选择具有最大后验概率的类别作为分类结果。</p>
</li>
</ol>
<p><strong>解释</strong>：</p>
<p>在 Naïve Bayes 算法中，”Naïve” 指的是特征之间被假设为条件独立的，这是朴素贝叶斯算法的核心假设。虽然这个假设在实际应用中可能并不总是成立，但在许多情况下，这种简化可以带来高效且准确的分类结果。</p>
<p>对于离散值特征，我们需要计算每个特征在给定类别下的条件概率 $P(X_i | C)$。在文本分类问题中，特征通常是单词或词汇，条件概率表示在给定类别的条件下，每个单词在文本中出现的概率。通过计算先验概率 $P(C)$ 和条件概率 $P(X_i | C)$，我们可以使用贝叶斯定理来计算后验概率 $P(C | X_1, X_2, …, X_n)$，即在给定特征值的条件下，样本属于各个类别的概率。最终，我们选择具有最大后验概率的类别作为分类结果。</p>
<p>Naïve Bayes 算法因其简单、高效且在许多实际问题中表现良好而受到广泛应用，特别是在文本分类和垃圾邮件过滤等任务中。由于它不需要大量的参数调整和复杂的特征工程，因此在实践中通常是一种强大的分类工具。</p>
<h3 id="朴素贝叶斯的代码实现"><a href="#朴素贝叶斯的代码实现" class="headerlink" title="朴素贝叶斯的代码实现"></a>朴素贝叶斯的代码实现</h3><p><strong>Python语言实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_naive_bayes_classifier</span>(<span class="params">train_data, train_labels</span>):</span><br><span class="line">    <span class="comment"># 创建特征提取器，将文本转换为特征向量</span></span><br><span class="line">    vectorizer = CountVectorizer()</span><br><span class="line">    X_train = vectorizer.fit_transform(train_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建朴素贝叶斯分类器并进行训练</span></span><br><span class="line">    nb_classifier = MultinomialNB()</span><br><span class="line">    nb_classifier.fit(X_train, train_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nb_classifier, vectorizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_naive_bayes_classifier</span>(<span class="params">classifier, vectorizer, test_data</span>):</span><br><span class="line">    <span class="comment"># 将测试数据转换为特征向量</span></span><br><span class="line">    X_test = vectorizer.transform(test_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行预测</span></span><br><span class="line">    predicted_labels = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predicted_labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">train_data = [<span class="string">&quot;This is a positive review.&quot;</span>,</span><br><span class="line">              <span class="string">&quot;I really enjoyed this movie.&quot;</span>,</span><br><span class="line">              <span class="string">&quot;The acting was superb!&quot;</span>,</span><br><span class="line">              <span class="string">&quot;Terrible movie, do not watch it.&quot;</span>,</span><br><span class="line">              <span class="string">&quot;Waste of time and money.&quot;</span>]</span><br><span class="line"></span><br><span class="line">train_labels = [<span class="string">&quot;positive&quot;</span>, <span class="string">&quot;positive&quot;</span>, <span class="string">&quot;positive&quot;</span>, <span class="string">&quot;negative&quot;</span>, <span class="string">&quot;negative&quot;</span>]</span><br><span class="line"></span><br><span class="line">test_data = [<span class="string">&quot;I loved the movie, it was fantastic!&quot;</span>,</span><br><span class="line">             <span class="string">&quot;Avoid this movie, it&#x27;s awful.&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练朴素贝叶斯分类器</span></span><br><span class="line">nb_classifier, vectorizer = train_naive_bayes_classifier(train_data, train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">predicted_labels = predict_naive_bayes_classifier(nb_classifier, vectorizer, test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出预测结果</span></span><br><span class="line"><span class="built_in">print</span>(predicted_labels)  <span class="comment"># 输出: [&#x27;positive&#x27; &#x27;negative&#x27;]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;positive&#x27;</span> <span class="string">&#x27;positive&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><strong>使用R语言实现朴素贝叶斯</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">naive_bayes_train &lt;- function(data, feature_col, class_col) &#123;</span><br><span class="line">  # 先验概率</span><br><span class="line">  priors &lt;- table(data[[class_col]]) / nrow(data)</span><br><span class="line">  </span><br><span class="line">  # 条件概率</span><br><span class="line">  conditional_probs &lt;- list()</span><br><span class="line">  for(class in unique(data[[class_col]])) &#123;</span><br><span class="line">    subset_data &lt;- data[data[[class_col]] == class, ]</span><br><span class="line">    conditional_probs[[class]] &lt;- table(subset_data[[feature_col]]) / nrow(subset_data)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  list(priors = priors, conditional_probs = conditional_probs)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">naive_bayes_predict &lt;- function(model, new_feature) &#123;</span><br><span class="line">  scores &lt;- c()</span><br><span class="line">  </span><br><span class="line">  for(class in names(model$priors)) &#123;</span><br><span class="line">    # P(class)</span><br><span class="line">    score &lt;- model$priors[class]</span><br><span class="line">    </span><br><span class="line">    # P(feature|class)</span><br><span class="line">    if(new_feature %in% names(model$conditional_probs[[class]])) &#123;</span><br><span class="line">      score &lt;- score * model$conditional_probs[[class]][new_feature]</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      score &lt;- 0</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    scores[class] &lt;- score</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  names(which.max(scores))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 训练</span><br><span class="line">model &lt;- naive_bayes_train(data, &#x27;Feature&#x27;, &#x27;Play&#x27;)</span><br><span class="line"></span><br><span class="line"># 预测</span><br><span class="line">prediction &lt;- naive_bayes_predict(model, &#x27;sunny&#x27;)</span><br><span class="line">print(prediction)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; # 预测</span><br><span class="line">&gt; prediction &lt;- naive_bayes_predict(model, &#x27;sunny&#x27;)</span><br><span class="line">&gt; print(prediction)</span><br><span class="line">[1] &quot;yes</span><br></pre></td></tr></table></figure>

<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>朴素贝叶斯的优点</strong>：</p>
<ol>
<li><p>简单高效：朴素贝叶斯算法是一种简单高效的分类算法。它基于概率模型，假设特征之间是条件独立的，使得算法计算高效，适用于大规模数据集。</p>
</li>
<li><p>适用于高维数据：朴素贝叶斯算法在高维数据上表现良好，尤其在文本分类等自然语言处理任务中广泛应用。</p>
</li>
<li><p>对小规模数据集有较好表现：即使在小规模数据集上，朴素贝叶斯也能表现出较好的性能。</p>
</li>
<li><p>适用于多分类问题：朴素贝叶斯算法天生支持多分类问题，可以轻松处理多个类别的分类任务。</p>
</li>
</ol>
<p><strong>朴素贝叶斯的缺点</strong>：</p>
<ol>
<li><p>忽略特征之间的相关性：朴素贝叶斯算法假设所有特征之间是条件独立的，但在现实问题中，很多特征可能存在相关性。因此，在处理特征之间强相关的数据时，朴素贝叶斯可能会导致分类性能下降。</p>
</li>
<li><p>处理连续特征困难：朴素贝叶斯算法通常假设特征是离散值的，对于连续值特征，需要进行离散化处理，可能会引入信息损失。</p>
</li>
<li><p>对数据分布的假设：朴素贝叶斯算法假设特征之间是条件独立的，这种假设在某些数据集上可能不成立，导致分类性能下降。</p>
</li>
</ol>
<p><strong>具体例子</strong>：</p>
<p>假设我们有一个电子邮件分类问题，需要将邮件分类为“垃圾邮件”或“非垃圾邮件”。我们使用朴素贝叶斯算法来处理这个问题。</p>
<p><strong>优点</strong>：</p>
<ul>
<li>简单高效：朴素贝叶斯算法处理文本分类问题非常高效，并且在大规模数据集上表现出色。</li>
<li>适用于高维数据：文本分类问题通常涉及大量特征（每个单词都是一个特征），朴素贝叶斯在高维数据上表现良好。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>忽略特征之间的相关性：例如，对于文本分类问题，一些词汇可能有强相关性（如“good”和“excellent”），而朴素贝叶斯算法忽略了这些相关性。</li>
<li>处理连续特征困难：如果文本分类问题中需要考虑词汇的频率等连续值特征，朴素贝叶斯算法需要进行离散化处理，可能会引入信息损失。</li>
<li>对数据分布的假设：如果文本分类问题中不同类别的邮件之间有交叉词汇，朴素贝叶斯的条件独立性假设可能导致性能下降。</li>
</ul>
<h2 id="Practical-Weka"><a href="#Practical-Weka" class="headerlink" title="Practical - Weka"></a>Practical - Weka</h2><h3 id="什么是weka"><a href="#什么是weka" class="headerlink" title="什么是weka"></a>什么是weka</h3><p>Weka是一种开源的机器学习软件，它是由新西兰怀卡托大学开发的。Weka的名称来源于新西兰土著毛利人的一种鸟类，同样也是软件的缩写，代表”Waikato Environment for Knowledge Analysis”，意为怀卡托知识分析环境。</p>
<p>Weka提供了各种机器学习算法和数据挖掘工具，使用户能够进行数据预处理、分类、回归、聚类、关联规则挖掘等任务。它是一款功能强大且易于使用的工具，适合初学者和专业人士。</p>
<p>Weka的主要特点包括：</p>
<ol>
<li>用户友好：它提供了图形用户界面(GUI)，使用户可以轻松地进行数据建模和分析，同时也支持命令行接口。</li>
<li>开源：Weka是开源软件，任何人都可以免费使用、修改和分发。</li>
<li>扩展性：用户可以方便地编写自己的算法或添加插件来扩展软件的功能。</li>
<li>大量的算法：Weka内置了许多经典和先进的机器学习算法，包括决策树、支持向量机、朴素贝叶斯、K近邻、神经网络等。</li>
<li>数据可视化：Weka提供了数据可视化功能，使用户可以更好地理解和分析数据。</li>
</ol>
<p>总的来说，Weka是一款功能丰富、易于使用的机器学习软件，为研究人员、数据科学家和机器学习爱好者提供了一个强大的工具，用于解决各种数据挖掘和机器学习问题。</p>
<h3 id="下载和安装weka"><a href="#下载和安装weka" class="headerlink" title="下载和安装weka"></a>下载和安装weka</h3><p>官方网址：<a target="_blank" rel="noopener" href="https://www.cs.waikato.ac.nz/ml/weka/">https://www.cs.waikato.ac.nz/ml/weka/</a></p>
<p>首先是需要的环境：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731161357855.png" alt="image-20230731161357855"></p>
<p>然后是下载：<a target="_blank" rel="noopener" href="https://waikato.github.io/weka-wiki/downloading_weka/">https://waikato.github.io/weka-wiki/downloading_weka/</a></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731161434097.png" alt="image-20230731161434097"></p>
<p>一路默认安装就可以了。</p>
<h3 id="第一个Weka程序"><a href="#第一个Weka程序" class="headerlink" title="第一个Weka程序"></a>第一个Weka程序</h3><p>首先运行weka：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162014045.png" alt="image-20230731162014045"></p>
<p>点击确定就可以了，然后点击explorer：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162059203.png" alt="image-20230731162059203"></p>
<p>接着是了解数据所在的路径：</p>
<blockquote>
<p>C:\Program Files\Weka-3-8-6\data</p>
</blockquote>
<p>导入数据：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162230334.png" alt="image-20230731162230334"></p>
<p>得到如下所示的结果：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162242550.png" alt="image-20230731162242550"></p>
<p>在classify中选择：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162322829.png" alt="image-20230731162322829"></p>
<p>最终得到：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162336316.png" alt="image-20230731162336316"></p>
<p>点击 start 可以得到：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162358384.png" alt="image-20230731162358384"></p>
<p>output全部内容为：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">=== Run information ===</span><br><span class="line"></span><br><span class="line">Scheme:       weka.classifiers.bayes.NaiveBayes </span><br><span class="line">Relation:     weather.symbolic</span><br><span class="line">Instances:    14</span><br><span class="line">Attributes:   5</span><br><span class="line">              outlook</span><br><span class="line">              temperature</span><br><span class="line">              humidity</span><br><span class="line">              windy</span><br><span class="line">              play</span><br><span class="line">Test mode:    evaluate on training data</span><br><span class="line"></span><br><span class="line">=== Classifier model (full training set) ===</span><br><span class="line"></span><br><span class="line">Naive Bayes Classifier</span><br><span class="line"></span><br><span class="line">                Class</span><br><span class="line">Attribute         yes     no</span><br><span class="line">               (0.63) (0.38)</span><br><span class="line">=============================</span><br><span class="line">outlook</span><br><span class="line">  sunny            3.0    4.0</span><br><span class="line">  overcast         5.0    1.0</span><br><span class="line">  rainy            4.0    3.0</span><br><span class="line">  [total]         12.0    8.0</span><br><span class="line"></span><br><span class="line">temperature</span><br><span class="line">  hot              3.0    3.0</span><br><span class="line">  mild             5.0    3.0</span><br><span class="line">  cool             4.0    2.0</span><br><span class="line">  [total]         12.0    8.0</span><br><span class="line"></span><br><span class="line">humidity</span><br><span class="line">  high             4.0    5.0</span><br><span class="line">  normal           7.0    2.0</span><br><span class="line">  [total]         11.0    7.0</span><br><span class="line"></span><br><span class="line">windy</span><br><span class="line">  TRUE             4.0    4.0</span><br><span class="line">  FALSE            7.0    3.0</span><br><span class="line">  [total]         11.0    7.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Time taken to build model: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Evaluation on training set ===</span><br><span class="line"></span><br><span class="line">Time taken to test model on training data: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Summary ===</span><br><span class="line"></span><br><span class="line">Correctly Classified Instances          13               92.8571 %</span><br><span class="line">Incorrectly Classified Instances         1                7.1429 %</span><br><span class="line">Kappa statistic                          0.8372</span><br><span class="line">Mean absolute error                      0.2917</span><br><span class="line">Root mean squared error                  0.3392</span><br><span class="line">Relative absolute error                 62.8233 %</span><br><span class="line">Root relative squared error             70.7422 %</span><br><span class="line">Total Number of Instances               14     </span><br><span class="line"></span><br><span class="line">=== Detailed Accuracy By Class ===</span><br><span class="line"></span><br><span class="line">                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class</span><br><span class="line">                 1.000    0.200    0.900      1.000    0.947      0.849    0.922     0.947     yes</span><br><span class="line">                 0.800    0.000    1.000      0.800    0.889      0.849    0.911     0.911     no</span><br><span class="line">Weighted Avg.    0.929    0.129    0.936      0.929    0.926      0.849    0.918     0.934     </span><br><span class="line"></span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line"></span><br><span class="line"> a b   &lt;-- classified as</span><br><span class="line"> 9 0 | a = yes</span><br><span class="line"> 1 4 | b = no</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对上述output结果的解读：</p>
<p>这个运行结果是基于Weka中朴素贝叶斯分类器（Naive Bayes Classifier）对一个名为”weather.symbolic”的数据集进行训练和测试得到的。数据集中共有14个实例（样本），每个实例有5个属性（特征）。</p>
<p>下面对运行结果进行解读：</p>
<ol>
<li><p>Scheme信息：</p>
<ul>
<li>使用的分类器：朴素贝叶斯分类器（NaiveBayes）</li>
<li>数据集关系：weather.symbolic</li>
<li>实例数量：14</li>
<li>属性数量：5</li>
<li>测试模式：在训练数据上进行评估</li>
</ul>
</li>
<li><p>分类器模型：</p>
<ul>
<li>该部分显示了朴素贝叶斯分类器在训练集上的结果。它展示了每个属性值对应的”yes”（是）和”no”（否）的分类情况。</li>
</ul>
</li>
<li><p>评估结果：</p>
<ul>
<li>正确分类实例数：13，正确率为92.8571%</li>
<li>错误分类实例数：1，错误率为7.1429%</li>
<li>Kappa统计量：0.8372（衡量分类器性能的一种指标，值越接近1表示性能越好）</li>
<li>平均绝对误差：0.2917</li>
<li>均方根误差：0.3392</li>
<li>相对绝对误差：62.8233%（平均绝对误差相对于真实值的百分比）</li>
<li>根相对平方误差：70.7422%（均方根误差相对于真实值的百分比）</li>
<li>总实例数：14</li>
</ul>
</li>
<li><p>类别详细准确率：</p>
<ul>
<li>以每个类别（”yes”和”no”）为基础，给出了真阳性率、假阳性率、精确率、召回率和F1得分等指标。</li>
</ul>
</li>
<li><p>混淆矩阵：</p>
<ul>
<li>混淆矩阵展示了真实类别与预测类别之间的对应关系。在本例中，”yes”被正确分类为”yes”的有9个实例，”no”被正确分类为”no”的有4个实例，但是有1个”no”实例被错误分类为”yes”。</li>
</ul>
</li>
</ol>
<p>综上所述，朴素贝叶斯分类器在这个数据集上表现良好，正确率达到92.8571%，而且Kappa统计量较高，说明分类器的性能较好。然而，要注意这是在训练集上进行的评估，未来还需要在独立的测试集上进行评估，以确保分类器在未见过的数据上的泛化能力。</p>
<p>接着将数据复制到桌面，然后改名为：weather.nominal-02.arff</p>
<p>接着将数据的内容修改为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@relation weather.symbolic</span><br><span class="line"></span><br><span class="line">@attribute outlook &#123;sunny, overcast, rainy&#125;</span><br><span class="line">@attribute temperature &#123;hot, mild, cool&#125;</span><br><span class="line">@attribute humidity &#123;high, normal&#125;</span><br><span class="line">@attribute windy &#123;TRUE, FALSE&#125;</span><br><span class="line">@attribute play &#123;yes, no&#125;</span><br><span class="line"></span><br><span class="line">@data</span><br><span class="line">sunny, cool, high, TRUE, no</span><br></pre></td></tr></table></figure>

<p>接着将修改之后的数据导入：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731162854763.png" alt="image-20230731162854763"></p>
<p>点击start，得到结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">=== Run information ===</span><br><span class="line"></span><br><span class="line">Scheme:       weka.classifiers.bayes.NaiveBayes </span><br><span class="line">Relation:     weather.symbolic</span><br><span class="line">Instances:    14</span><br><span class="line">Attributes:   5</span><br><span class="line">              outlook</span><br><span class="line">              temperature</span><br><span class="line">              humidity</span><br><span class="line">              windy</span><br><span class="line">              play</span><br><span class="line">Test mode:    user supplied test set:  size unknown (reading incrementally)</span><br><span class="line"></span><br><span class="line">=== Classifier model (full training set) ===</span><br><span class="line"></span><br><span class="line">Naive Bayes Classifier</span><br><span class="line"></span><br><span class="line">                Class</span><br><span class="line">Attribute         yes     no</span><br><span class="line">               (0.63) (0.38)</span><br><span class="line">=============================</span><br><span class="line">outlook</span><br><span class="line">  sunny            3.0    4.0</span><br><span class="line">  overcast         5.0    1.0</span><br><span class="line">  rainy            4.0    3.0</span><br><span class="line">  [total]         12.0    8.0</span><br><span class="line"></span><br><span class="line">temperature</span><br><span class="line">  hot              3.0    3.0</span><br><span class="line">  mild             5.0    3.0</span><br><span class="line">  cool             4.0    2.0</span><br><span class="line">  [total]         12.0    8.0</span><br><span class="line"></span><br><span class="line">humidity</span><br><span class="line">  high             4.0    5.0</span><br><span class="line">  normal           7.0    2.0</span><br><span class="line">  [total]         11.0    7.0</span><br><span class="line"></span><br><span class="line">windy</span><br><span class="line">  TRUE             4.0    4.0</span><br><span class="line">  FALSE            7.0    3.0</span><br><span class="line">  [total]         11.0    7.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Time taken to build model: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Evaluation on test set ===</span><br><span class="line"></span><br><span class="line">Time taken to test model on supplied test set: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Summary ===</span><br><span class="line"></span><br><span class="line">Correctly Classified Instances           1              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0.2647</span><br><span class="line">Root mean squared error                  0.2647</span><br><span class="line">Relative absolute error                 42.3498 %</span><br><span class="line">Root relative squared error             42.3498 %</span><br><span class="line">Total Number of Instances                1     </span><br><span class="line"></span><br><span class="line">=== Detailed Accuracy By Class ===</span><br><span class="line"></span><br><span class="line">                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class</span><br><span class="line">                 ?        0.000    ?          ?        ?          ?        ?         ?         yes</span><br><span class="line">                 1.000    ?        1.000      1.000    1.000      ?        ?         1.000     no</span><br><span class="line">Weighted Avg.    1.000    ?        1.000      1.000    1.000      ?        ?         1.000     </span><br><span class="line"></span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line"></span><br><span class="line"> a b   &lt;-- classified as</span><br><span class="line"> 0 0 | a = yes</span><br><span class="line"> 0 1 | b = no</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果显示了朴素贝叶斯分类器在一个测试集上的评估结果。这里测试集只包含一个实例（样本），具体信息如下：</p>
<ol>
<li><p>Scheme信息：</p>
<ul>
<li>使用的分类器：朴素贝叶斯分类器（NaiveBayes）</li>
<li>数据集关系：weather.symbolic</li>
<li>实例数量：14</li>
<li>属性数量：5</li>
<li>测试模式：使用用户提供的测试集进行评估，但并未显示测试集的大小（size unknown）。</li>
</ul>
</li>
<li><p>分类器模型：</p>
<ul>
<li>该部分和之前的结果相同，显示了朴素贝叶斯分类器在训练集上的分类结果。</li>
</ul>
</li>
<li><p>评估结果：</p>
<ul>
<li>正确分类实例数：1，正确率为100%</li>
<li>错误分类实例数：0，错误率为0%</li>
<li>Kappa统计量：1（这是极好的Kappa值，意味着预测结果与实际结果完全一致）</li>
<li>平均绝对误差：0.2647</li>
<li>均方根误差：0.2647</li>
<li>相对绝对误差：42.3498%（平均绝对误差相对于真实值的百分比）</li>
<li>根相对平方误差：42.3498%（均方根误差相对于真实值的百分比）</li>
<li>总实例数：1</li>
</ul>
</li>
<li><p>类别详细准确率：</p>
<ul>
<li>对于”yes”类别，由于测试集只有一个实例，无法给出具体的真阳性率、假阳性率、精确率、召回率和F1得分等指标。因此，显示为”?”。但因为该实例被正确分类为”yes”，所以准确率为100%。</li>
<li>对于”no”类别，同样由于测试集只有一个实例，无法给出具体的统计信息，但该实例也被正确分类为”no”，准确率为100%。</li>
</ul>
</li>
<li><p>混淆矩阵：</p>
<ul>
<li>由于测试集只有一个实例，无法生成混淆矩阵。</li>
</ul>
</li>
</ol>
<p>综上所述，朴素贝叶斯分类器在这个测试集上表现非常好，将测试集中的实例100%地正确分类为”yes”类别。由于测试集只有一个实例，因此无法提供其他详细准确率和混淆矩阵等信息。</p>
<p><strong>离散化对朴素贝叶斯模型的提升</strong></p>
<p>接着使用另一个数据集：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731163520091.png" alt="image-20230731163520091"></p>
<p>classifier还是Naive Bayes，然后将percentage split设置为75%，也就是75%的数据作为训练数据集，25%的数据集设置为测试数据集：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731163812853.png" alt="image-20230731163812853"></p>
<p>接着按 start，得到结果为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">=== Run information ===</span><br><span class="line"></span><br><span class="line">Scheme:       weka.classifiers.bayes.NaiveBayes </span><br><span class="line">Relation:     pima_diabetes</span><br><span class="line">Instances:    768</span><br><span class="line">Attributes:   9</span><br><span class="line">              preg</span><br><span class="line">              plas</span><br><span class="line">              pres</span><br><span class="line">              skin</span><br><span class="line">              insu</span><br><span class="line">              mass</span><br><span class="line">              pedi</span><br><span class="line">              age</span><br><span class="line">              class</span><br><span class="line">Test mode:    split 75.0% train, remainder test</span><br><span class="line"></span><br><span class="line">=== Classifier model (full training set) ===</span><br><span class="line"></span><br><span class="line">Naive Bayes Classifier</span><br><span class="line"></span><br><span class="line">                         Class</span><br><span class="line">Attribute      tested_negative tested_positive</span><br><span class="line">                        (0.65)          (0.35)</span><br><span class="line">===============================================</span><br><span class="line">preg</span><br><span class="line">  mean                   3.4234          4.9795</span><br><span class="line">  std. dev.              3.0166          3.6827</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              1.0625          1.0625</span><br><span class="line"></span><br><span class="line">plas</span><br><span class="line">  mean                 109.9541        141.2581</span><br><span class="line">  std. dev.             26.1114         31.8728</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              1.4741          1.4741</span><br><span class="line"></span><br><span class="line">pres</span><br><span class="line">  mean                  68.1397          70.718</span><br><span class="line">  std. dev.             17.9834         21.4094</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              2.6522          2.6522</span><br><span class="line"></span><br><span class="line">skin</span><br><span class="line">  mean                  19.8356         22.2824</span><br><span class="line">  std. dev.             14.8974         17.6992</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision                1.98            1.98</span><br><span class="line"></span><br><span class="line">insu</span><br><span class="line">  mean                  68.8507        100.2812</span><br><span class="line">  std. dev.              98.828        138.4883</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision               4.573           4.573</span><br><span class="line"></span><br><span class="line">mass</span><br><span class="line">  mean                  30.3009         35.1475</span><br><span class="line">  std. dev.              7.6833          7.2537</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              0.2717          0.2717</span><br><span class="line"></span><br><span class="line">pedi</span><br><span class="line">  mean                   0.4297          0.5504</span><br><span class="line">  std. dev.              0.2986          0.3715</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              0.0045          0.0045</span><br><span class="line"></span><br><span class="line">age</span><br><span class="line">  mean                  31.2494         37.0808</span><br><span class="line">  std. dev.             11.6059         10.9146</span><br><span class="line">  weight sum                500             268</span><br><span class="line">  precision              1.1765          1.1765</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Time taken to build model: 0.01 seconds</span><br><span class="line"></span><br><span class="line">=== Evaluation on test split ===</span><br><span class="line"></span><br><span class="line">Time taken to test model on test split: 0.01 seconds</span><br><span class="line"></span><br><span class="line">=== Summary ===</span><br><span class="line"></span><br><span class="line">Correctly Classified Instances         150               78.125  %</span><br><span class="line">Incorrectly Classified Instances        42               21.875  %</span><br><span class="line">Kappa statistic                          0.4998</span><br><span class="line">Mean absolute error                      0.2642</span><br><span class="line">Root mean squared error                  0.3848</span><br><span class="line">Relative absolute error                 58.7363 %</span><br><span class="line">Root relative squared error             82.0682 %</span><br><span class="line">Total Number of Instances              192     </span><br><span class="line"></span><br><span class="line">=== Detailed Accuracy By Class ===</span><br><span class="line"></span><br><span class="line">                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class</span><br><span class="line">                 0.838    0.339    0.838      0.838    0.838      0.500    0.850     0.911     tested_negative</span><br><span class="line">                 0.661    0.162    0.661      0.661    0.661      0.500    0.850     0.772     tested_positive</span><br><span class="line">Weighted Avg.    0.781    0.281    0.781      0.781    0.781      0.500    0.850     0.866     </span><br><span class="line"></span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line"></span><br><span class="line">   a   b   &lt;-- classified as</span><br><span class="line"> 109  21 |   a = tested_negative</span><br><span class="line">  21  41 |   b = tested_positive</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个运行结果是基于朴素贝叶斯分类器（NaiveBayes）对一个名为”pima_diabetes”的数据集进行训练和测试得到的。数据集包含768个实例（样本），每个实例有9个属性（特征）。</p>
<p>下面对运行结果进行解读：</p>
<ol>
<li><p>Scheme信息：</p>
<ul>
<li>使用的分类器：朴素贝叶斯分类器（NaiveBayes）</li>
<li>数据集关系：pima_diabetes</li>
<li>实例数量：768</li>
<li>属性数量：9</li>
<li>测试模式：采用75.0%的数据作为训练集，其余作为测试集。</li>
</ul>
</li>
<li><p>分类器模型：</p>
<ul>
<li>该部分展示了朴素贝叶斯分类器在训练集上对每个属性在各个类别（tested_negative和tested_positive）上的统计信息，包括均值、标准差、样本数和精度。</li>
</ul>
</li>
<li><p>评估结果：</p>
<ul>
<li>正确分类实例数：150，正确率为78.125%</li>
<li>错误分类实例数：42，错误率为21.875%</li>
<li>Kappa统计量：0.4998（0表示随机分类，1表示完全一致分类）</li>
<li>平均绝对误差：0.2642</li>
<li>均方根误差：0.3848</li>
<li>相对绝对误差：58.7363%（平均绝对误差相对于真实值的百分比）</li>
<li>根相对平方误差：82.0682%（均方根误差相对于真实值的百分比）</li>
<li>总实例数：192</li>
</ul>
</li>
<li><p>类别详细准确率：</p>
<ul>
<li>对于”tested_negative”类别，真阳性率（召回率）为0.838，假阳性率为0.339，精确率为0.838。</li>
<li>对于”tested_positive”类别，真阳性率（召回率）为0.661，假阳性率为0.162，精确率为0.661。</li>
<li>加权平均（Weighted Avg.）的指标是综合两个类别的准确率得出的。</li>
</ul>
</li>
<li><p>混淆矩阵：</p>
<ul>
<li>混淆矩阵展示了真实类别与预测类别之间的对应关系。在本例中，共有192个测试集实例。</li>
<li>对于”tested_negative”类别，其中109个实例被正确分类为”tested_negative”，21个实例被错误分类为”tested_positive”。</li>
<li>对于”tested_positive”类别，其中41个实例被正确分类为”tested_positive”，21个实例被错误分类为”tested_negative”。</li>
</ul>
</li>
</ol>
<p>综上所述，朴素贝叶斯分类器在这个数据集上表现一般，正确率为78.125%，Kappa统计量为0.4998。需要进一步优化算法或者尝试其他的分类器来提高分类性能。</p>
<p>在混淆矩阵中，我们可以找到真阴性（True Negative，TN）、假阳性（False Positive，FP）、假阴性（False Negative，FN）、真阳性（True Positive，TP）的值。</p>
<p>混淆矩阵如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  a   b   &lt;-- classified as</span><br><span class="line">109  21 |   a = tested_negative</span><br><span class="line"> 21  41 |   b = tested_positive</span><br></pre></td></tr></table></figure>

<p>其中，</p>
<ul>
<li>a表示实际为”tested_negative”类别的样本数量（即负样本），</li>
<li>b表示实际为”tested_positive”类别的样本数量（即正样本）。</li>
</ul>
<p>现在我们可以计算TN、FP、FN和TP：</p>
<ul>
<li>TN：实际为”tested_negative”且被正确分类为”tested_negative”的样本数量。</li>
<li>FP：实际为”tested_negative”但被错误分类为”tested_positive”的样本数量。</li>
<li>FN：实际为”tested_positive”但被错误分类为”tested_negative”的样本数量。</li>
<li>TP：实际为”tested_positive”且被正确分类为”tested_positive”的样本数量。</li>
</ul>
<p>根据混淆矩阵中的值，我们可以得到：</p>
<ul>
<li>TN &#x3D; 109</li>
<li>FP &#x3D; 21</li>
<li>FN &#x3D; 21</li>
<li>TP &#x3D; 41</li>
</ul>
<p>下面是表格展示结果：</p>
<table>
<thead>
<tr>
<th></th>
<th>实际”tested_negative”</th>
<th>实际”tested_positive”</th>
</tr>
</thead>
<tbody><tr>
<td>预测”tested_negative”</td>
<td>TN &#x3D; 109</td>
<td>FP &#x3D; 21</td>
</tr>
<tr>
<td>预测”tested_positive”</td>
<td>FN &#x3D; 21</td>
<td>TP &#x3D; 41</td>
</tr>
</tbody></table>
<p>请注意，混淆矩阵中的行表示真实类别，列表示预测类别。通过计算TN、FP、FN和TP，我们可以更好地评估分类器的性能。</p>
<p><strong>Filter</strong></p>
<p>接着提供一个filter：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731164221343.png" alt="image-20230731164221343"></p>
<p>然后点击apply：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230731164239239.png" alt="image-20230731164239239"></p>
<p>运行得到结果为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">=== Run information ===</span><br><span class="line"></span><br><span class="line">Scheme:       weka.classifiers.bayes.NaiveBayes </span><br><span class="line">Relation:     pima_diabetes-weka.filters.supervised.attribute.Discretize-Rfirst-last-precision6</span><br><span class="line">Instances:    768</span><br><span class="line">Attributes:   9</span><br><span class="line">              preg</span><br><span class="line">              plas</span><br><span class="line">              pres</span><br><span class="line">              skin</span><br><span class="line">              insu</span><br><span class="line">              mass</span><br><span class="line">              pedi</span><br><span class="line">              age</span><br><span class="line">              class</span><br><span class="line">Test mode:    split 75.0% train, remainder test</span><br><span class="line"></span><br><span class="line">=== Classifier model (full training set) ===</span><br><span class="line"></span><br><span class="line">Naive Bayes Classifier</span><br><span class="line"></span><br><span class="line">                              Class</span><br><span class="line">Attribute           tested_negative tested_positive</span><br><span class="line">                             (0.65)          (0.35)</span><br><span class="line">====================================================</span><br><span class="line">preg</span><br><span class="line">  &#x27;(-inf-6.5]&#x27;                 427.0           174.0</span><br><span class="line">  &#x27;(6.5-inf)&#x27;                   75.0            96.0</span><br><span class="line">  [total]                      502.0           270.0</span><br><span class="line"></span><br><span class="line">plas</span><br><span class="line">  &#x27;(-inf-99.5]&#x27;                182.0            17.0</span><br><span class="line">  &#x27;(99.5-127.5]&#x27;               211.0            79.0</span><br><span class="line">  &#x27;(127.5-154.5]&#x27;               86.0            77.0</span><br><span class="line">  &#x27;(154.5-inf)&#x27;                 25.0            99.0</span><br><span class="line">  [total]                      504.0           272.0</span><br><span class="line"></span><br><span class="line">pres</span><br><span class="line">  &#x27;All&#x27;                        501.0           269.0</span><br><span class="line">  [total]                      501.0           269.0</span><br><span class="line"></span><br><span class="line">skin</span><br><span class="line">  &#x27;All&#x27;                        501.0           269.0</span><br><span class="line">  [total]                      501.0           269.0</span><br><span class="line"></span><br><span class="line">insu</span><br><span class="line">  &#x27;(-inf-14.5]&#x27;                237.0           140.0</span><br><span class="line">  &#x27;(14.5-121]&#x27;                 165.0            28.0</span><br><span class="line">  &#x27;(121-inf)&#x27;                  101.0           103.0</span><br><span class="line">  [total]                      503.0           271.0</span><br><span class="line"></span><br><span class="line">mass</span><br><span class="line">  &#x27;(-inf-27.85]&#x27;               196.0            28.0</span><br><span class="line">  &#x27;(27.85-inf)&#x27;                306.0           242.0</span><br><span class="line">  [total]                      502.0           270.0</span><br><span class="line"></span><br><span class="line">pedi</span><br><span class="line">  &#x27;(-inf-0.5275]&#x27;              362.0           149.0</span><br><span class="line">  &#x27;(0.5275-inf)&#x27;               140.0           121.0</span><br><span class="line">  [total]                      502.0           270.0</span><br><span class="line"></span><br><span class="line">age</span><br><span class="line">  &#x27;(-inf-28.5]&#x27;                297.0            72.0</span><br><span class="line">  &#x27;(28.5-inf)&#x27;                 205.0           198.0</span><br><span class="line">  [total]                      502.0           270.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Time taken to build model: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Evaluation on test split ===</span><br><span class="line"></span><br><span class="line">Time taken to test model on test split: 0 seconds</span><br><span class="line"></span><br><span class="line">=== Summary ===</span><br><span class="line"></span><br><span class="line">Correctly Classified Instances         165               85.9375 %</span><br><span class="line">Incorrectly Classified Instances        27               14.0625 %</span><br><span class="line">Kappa statistic                          0.677 </span><br><span class="line">Mean absolute error                      0.2334</span><br><span class="line">Root mean squared error                  0.3339</span><br><span class="line">Relative absolute error                 51.9021 %</span><br><span class="line">Root relative squared error             71.2158 %</span><br><span class="line">Total Number of Instances              192     </span><br><span class="line"></span><br><span class="line">=== Detailed Accuracy By Class ===</span><br><span class="line"></span><br><span class="line">                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class</span><br><span class="line">                 0.900    0.226    0.893      0.900    0.897      0.677    0.904     0.946     tested_negative</span><br><span class="line">                 0.774    0.100    0.787      0.774    0.780      0.677    0.904     0.833     tested_positive</span><br><span class="line">Weighted Avg.    0.859    0.185    0.859      0.859    0.859      0.677    0.904     0.909     </span><br><span class="line"></span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line"></span><br><span class="line">   a   b   &lt;-- classified as</span><br><span class="line"> 117  13 |   a = tested_negative</span><br><span class="line">  14  48 |   b = tested_positive</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个运行结果是基于朴素贝叶斯分类器（NaiveBayes）对一个名为”pima_diabetes”的数据集进行训练和测试得到的。数据集包含768个实例（样本），每个实例有9个属性（特征）。</p>
<p>下面对运行结果进行解读：</p>
<ol>
<li><p>Scheme信息：</p>
<ul>
<li>使用的分类器：朴素贝叶斯分类器（NaiveBayes）</li>
<li>数据集关系：pima_diabetes-weka.filters.supervised.attribute.Discretize-Rfirst-last-precision6</li>
<li>实例数量：768</li>
<li>属性数量：9</li>
<li>测试模式：采用75.0%的数据作为训练集，其余作为测试集。</li>
</ul>
</li>
<li><p>分类器模型：</p>
<ul>
<li>该部分展示了朴素贝叶斯分类器在训练集上对每个属性进行离散化后，对各个离散化区间在各个类别（tested_negative和tested_positive）上的统计信息，包括样本数和精度。</li>
</ul>
</li>
<li><p>评估结果：</p>
<ul>
<li>正确分类实例数：165，正确率为85.9375%</li>
<li>错误分类实例数：27，错误率为14.0625%</li>
<li>Kappa统计量：0.677（0表示随机分类，1表示完全一致分类）</li>
<li>平均绝对误差：0.2334</li>
<li>均方根误差：0.3339</li>
<li>相对绝对误差：51.9021%（平均绝对误差相对于真实值的百分比）</li>
<li>根相对平方误差：71.2158%（均方根误差相对于真实值的百分比）</li>
<li>总实例数：192</li>
</ul>
</li>
<li><p>类别详细准确率：</p>
<ul>
<li>对于”tested_negative”类别，真阳性率（召回率）为0.900，假阳性率为0.226，精确率为0.893。</li>
<li>对于”tested_positive”类别，真阳性率（召回率）为0.774，假阳性率为0.100，精确率为0.787。</li>
<li>加权平均（Weighted Avg.）的指标是综合两个类别的准确率得出的。</li>
</ul>
</li>
<li><p>混淆矩阵：</p>
<ul>
<li>混淆矩阵展示了真实类别与预测类别之间的对应关系。在本例中，共有192个测试集实例。</li>
<li>对于”tested_negative”类别，其中117个实例被正确分类为”tested_negative”，13个实例被错误分类为”tested_positive”。</li>
<li>对于”tested_positive”类别，其中48个实例被正确分类为”tested_positive”，14个实例被错误分类为”tested_negative”。</li>
</ul>
</li>
</ol>
<p>综上所述，朴素贝叶斯分类器在这个数据集上表现较好，正确率为85.9375%，Kappa统计量为0.677。需要进一步优化离散化过程或者尝试其他的分类器来提高分类性能。</p>
<p>根据混淆矩阵，我们可以计算真阴性（True Negative，TN）、假阳性（False Positive，FP）、假阴性（False Negative，FN）、真阳性（True Positive，TP）的值。</p>
<p>混淆矩阵如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  a   b   &lt;-- classified as</span><br><span class="line">117  13 |   a = tested_negative</span><br><span class="line"> 14  48 |   b = tested_positive</span><br></pre></td></tr></table></figure>

<p>其中，</p>
<ul>
<li>a表示实际为”tested_negative”类别的样本数量（即负样本），</li>
<li>b表示实际为”tested_positive”类别的样本数量（即正样本）。</li>
</ul>
<p>现在我们可以计算TN、FP、FN和TP：</p>
<ul>
<li>TN：实际为”tested_negative”且被正确分类为”tested_negative”的样本数量。</li>
<li>FP：实际为”tested_negative”但被错误分类为”tested_positive”的样本数量。</li>
<li>FN：实际为”tested_positive”但被错误分类为”tested_negative”的样本数量。</li>
<li>TP：实际为”tested_positive”且被正确分类为”tested_positive”的样本数量。</li>
</ul>
<p>根据混淆矩阵中的值，我们可以得到：</p>
<ul>
<li>TN &#x3D; 117</li>
<li>FP &#x3D; 13</li>
<li>FN &#x3D; 14</li>
<li>TP &#x3D; 48</li>
</ul>
<p>下面是表格展示结果：</p>
<table>
<thead>
<tr>
<th></th>
<th>实际”tested_negative”</th>
<th>实际”tested_positive”</th>
</tr>
</thead>
<tbody><tr>
<td>预测”tested_negative”</td>
<td>TN &#x3D; 117</td>
<td>FP &#x3D; 13</td>
</tr>
<tr>
<td>预测”tested_positive”</td>
<td>FN &#x3D; 14</td>
<td>TP &#x3D; 48</td>
</tr>
</tbody></table>
<p>请注意，混淆矩阵中的行表示真实类别，列表示预测类别。通过计算TN、FP、FN和TP，我们可以更好地评估分类器的性能。</p>
<p><strong>比较</strong></p>
<p>通过比较这两个结果，我们可以看出将 filter 设置为 discretize 之后的结果相较于原先的结果，模型在测试集上的性能有所提升。现在我们来解释一下为什么将 filter 设置为 discretize 会带来这个影响：</p>
<ol>
<li><p>数据转换：将连续特征进行离散化后，使得原本连续的数值特征被转换为一系列离散的区间或类别。这样做的好处是，朴素贝叶斯分类器对于处理离散的特征更加高效和适用，因为朴素贝叶斯算法是基于概率的分类器，而概率的计算对于离散特征更简单。</p>
</li>
<li><p>解决连续特征的问题：原先的数据集中包含连续特征，朴素贝叶斯分类器在处理连续特征时可能受到数据分布的影响，特别是当数据分布不符合朴素贝叶斯假设（特征之间相互独立）时。将连续特征离散化后，能够一定程度上解决这个问题，使得模型对连续特征的处理更加稳定和有效。</p>
</li>
<li><p>数据稀疏性：在原先的结果中，朴素贝叶斯分类器在处理连续特征时可能由于数据稀疏性导致预测性能下降。而通过离散化，可以使得每个区间都有样本数据，减轻了数据稀疏性的影响，提高了模型的泛化能力。</p>
</li>
<li><p>去除异常值影响：离散化过程中，可以将异常值归入相应的区间，避免了异常值对模型的影响。</p>
</li>
<li><p>捕捉非线性关系：在某些情况下，离散化能够更好地捕捉特征与目标变量之间的非线性关系，使得模型更准确。</p>
</li>
</ol>
<p>总的来说，将 filter 设置为 discretize 的效果表现在了对连续特征的处理和模型性能的提升上。然而，离散化也可能带来一定的信息损失，需要根据具体的数据集和问题来进行调试和优化，以获得更好的模型性能。</p>
<h1 id="03-贝叶斯网络"><a href="#03-贝叶斯网络" class="headerlink" title="03-贝叶斯网络"></a>03-贝叶斯网络</h1><h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><h3 id="什么是贝叶斯网络"><a href="#什么是贝叶斯网络" class="headerlink" title="什么是贝叶斯网络"></a>什么是贝叶斯网络</h3><p>贝叶斯网络（Bayesian Network），也称为信度网络或概率图模型，是一种用图形模型来表示一组变量之间概率关系的统计模型。这种模型用于表现变量之间的依赖关系，并可以用于推断、学习和决策。</p>
<p>贝叶斯网络有两个主要部分：</p>
<ol>
<li><p><strong>结构</strong>：一个有向无环图（DAG），其中节点表示随机变量（可以是观测到的数据、未知参数或隐藏变量），边则表示变量之间的概率依赖关系。</p>
</li>
<li><p><strong>参数</strong>：给定其父变量，每个节点的概率分布。</p>
</li>
</ol>
<p>以下是贝叶斯网络的一些主要特点和应用：</p>
<ol>
<li><p><strong>因果关系</strong>：如果有一条从节点A指向节点B的边，我们可以说A是B的一个原因或父节点。这意味着知道A的状态会影响我们对B状态的信念。</p>
</li>
<li><p><strong>条件独立性</strong>：给定其父节点的状态，某节点与它的非后代节点是条件独立的。</p>
</li>
<li><p><strong>推断</strong>：给定一些节点的观测值，我们可以使用贝叶斯网络推断其他未观测节点的分布。</p>
</li>
<li><p><strong>学习</strong>：如果我们有一个数据集，我们可以使用它来学习网络的结构和&#x2F;或参数。</p>
</li>
<li><p><strong>应用</strong>：贝叶斯网络在许多领域都有广泛的应用，包括医疗诊断、机器学习、风险评估和自然语言处理等。</p>
</li>
</ol>
<p>贝叶斯网络的一个核心思想是利用贝叶斯定理来更新或计算给定观测数据时的各种不确定性或概率分布。</p>
<p>DAG 是 “Directed Acyclic Graph”（有向无环图）的缩写。它是一个由节点（或称顶点）和有向边组成的图，且满足以下条件：</p>
<ol>
<li><p><strong>有向</strong>：图中的每一条边都有一个起点和一个终点，明确地表示了方向。</p>
</li>
<li><p><strong>无环</strong>：从任何节点出发，沿着边的方向前进，永远不会回到这个节点，即不存在闭环。</p>
</li>
</ol>
<p>DAG 在许多领域中都有应用，例如：</p>
<ol>
<li><p><strong>计算机科学</strong>：DAG 用于数据依赖关系，如在优化编译器中。</p>
</li>
<li><p><strong>生物信息学</strong>：DAG 可用于表示基因或蛋白质的功能关系。</p>
</li>
<li><p><strong>贝叶斯网络</strong>：如上文所述，DAG 用于表示变量之间的因果关系。</p>
</li>
<li><p><strong>项目管理</strong>：DAG 可用于表示任务之间的先后关系，如在 PERT&#x2F;CPM 图中。</p>
</li>
<li><p><strong>数据库</strong>：DAG 可用于表示事务的依赖关系，以确定事务的提交顺序。</p>
</li>
<li><p><strong>数据流编程和并行计算</strong>：DAG 用于表示数据的流动和任务的执行顺序。</p>
</li>
</ol>
<p>简而言之，任何需要表示有向关系且关系中不存在循环的情境都可以使用 DAG。</p>
<h3 id="贝叶斯网络列子①"><a href="#贝叶斯网络列子①" class="headerlink" title="贝叶斯网络列子①"></a>贝叶斯网络列子①</h3><p><strong>一个贝叶斯网络的例子。</strong></p>
<p>当然可以，首先我们选一个经典的例子：草地湿润的原因。考虑两个因素可能导致草地湿润：下雨和喷水。</p>
<p>我们有三个随机变量：</p>
<ol>
<li>$R$：是否下雨</li>
<li>$S$：是否喷水</li>
<li>$W$：草地是否湿润</li>
</ol>
<p>依赖关系为：下雨和喷水都可以导致草地湿润，但它们是相互独立的。</p>
<p>这个模型的结构可以表示为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">R -&gt; W</span><br><span class="line">S -&gt; W</span><br></pre></td></tr></table></figure>

<p>在 Markdown 中，我们可以使用 ASCII 图形来表示这个结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">R</span><br><span class="line">|</span><br><span class="line">V</span><br><span class="line">W &lt;- S</span><br></pre></td></tr></table></figure>

<p>然后，我们为这个网络定义概率：</p>
<ol>
<li>$P(R)$：下雨的概率</li>
<li>$P(S)$：喷水的概率</li>
<li>$P(W|R,S)$：给定下雨和&#x2F;或喷水的情况下，草地湿润的概率</li>
</ol>
<p>例如：</p>
<ul>
<li>$P(R) &#x3D; 0.2$（下雨的概率为 20%）</li>
<li>$P(S) &#x3D; 0.5$（喷水的概率为 50%）</li>
<li>$P(W| R, S) &#x3D; 1$：如果下雨且喷水，则草地一定是湿的</li>
<li>$P(W| R, \neg S) &#x3D; 0.9$：如果只下雨但不喷水，草地湿润的概率为 90%</li>
<li>$P(W| \neg R, S) &#x3D; 0.8$：如果只喷水但不下雨，草地湿润的概率为 80%</li>
<li>$P(W| \neg R, \neg S) &#x3D; 0$：如果既不下雨也不喷水，草地是干的</li>
</ul>
<p>这就是贝叶斯网络的基本结构和概率分布的简化示例。在真实的应用中，贝叶斯网络可能会包含更多的节点和复杂的依赖关系。</p>
<p>使用上述贝叶斯网络和概率表，您可以进行多种概率推断。以下是一些可能的推断：</p>
<ol>
<li><p><strong>边际概率计算</strong>：计算某事件发生的概率，无需考虑其他事件。例如，您可以直接使用给出的 $P(R)$ 和  $P(S)$ 来得知下雨或喷水的概率。</p>
</li>
<li><p><strong>联合概率计算</strong>：计算两个或多个事件同时发生的概率。例如，计算既下雨又喷水的概率：$P(R, S) &#x3D; P(R) \times P(S) &#x3D; 0.2 \times 0.5 &#x3D; 0.1$</p>
</li>
<li><p><strong>条件概率推断</strong>：基于某些已知情况推断其他事件的概率。例如，已知草地湿润，我们想知道是因为下雨还是喷水。为了计算这个，您需要使用贝叶斯定理和全概率定理。</p>
<p>以计算已知草地湿润时下雨的概率为例：</p>
<p>$P(R|W) &#x3D; \frac{P(W|R) \times P(R)}{P(W)}$</p>
<p>其中 $P(W)$ 可以通过全概率定理计算：</p>
<p>$P(W) &#x3D; P(W|R, S) \times P(R) \times P(S) + P(W|R, \neg S) \times P(R) \times (1-P(S)) + P(W| \neg R, S) \times (1-P(R)) \times P(S) + P(W| \neg R, \neg S) \times (1-P(R)) \times (1-P(S))$</p>
</li>
<li><p><strong>预测</strong>：基于某些已知情况预测其他事件的概率。例如，已知今天下雨且喷水，我们可以使用 $P(W|R, S)$ 预测草地湿润的概率。</p>
</li>
<li><p><strong>因果推断</strong>：评估一个事件对另一个事件的概率产生的影响。例如，如果我们想知道不喷水（也许是为了节约水）对草地湿润的影响，我们可以比较 $P(W| R, S)$ 和 $P(W| R, \neg S)$。</p>
</li>
</ol>
<p>这只是使用贝叶斯网络进行推断的一些基本方式。在实际应用中，可能需要使用更复杂的算法，特别是当网络结构和关联概率变得更加复杂时。</p>
<p><strong>补充知识：全概率公式。</strong></p>
<p>全概率公式是概率论中的一个基本公式，它为计算复杂事件的概率提供了一个框架。具体地说，全概率公式处理了在给定一组互斥且完备的事件的条件下，计算另一事件概率的问题。</p>
<p>设事件组 $B_1, B_2, \dots, B_n$ 是样本空间 $S$ 的一个划分，即它们互斥并且其并集为整个样本空间。也就是说：</p>
<ol>
<li>$B_i \cap B_j &#x3D; \emptyset$ 对于所有的 $i \neq j$。</li>
<li>$\cup_{i&#x3D;1}^{n} B_i &#x3D; S$。</li>
</ol>
<p>对于任何事件 ( A ) 在样本空间中，全概率公式为：</p>
<p>$P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(A|B_i)P(B_i)$</p>
<p>证明：</p>
<p>考虑事件 $A$。它可以被表示为与 $B_1, B_2, \dots, B_n$ 的交集的并集，即：</p>
<p>$A &#x3D; (A \cap B_1) \cup (A \cap B_2) \cup \dots \cup (A \cap B_n)$</p>
<p>因为 $B_1, B_2, \dots, B_n$ 互斥，所以 $A \cap B_i$ 和 $A \cap B_j$ 也是互斥的，对于所有的 $i \neq j$。</p>
<p>现在我们可以计算 ( A ) 的概率：</p>
<p>$P(A) &#x3D; P((A \cap B_1) \cup (A \cap B_2) \cup \dots \cup (A \cap B_n))$</p>
<p>由于这些交集是互斥的，所以并集的概率就是各个交集概率的和：</p>
<p>$P(A) &#x3D; P(A \cap B_1) + P(A \cap B_2) + \dots + P(A \cap B_n)$</p>
<p>使用条件概率的定义，我们有：</p>
<p>$P(A \cap B_i) &#x3D; P(A|B_i)P(B_i)$</p>
<p>代入上述方程，我们得到：</p>
<p>$P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(A|B_i)P(B_i)$</p>
<p>这就完成了全概率公式的证明。</p>
<p>在全概率公式中，事件的互斥性和完备性是关键的，因为它们确保了在给定的样本空间中，我们正确且完整地考虑了所有可能的情况。让我们详细解释这两个条件的重要性：</p>
<ol>
<li><p><strong>互斥 (Mutually Exclusive)</strong>: 互斥意味着任意两个事件不能同时发生。这确保了当我们在全概率公式中对事件的概率进行求和时，没有任何重叠或双重计数。如果事件不是互斥的，我们可能会错误地将某些事件计数多次，从而导致最终的概率超过1，这是不合逻辑的。</p>
</li>
<li><p><strong>完备 (Collectively Exhaustive)</strong>: 完备意味着这组事件覆盖了所有可能的情况，即它们的并集构成了整个样本空间。这确保了我们在计算全概率时考虑了所有可能的情况，没有遗漏。如果事件集不是完备的，那么可能会有某些未考虑的情况，导致我们的概率计算低于实际概率。</p>
</li>
</ol>
<p>为了更好地理解，考虑一个简单的例子：掷一个公正的六面骰子。设想以下三个事件：</p>
<ul>
<li>$B_1$：掷出的是奇数。</li>
<li>$B_2$：掷出的是偶数。</li>
<li>$B_3$：掷出的是7。</li>
</ul>
<p>在这里，$B_1$和$B_2$是互斥的，因为一个数字不能同时是奇数和偶数。而$B_1$、$B_2$和 $B_3$ 都与其他事件互斥。但这组事件不是完备的，因为7不是一个有效的掷骰子结果。此外，只有 $B_1$和$B_2$就已经是完备的，因为它们的并集涵盖了所有可能的掷骰子结果。如果我们使用这三个事件来应用全概率公式，我们会得到一个错误的结果，因为我们考虑了一个不可能的事件$B_3$。因此，确保事件是互斥并且完备的是至关重要的。</p>
<h3 id="贝叶斯网络例子②"><a href="#贝叶斯网络例子②" class="headerlink" title="贝叶斯网络例子②"></a>贝叶斯网络例子②</h3><p><strong>这是来自于课件的例子。</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918181604329.png" alt="image-20230918181604329"></p>
<p>首先这个贝叶斯网络是有向无环的。如果在有向图中，节点 $a$ 指向节点 $b$ 并且节点 $b$ 也指向节点 $a$，那么这构成了一个环。在图论中，一个环是一个非空的有限集合，它首尾相连的一系列边组成，并且首尾是同一个节点。在您描述的情况下，从节点 $a$ 到节点 $b$，然后再从节点 $b$ 返回到节点 $a$，这构成了一个长度为2的环。此外，应该注意的是，有环的图在某些上下文中可能不是理想的。例如，在贝叶斯网络中，图是有向无环图 (DAG)，意味着任何两个节点之间都不应存在这种互相指向的环。</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918181938152.png" alt="image-20230918181938152"></p>
<p>如果A指向了B，那么A则是B的Parent Node，而箭头的方向则表示了因果关系，以及A has a direct influence on B。</p>
<p>接着是一个概率表。</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918182117265.png" alt="image-20230918182117265"></p>
<p>P(A)给出了A分别为True和False的可能性，而P(B|A)则给出了A分别为True和False的情况之下，B分别为True和False的可能性。</p>
<p><strong>Conditional Probability（条件概率）知识点补充。</strong></p>
<p>条件概率度量了在给定一个事件发生的情况下，另一个事件发生的概率。换句话说，它是在某一条件下某一事件的概率。</p>
<p>记作 $P(A|B)$，读作“给定 $B$ 的条件下 $A$ 的概率”，它的定义为：</p>
<p>$P(A|B) &#x3D; \frac{P(A \cap B)}{P(B)}$</p>
<p>其中：</p>
<ul>
<li>$P(A \cap B)$ 是 $A$ 和  $B$ 同时发生的概率（联合概率）。</li>
<li>$P(B)$ 是事件 $B$ 发生的概率。</li>
</ul>
<p>注意：$P(B) \neq 0$，因为我们不能在 $B$ 不可能发生的情况下条件 $B$。</p>
<p>例子：假设我们有一副扑克牌，并想要知道在已知我们拿到了一张红色的牌（红桃或者方块）的情况下，这张牌是一个王$K$的概率。</p>
<p>设事件 $A$ 是拿到王，事件 $B$ 是拿到红色的牌。我们知道：</p>
<ul>
<li>$P(A)$：任意拿到王的概率是 $\frac{4}{52}$（因为有4张王）。</li>
<li>$P(B)$：任意拿到红色牌的概率是 $\frac{26}{52} &#x3D; \frac{1}{2}$（因为有26张红色的牌）。</li>
<li>$P(A \cap B) $：拿到红色的王的概率是 $\frac{2}{52}$（因为有2张红色的王）。</li>
</ul>
<p>因此，给定我们拿到了一张红色的牌，拿到王的概率是：</p>
<p>$P(A|B) &#x3D; \frac{P(A \cap B)}{P(B)} &#x3D; \frac{\frac{2}{52}}{\frac{1}{2}} &#x3D; \frac{2}{26} &#x3D; \frac{1}{13}$</p>
<p>所以，如果我们知道我们拿到的是红色的牌，那么这张牌是王的概率是 $\frac{1}{13}$。</p>
<h3 id="条件独立性"><a href="#条件独立性" class="headerlink" title="条件独立性"></a>条件独立性</h3><p><strong>什么是条件独立性</strong></p>
<p>条件独立是概率论和统计学中的一个重要概念，它描述了在给定某个或某些条件时，两个或多个随机变量之间的关系。</p>
<p><strong>定义</strong>：设有三个随机变量 $A$, $B$, 和 $C$。如果给定 $C$ 的情况下 $A$ 与 $B$ 是独立的，那么我们说在给定 $C$ 的条件下，$A$ 与 $B$ 是条件独立的。数学上，这可以表示为：</p>
<p>$P(A, B | C) &#x3D; P(A | C) \times P(B | C)$</p>
<p>或者等价地：</p>
<p>$P(A | B, C) &#x3D; P(A | C)$</p>
<p>这意味着，当我们知道 $C$ 的信息时，$A$ 的概率分布不会被 $B$ 的任何信息所影响（反之亦然）。</p>
<p>使用文氏图来进行表示：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918183937958.png" alt="image-20230918183937958"></p>
<p><strong>关于上图的知识点补充</strong></p>
<p>不，$A$ 和 $B$ 是独立的并不意味着它们没有交集。在概率论中，“独立”和“没有交集”是两个完全不同的概念。</p>
<p>让我们先解释一下这两个概念：</p>
<ol>
<li><p><strong>独立</strong>：当两个事件 $A$ 和 $B$ 是独立的，这意味着一个事件的发生不影响另一个事件的发生。数学上，事件 $A$ 和 $B$ 是独立的当且仅当 $P(A \cap B) &#x3D; P(A) \times P(B)$。</p>
</li>
<li><p><strong>没有交集</strong>：如果事件 $A$ 和 $B$ 之间没有任何公共结果，那么它们是互斥的或不相交的。这意味着两个事件不可能同时发生。数学上，如果 $A$ 和 $B$ 是互斥的，那么 $P(A \cap B) &#x3D; 0$。</p>
</li>
</ol>
<p>从上述定义中，我们可以看到，“独立”和“没有交集”是不同的。独立的事件可以有交集（其实，它们通常会有交集，除非其中一个事件的概率是0），而互斥的事件永远不可能同时发生。</p>
<p>举个简单的例子：考虑一次公正的掷骰子。事件 $A$ 是得到一个偶数，事件$B$ 是得到一个大于3的数。这两个事件明显有交集（即4和6），但它们是独立的，因为知道一次掷骰子得到偶数并不影响它得到大于3的数的概率，反之亦然。</p>
<p><strong>为什么条件独立性重要？</strong></p>
<p>在很多情况下，两个随机变量可能是相关的，但当我们知道第三个变量的信息时，这两个变量可能变得独立。为了更清楚地理解这一点，让我们看一个经典的例子：</p>
<p><strong>例子</strong>：考虑两个随机变量：一个人是否带伞$(A)$和当天是否下雨$(B)$。通常情况下，这两个变量是相关的，因为人们更可能在下雨的日子带伞。但现在，考虑第三个变量：这个人是否听了天气预报$(C)$。</p>
<p>如果我们知道这个人听了预报，并且预报说当天会下雨，那么不论这个人是否带伞，我们都已经知道了关于是否下雨的信息。在这种情况下，知道他是否带伞并不会给我们提供更多关于是否真的下雨的信息。因此，在知道天气预报的条件下，带伞与否与当天是否下雨是条件独立的。</p>
<p>这种条件独立性在许多领域，特别是在机器学习和统计建模中，都有重要应用。例如，它是贝叶斯网络中关键概念的基础。</p>
<p>如果现在的情况如下所示：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A → B</span><br><span class="line">B → D</span><br><span class="line">C → D</span><br></pre></td></tr></table></figure>

<p>分析如下所示：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918183819625.png" alt="image-20230918183819625"></p>
<p><strong>条件独立和绝对独立（或简单地称为“独立”）是两个不同的概念。</strong></p>
<p><strong>绝对独立</strong>：两个事件 $A$ 和 $B$ 是独立的，当且仅当 $P(A \cap B) &#x3D; P(A) \times P(B)$。这意味着事件 $A$ 的发生不会影响事件 $B$ 的发生，反之亦然。</p>
<p><strong>条件独立</strong>：给定第三个事件 $C$，如果 $A$ 和 $B$ 是在 $C$ 的条件下独立的，那么我们可以写作 $P(A \cap B | C) &#x3D; P(A | C) \times P(B | C)$。这意味着，当事件 $C$ 已知时，知道 $A$ 的情况并不会为我们提供关于 $B$ 的任何额外信息，反之亦然。</p>
<p>如您所述，两个事件可以是条件独立的，但在没有给定条件的情况下，它们可能并不是绝对独立的。同样地，即使两个事件在没有给定条件的情况下是绝对独立的，给定某些条件后，它们可能不再是条件独立的。</p>
<p>举个经典的例子：考虑两个随机变量，一个人是否带伞 $A$ 和当天是否下雨 $B$。通常，这两个变量是相关的，因为人们更可能在下雨的日子带伞。但如果引入第三个变量：这个人是否听了天气预报 $C$。如果我们知道这个人听了预报并且预报说当天会下雨，那么不论这个人是否带伞，我们都已经知道了关于是否下雨的信息。因此，在知道天气预报的条件下，带伞与否与当天是否下雨是条件独立的，尽管在没有这个条件的情况下，它们是相关的。</p>
<h3 id="马尔科夫条件"><a href="#马尔科夫条件" class="headerlink" title="马尔科夫条件"></a>马尔科夫条件</h3><p>马尔可夫条件（The Markov Condition）和条件独立性（Conditional Independence）在贝叶斯网络中是密切相关的概念。为了解释它们之间的关系，我们首先需要明确每个概念的定义。</p>
<ol>
<li><p><strong>马尔可夫条件（The Markov Condition）</strong>：对于贝叶斯网络中的任何节点 $X$，给定其父节点，它与非后代节点是条件独立的。</p>
</li>
<li><p><strong>条件独立性（Conditional Independence）</strong>：给定某些事件或变量 $C$，如果两个事件或变量 $A$ 和 $B$ 是条件独立的，这意味着知道 $C$ 的情况下，$A$ 的信息不提供关于 $B$ 的任何额外信息，反之亦然。数学上，这可以表示为 $P(A, B|C) &#x3D; P(A|C) \times P(B|C)$。</p>
</li>
</ol>
<p><strong>关系</strong>：</p>
<p>马尔可夫条件实际上是条件独立性在贝叶斯网络中的一个特定应用。它确保了，当我们知道一个节点的父节点的值时，该节点与其非后代节点是条件独立的。这种独立性质极大地简化了贝叶斯网络的计算，因为我们可以利用条件概率和乘法规则来计算联合分布，而不需要知道整个网络的所有细节。</p>
<p>所以，马尔可夫条件为贝叶斯网络提供了一种结构，使得我们只需要关心节点与其直接父节点之间的关系，而不是整个网络的全局关系。而条件独立性是这种结构背后的核心数学概念。</p>
<p>马尔科夫条件是贝叶斯网络中的一个核心概念。考虑以下示例：</p>
<p>假设我们有一个关于学生学术表现的贝叶斯网络。在这个网络中，我们有以下几个节点：</p>
<ul>
<li><strong>智力（Intelligence）</strong>：学生的智力，可以是”高”或”低”。</li>
<li><strong>难度（Difficulty）</strong>：考试的难度，可以是”容易”或”困难”。</li>
<li><strong>成绩（Grade）</strong>：学生的考试成绩，可以是”A”、”B”或”C”。</li>
</ul>
<p>我们可以认为成绩（Grade）直接取决于智力（Intelligence）和考试的难度（Difficulty）。因此，我们可以有以下的关系：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Intelligence -&gt; Grade</span><br><span class="line">Difficulty -&gt; Grade</span><br></pre></td></tr></table></figure>

<p>在这个模型中，马尔科夫条件告诉我们：</p>
<ol>
<li><p>给定成绩（Grade），智力（Intelligence）与难度（Difficulty）是条件独立的。这意味着，如果我们知道了一个学生的成绩，那么知道他的智力水平并不会为我们提供任何关于考试难度的额外信息，反之亦然。</p>
</li>
<li><p>另外，考虑智力（Intelligence）节点。因为它没有父节点，所以它与任何其他非后代节点（在这里是难度）是独立的。</p>
</li>
</ol>
<p>这个简单的例子展示了如何在贝叶斯网络中应用马尔科夫条件，以及它是如何利用局部关系来简化复杂网络中的条件独立性问题的。</p>
<h3 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h3><p><strong>联合概率分布</strong>描述了两个或更多随机变量同时取各种可能值的概率。对于离散的随机变量，它给出了所有可能的值组合的概率。对于连续的随机变量，它则是关于这些变量的多变量概率密度函数。</p>
<p>举个简单的例子来理解：</p>
<p>假设有两个离散随机变量 $X$ 和 $Y$，它们的取值分别为 ${x_1, x_2}$ 和 $y_1, y_2$。那么，它们的联合概率分布可以表示为一个表格，显示了它们各种值组合的概率：</p>
<table>
<thead>
<tr>
<th>$X \backslash Y$</th>
<th>$y_1$</th>
<th>$y_2$</th>
</tr>
</thead>
<tbody><tr>
<td>$x_1$</td>
<td>$P(X &#x3D; x_1, Y &#x3D; y_1)$</td>
<td>$P(X &#x3D; x_1, Y &#x3D; y_2)$</td>
</tr>
<tr>
<td>$x_2$</td>
<td>$P(X &#x3D; x_2, Y &#x3D; y_1)$</td>
<td>$P(X &#x3D; x_2, Y &#x3D; y_2)$</td>
</tr>
</tbody></table>
<p>在这个表格中，每个单元格的概率值表示了 $X$ 和 $Y$ 同时取对应值的概率。例如，$P(X &#x3D; x_1, Y &#x3D; y_2)$ 表示 $X$ 取值 $x_1$ 和 $Y$ 取值 $y_2$ 时的联合概率。</p>
<p>知道联合概率分布可以帮助我们计算边缘概率和条件概率，以及多个随机变量之间的各种关系和依赖性。</p>
<p>由于马尔可夫条件，我们可以使用以下公式计算贝叶斯网络中所有变量 $X_1, \ldots, X_n$ 的联合概率分布：</p>
<p>$P(X_1 &#x3D; x_1, \ldots, X_n &#x3D; x_n) &#x3D; \prod_{i&#x3D;1}^{n} P(X_i &#x3D; x_i \mid Parents(X_i))$</p>
<p>这个公式的意思是：</p>
<ul>
<li>贝叶斯网络中所有变量的联合概率分布可以由网络中每个节点的条件概率之积表示。</li>
<li>具体来说，对于网络中的每个节点 $X_i$，我们考虑其在给定其父节点（即$Parents(X_i)$值的条件下的概率 $P(X_i &#x3D; x_i \mid Parents(X_i))$。</li>
<li>我们将这些条件概率相乘，得到整个网络的联合概率分布。</li>
</ul>
<p>这基于马尔可夫条件的理念，即给定其父节点的值，节点与其所有非后代节点是条件独立的。这使得我们可以使用上面的公式简化整个贝叶斯网络的联合概率分布的计算。</p>
<p>首先，考虑一个简单的情形。假设我们只有两个变量 $X$ 和 $Y$，并且 $Y$ 依赖于 $X$。根据贝叶斯网络，我们有：</p>
<p>$P(X, Y) &#x3D; P(X) \times P(Y|X)$</p>
<p>这是直观的，因为 $Y$ 的出现依赖于 $X$。</p>
<p>现在，考虑一个稍微复杂一点的情形。假设我们有三个变量：$X$, $Y$, 和 $Z$。其中 $Y$ 依赖于 $X$，而 $Z$ 依赖于 $X$ 和 $Y$。根据贝叶斯网络，我们有：</p>
<p>$P(X, Y, Z) &#x3D; P(X) \times P(Y|X) \times P(Z|X, Y)$</p>
<p>这是因为，为了知道 $Z$ 发生的概率，我们需要同时考虑 $X$ 和 $Y$。</p>
<p>现在，我们可以用归纳法进行总体证明。假设上述公式对 $n-1$ 个变量成立，我们需要证明它对 $n$ 个变量也成立。</p>
<p>考虑一个贝叶斯网络，其中 $X_n$ 依赖于一组变量 $Parents(X_n)$。因此，我们有：</p>
<p>$P(X_1, \ldots, X_{n-1}, X_n) &#x3D; P(X_1, \ldots, X_{n-1}) \times P(X_n|Parents(X_n))$</p>
<p>由于我们已经假设公式对 $n-1$ 个变量成立，所以：</p>
<p>$P(X_1, \ldots, X_{n-1}) &#x3D; \prod_{i&#x3D;1}^{n-1} P(X_i|Parents(X_i))$</p>
<p>代入上面的公式，我们得到：</p>
<p>$P(X_1, \ldots, X_n) &#x3D; \prod_{i&#x3D;1}^{n} P(X_i|Parents(X_i))$</p>
<p>这就完成了归纳证明。</p>
<p>总之，马尔可夫条件允许我们将一个大型贝叶斯网络的联合概率分布分解为每个节点给定其父节点的条件概率的乘积。</p>
<p>这是一个例子：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918200814802.png" alt="image-20230918200814802"></p>
<h3 id="贝叶斯网络推理"><a href="#贝叶斯网络推理" class="headerlink" title="贝叶斯网络推理"></a>贝叶斯网络推理</h3><p>“Inference” 在计算机科学和其他学科中是一个广泛的概念，但基本上它指的是从给定的信息中得出结论或预测。这里是几个上下文中的 “inference” 的定义：</p>
<ol>
<li><p><strong>统计和概率论</strong>：推断是根据样本数据得出关于总体的结论的过程。例如，我们可以从一个小样本中推断整个人口的平均值。常用的方法包括假设检验和置信区间。</p>
</li>
<li><p><strong>逻辑</strong>：在逻辑中，推断是从一个或多个前提出发得出结论的过程。</p>
</li>
<li><p><strong>机器学习和人工智能</strong>：在这个上下文中，推断通常指的是已经训练好的模型给出预测或决策的过程。例如，在训练了一个图像识别模型后，使用该模型来识别新图像中的对象就是推断过程。</p>
</li>
<li><p><strong>贝叶斯网络</strong>：推断是根据已知的证据变量来计算某些未知变量的后验概率的过程。</p>
</li>
<li><p><strong>计算机科学 - 编程</strong>：在某些编程语境中，类型推断指的是编译器或解释器自动确定表达式的类型，而不是由程序员明确指定。</p>
</li>
</ol>
<p>每个上下文中的推断都涉及到从已知信息中得出结论，但具体的方法和含义可能会有所不同。</p>
<p>在这里：使用贝叶斯网络来计算概率被称为推断。</p>
<p>贝叶斯网络是一个表示变量之间概率关系的图模型。在这个网络中，节点代表随机变量，而边则代表变量之间的概率依赖关系。每个节点都有一个与之关联的条件概率分布。</p>
<p><strong>推断</strong> 是贝叶斯网络中的一种关键操作，涉及到基于已知证据来计算其他变量的概率。简而言之，给定某些节点（即已知变量或观测）的状态，我们想知道其他节点（即未知变量或查询）的概率分布。</p>
<p>一般来说，贝叶斯网络推断的问题是NP-hard的（与图的大小成指数关系）。</p>
<p><strong>精确推断</strong>：这些方法旨在计算与查询变量相关的确切概率分布。</p>
<ul>
<li><strong>概率和马尔可夫条件</strong>：基于贝叶斯网络的基本概率性质和马尔可夫条件进行推断。</li>
<li><strong>变量消除</strong>：一种基于求和和乘法操作来消除非查询变量的技术。</li>
<li><strong>聚类 &#x2F; 连接树算法</strong>：将贝叶斯网络转化为一种特定的数据结构，如连接树，然后在此结构上进行推断。</li>
</ul>
<p><strong>近似推断</strong>：当精确推断在计算上变得不切实际时，近似推断方法可以给出概率分布的近似估计。</p>
<ul>
<li><strong>随机模拟 &#x2F; 抽样方法</strong>：通过随机抽样从概率分布中得到近似解。</li>
<li><strong>马尔可夫链蒙特卡洛方法</strong>：一种使用马尔可夫链进行随机抽样的技术。</li>
<li><strong>遗传算法</strong>：模拟自然选择过程来逼近最优解的搜索算法。</li>
<li><strong>神经网络</strong>：可以模拟贝叶斯网络并进行近似推断的模型。</li>
<li><strong>模拟退火</strong>：一种全局优化方法，模拟金属冷却和退火的过程。</li>
<li><strong>平均场理论</strong>：一种物理启发式的方法，用于近似复杂系统的行为。</li>
</ul>
<p>例如，考虑一个贝叶斯网络，该网络描述了天气、交通状况和一个人是否迟到的关系。如果我们知道今天的天气（例如下雨）和交通状况（例如塞车），我们可以使用推断来估计这个人迟到的概率。</p>
<p>这是一个例子：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918220300222.png" alt="image-20230918220300222"></p>
<p>Burglary和Earthquake是绝对独立的，这两者没有一者指向另一者的路径。而JohnCalls和MaryCalls对Burglary和Earthquake而言是conditional independent。</p>
<p>贝叶斯网络推理可以进行：</p>
<ul>
<li>诊断（证据、溯因）：从结果到原因<ul>
<li>$P(Buglary|JonhCalls), P(B|J)&#x3D;0.016$</li>
<li>$P(B|J,M)&#x3D;0.29$</li>
<li>$P(A|J,M)&#x3D;0.76$</li>
</ul>
</li>
<li>因果（预测）：从原因到结果<ul>
<li>$P(J|B)&#x3D;0.86$</li>
<li>$P(M|B)&#x3D;0.67$</li>
</ul>
</li>
<li>因果关系（解释掉）：共同效应<ul>
<li>$P(B|A)&#x3D;0.38$</li>
<li>$P(B|A, E)&#x3D;0.003$</li>
</ul>
</li>
<li>混合：以上两种或两种以上的组合<ul>
<li>$P(A|J,E’)&#x3D;0.03$</li>
<li>$P(B|J,E’)&#x3D;0.017$</li>
</ul>
</li>
</ul>
<p><strong>诊断（证据、溯因）：从结果到原因的例子。</strong></p>
<p>求在J为True的情况之下，因为B的可能性有多少？即：$P(B│J &#x3D; True)&#x3D;P(B,J)&#x2F;P(J) $</p>
<p>首先可以得出：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222410936.png" alt="image-20230918222410936"></p>
<p>接着可以计算：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222439230.png" alt="image-20230918222439230"></p>
<p>然后求得 $P(B, J)$的数值：$P(B,J)&#x3D;0.9∗0.00095+0.005∗0.00005&#x3D;0.00086$</p>
<p>再接着计算P(J)：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918222721893.png" alt="image-20230918222721893"></p>
<p>最后得到最终的结果：$P(B│J)&#x3D;P(B,J)&#x2F;P(J) &#x3D;0.00086&#x2F;0.05212&#x3D;0.016$。</p>
<p><strong>上一个例子的一些补充的知识点。</strong></p>
<p>$P(A, B, C) $表示随机变量 $A$、$B$ 和 $C$ 的联合概率分布。简单来说，这表示 $A$、$B$ 和 $C$ 同时取特定值的概率。例如，如果 $A$、$B$ 和 $C$ 都是二元变量，那么 $P(A &#x3D; 1, B &#x3D; 0, C &#x3D; 1)$ 就表示 $A$ 取值为 1，$B$ 取值为 0，$C$ 取值为 1 的概率。联合概率分布描述了所有变量的所有可能组合的概率。对于离散随机变量，这通常可以通过一个概率表表示，该表列出了每种可能的变量组合和相应的概率值。</p>
<p>为什么$P(A, B) &#x3D; P(A, B, C) + P(A, B, C’)$</p>
<p>这是根据概率的加法规则和边际化（marginalization）过程得出的。为了更清楚地解释，我们从基本的定义开始。</p>
<p>首先，$P(A, B)$ 是随机变量 $A$ 和 $B$ 的联合概率分布，而 $P(A, B, C)$ 和 $P(A, B, C’)$ 分别是 $C$ 取其可能值或其补（不取该值）时的 $A$、$B$ 和 $C$ 的联合概率分布。现在，$P(A, B)$ 为 $A$ 和 $B$ 同时取某个值的概率，而不考虑 $C$ 的取值。要从 $P(A, B, C)$ 和 $P(A, B, C’) $ 得到 $P(A, B)$，我们需要对 $C$ 的所有可能值进行求和，这一过程叫做边际化。</p>
<p>公式如下：$P(A, B) &#x3D; \sum_{c} P(A, B, C&#x3D;c)$，其中 $c$ 是 $C$ 的所有可能值。</p>
<p>在二元情况（例如，C只能取0或1）中，该公式变为：$P(A, B) &#x3D; P(A, B, C&#x3D;0) + P(A, B, C&#x3D;1)$</p>
<p>或者使用 $C$和 $C’$ 的表示法其中 $C’$ 表示 $C$ 不取某个特定值）：$P(A, B) &#x3D; P(A, B, C) + P(A, B, C’)$</p>
<p>这就是为什么 $P(A, B)$ 是 $P(A, B, C)$ 和 $P(A, B, C’)$ 的和的原因。</p>
<p><strong>推理的另一个例子：从原因到结果</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918223847500.png" alt="image-20230918223847500"></p>
<p><strong>再一个例子：共同效应</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918223954876.png" alt="image-20230918223954876"></p>
<h2 id="Practical-贝叶斯-gRain"><a href="#Practical-贝叶斯-gRain" class="headerlink" title="Practical - 贝叶斯 gRain"></a>Practical - 贝叶斯 gRain</h2><h3 id="必备的package和依赖"><a href="#必备的package和依赖" class="headerlink" title="必备的package和依赖"></a>必备的package和依赖</h3><p>首先，安装必备的package：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918230719090.png" alt="image-20230918230719090"></p>
<p>接着导入：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> library<span class="punctuation">(</span>gRain<span class="punctuation">)</span></span><br><span class="line">载入需要的程辑包：gRbase</span><br><span class="line">Error<span class="operator">:</span> package or namespace load failed <span class="keyword">for</span> ‘gRbase’ <span class="keyword">in</span> loadNamespace<span class="punctuation">(</span>j <span class="operator">&lt;-</span> imp<span class="punctuation">[[</span><span class="number">1L</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span>lib.loc<span class="punctuation">,</span> .libPaths<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> versionCheck <span class="operator">=</span> vI<span class="punctuation">[[</span>j<span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line"> 不存在叫‘Rgraphviz’这个名字的程辑包</span><br><span class="line">Error<span class="operator">:</span> 无法载入程辑包‘gRbase’</span><br></pre></td></tr></table></figure>

<p>这是因为gRbase需要Rgraphviz。</p>
<p>先安装必备的包：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="keyword">if</span> <span class="punctuation">(</span><span class="operator">!</span>requireNamespace<span class="punctuation">(</span><span class="string">&quot;BiocManager&quot;</span><span class="punctuation">,</span> quietly <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">+</span>     install.packages<span class="punctuation">(</span><span class="string">&quot;BiocManager&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> </span><br><span class="line"><span class="operator">&gt;</span> BiocManager<span class="operator">::</span>install<span class="punctuation">(</span><span class="string">&quot;Rgraphviz&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="string">&#x27;getOption(&quot;repos&quot;)&#x27;</span> replaces Bioconductor standard repositories<span class="punctuation">,</span> see</span><br><span class="line"><span class="string">&#x27;help(&quot;repositories&quot;, package = &quot;BiocManager&quot;)&#x27;</span> <span class="keyword">for</span> details.</span><br><span class="line">Replacement repositories<span class="operator">:</span></span><br><span class="line">    CRAN<span class="operator">:</span> https<span class="operator">:</span><span class="operator">/</span><span class="operator">/</span>cran.rstudio.com<span class="operator">/</span></span><br><span class="line">Bioconductor version <span class="number">3.17</span> <span class="punctuation">(</span>BiocManager <span class="number">1.30</span>.22<span class="punctuation">)</span><span class="punctuation">,</span> R <span class="number">4.3</span>.1 <span class="punctuation">(</span><span class="number">2023</span><span class="operator">-</span><span class="number">06</span><span class="operator">-</span><span class="number">16</span> ucrt<span class="punctuation">)</span></span><br><span class="line">Installing package<span class="punctuation">(</span>s<span class="punctuation">)</span> <span class="string">&#x27;Rgraphviz&#x27;</span></span><br><span class="line">trying URL <span class="string">&#x27;https://bioconductor.org/packages/3.17/bioc/bin/windows/contrib/4.3/Rgraphviz_2.44.0.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">1457266</span> bytes <span class="punctuation">(</span><span class="number">1.4</span> MB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">1.4</span> MB</span><br><span class="line"></span><br><span class="line">程序包‘Rgraphviz’打开成功，MD5和检查也通过</span><br><span class="line"></span><br><span class="line">下载的二进制程序包在</span><br><span class="line">	C<span class="operator">:</span><span class="punctuation">\</span>Users<span class="punctuation">\</span>yejiu<span class="punctuation">\</span>AppData<span class="punctuation">\</span>Local<span class="punctuation">\</span>Temp<span class="punctuation">\</span>Rtmp2t3ab0<span class="punctuation">\</span>downloaded_packages里</span><br><span class="line">Old packages<span class="operator">:</span> <span class="string">&#x27;dplyr&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;ggplot2&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;htmltools&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;purrr&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;rmarkdown&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;xfun&#x27;</span></span><br><span class="line">Update <span class="built_in">all</span><span class="operator">/</span>some<span class="operator">/</span>none<span class="operator">?</span> <span class="punctuation">[</span>a<span class="operator">/</span>s<span class="operator">/</span>n<span class="punctuation">]</span><span class="operator">:</span> </span><br><span class="line">Update <span class="built_in">all</span><span class="operator">/</span>some<span class="operator">/</span>none<span class="operator">?</span> <span class="punctuation">[</span>a<span class="operator">/</span>s<span class="operator">/</span>n<span class="punctuation">]</span><span class="operator">:</span> </span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">  有二进制版本的，但源代码版本是后来的<span class="operator">:</span></span><br><span class="line">          binary source needs_compilation</span><br><span class="line">rmarkdown   <span class="number">2.24</span>   <span class="number">2.25</span>             <span class="literal">FALSE</span></span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/dplyr_1.1.3.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">1553163</span> bytes <span class="punctuation">(</span><span class="number">1.5</span> MB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">1.5</span> MB</span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/ggplot2_3.4.3.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">3326992</span> bytes <span class="punctuation">(</span><span class="number">3.2</span> MB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">3.2</span> MB</span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/htmltools_0.5.6.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">355449</span> bytes <span class="punctuation">(</span><span class="number">347</span> KB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">347</span> KB</span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/purrr_1.0.2.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">498372</span> bytes <span class="punctuation">(</span><span class="number">486</span> KB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">486</span> KB</span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/xfun_0.40.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">434168</span> bytes <span class="punctuation">(</span><span class="number">423</span> KB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">423</span> KB</span><br><span class="line"></span><br><span class="line">程序包‘dplyr’打开成功，MD5和检查也通过</span><br><span class="line">程序包‘ggplot2’打开成功，MD5和检查也通过</span><br><span class="line">程序包‘htmltools’打开成功，MD5和检查也通过</span><br><span class="line">程序包‘purrr’打开成功，MD5和检查也通过</span><br><span class="line">程序包‘xfun’打开成功，MD5和检查也通过</span><br><span class="line"></span><br><span class="line">下载的二进制程序包在</span><br><span class="line">	C<span class="operator">:</span><span class="punctuation">\</span>Users<span class="punctuation">\</span>yejiu<span class="punctuation">\</span>AppData<span class="punctuation">\</span>Local<span class="punctuation">\</span>Temp<span class="punctuation">\</span>Rtmp2t3ab0<span class="punctuation">\</span>downloaded_packages里</span><br><span class="line">安装源码包‘rmarkdown’</span><br><span class="line"></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/src/contrib/rmarkdown_2.25.tar.gz&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/x-gzip&#x27;</span> <span class="built_in">length</span> <span class="number">2188934</span> bytes <span class="punctuation">(</span><span class="number">2.1</span> MB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">2.1</span> MB</span><br><span class="line"></span><br><span class="line"><span class="operator">*</span> installing <span class="operator">*</span>source<span class="operator">*</span> package <span class="string">&#x27;rmarkdown&#x27;</span> ...</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> package <span class="string">&#x27;rmarkdown&#x27;</span> successfully unpacked and MD5 sums checked</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> using staged installation</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> R</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> inst</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> byte<span class="operator">-</span>compile and prepare package <span class="keyword">for</span> lazy loading</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> help</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span> installing help indices</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span> copying figures</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> building package indices</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> installing vignettes</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> testing <span class="keyword">if</span> installed package can be loaded from temporary location</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> testing <span class="keyword">if</span> installed package can be loaded from final location</span><br><span class="line"><span class="operator">*</span><span class="operator">*</span> testing <span class="keyword">if</span> installed package keeps a record of temporary installation path</span><br><span class="line"><span class="operator">*</span> DONE <span class="punctuation">(</span>rmarkdown<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">下载的程序包在</span><br><span class="line">	‘C<span class="operator">:</span><span class="punctuation">\</span>Users<span class="punctuation">\</span>yejiu<span class="punctuation">\</span>AppData<span class="punctuation">\</span>Local<span class="punctuation">\</span>Temp<span class="punctuation">\</span>Rtmp2t3ab0<span class="punctuation">\</span>downloaded_packages’里</span><br><span class="line"><span class="operator">&gt;</span> install.packages<span class="punctuation">(</span><span class="string">&quot;gRbase&quot;</span><span class="punctuation">)</span></span><br><span class="line">WARNING<span class="operator">:</span> Rtools is required to build R packages but is not currently installed. Please download and install the appropriate version of Rtools before proceeding<span class="operator">:</span></span><br><span class="line"></span><br><span class="line">https<span class="operator">:</span><span class="operator">/</span><span class="operator">/</span>cran.rstudio.com<span class="operator">/</span>bin<span class="operator">/</span>windows<span class="operator">/</span>Rtools<span class="operator">/</span></span><br><span class="line">将程序包安装入‘C<span class="operator">:</span><span class="operator">/</span>Users<span class="operator">/</span>yejiu<span class="operator">/</span>AppData<span class="operator">/</span>Local<span class="operator">/</span>R<span class="operator">/</span>win<span class="operator">-</span>library<span class="operator">/</span><span class="number">4.3</span>’</span><br><span class="line"><span class="punctuation">(</span>因为‘lib’没有被指定<span class="punctuation">)</span></span><br><span class="line">trying URL <span class="string">&#x27;https://cran.rstudio.com/bin/windows/contrib/4.3/gRbase_1.9.0.zip&#x27;</span></span><br><span class="line">Content type <span class="string">&#x27;application/zip&#x27;</span> <span class="built_in">length</span> <span class="number">2361559</span> bytes <span class="punctuation">(</span><span class="number">2.3</span> MB<span class="punctuation">)</span></span><br><span class="line">downloaded <span class="number">2.3</span> MB</span><br><span class="line"></span><br><span class="line">程序包‘gRbase’打开成功，MD5和检查也通过</span><br><span class="line"></span><br><span class="line">下载的二进制程序包在</span><br><span class="line">	C<span class="operator">:</span><span class="punctuation">\</span>Users<span class="punctuation">\</span>yejiu<span class="punctuation">\</span>AppData<span class="punctuation">\</span>Local<span class="punctuation">\</span>Temp<span class="punctuation">\</span>Rtmp2t3ab0<span class="punctuation">\</span>downloaded_packages里</span><br><span class="line"><span class="operator">&gt;</span> library<span class="punctuation">(</span>gRain<span class="punctuation">)</span></span><br><span class="line">载入需要的程辑包：gRbase</span><br></pre></td></tr></table></figure>

<p>可以发现已经成功的导入了。</p>
<h3 id="开始进行计算"><a href="#开始进行计算" class="headerlink" title="开始进行计算"></a>开始进行计算</h3><p>此示例使用以下二元变量： asia 、吸烟者 、tub（结核病）、lung（肺癌）、bronc（支气管炎）、either（结核病或肺癌）、dysp（呼吸困难）和 xray 。 每个变量都是二进制的，可以取值“ yes ”和“ no ”，并且其中一个是逻辑变量，如果 tube 或 lung 为 true ( yes )，则为 true ( yes )，否则为 false ( no )。</p>
<p>接着运行如下所示的代码：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行代码</span></span><br><span class="line">yn <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;yes&quot;</span><span class="punctuation">,</span><span class="string">&quot;no&quot;</span><span class="punctuation">)</span></span><br><span class="line">a <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>asia<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">99</span><span class="punctuation">)</span><span class="punctuation">,</span>levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">t.a <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>tub<span class="operator">|</span>asia<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">5</span><span class="punctuation">,</span><span class="number">95</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">99</span><span class="punctuation">)</span><span class="punctuation">,</span>levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">s <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>smoke<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">5</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">,</span> levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">l.s <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>lung<span class="operator">|</span>smoke<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">9</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">99</span><span class="punctuation">)</span><span class="punctuation">,</span> levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">b.s <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>bronc<span class="operator">|</span>smoke<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">6</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">7</span><span class="punctuation">)</span><span class="punctuation">,</span> levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">e.lt <span class="operator">&lt;-</span></span><br><span class="line">  cptable<span class="punctuation">(</span><span class="operator">~</span>either<span class="operator">|</span>lung<span class="operator">:</span>tub<span class="punctuation">,</span>values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span>levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">x.e <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>xray<span class="operator">|</span>either<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">98</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">,</span><span class="number">95</span><span class="punctuation">)</span><span class="punctuation">,</span> levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">d.be <span class="operator">&lt;-</span> cptable<span class="punctuation">(</span><span class="operator">~</span>dysp<span class="operator">|</span>bronc<span class="operator">:</span>either<span class="punctuation">,</span> values<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">9</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">7</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">8</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">9</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                levels<span class="operator">=</span>yn<span class="punctuation">)</span></span><br><span class="line">plist <span class="operator">&lt;-</span> compileCPT<span class="punctuation">(</span><span class="built_in">list</span><span class="punctuation">(</span>a<span class="punctuation">,</span> t.a<span class="punctuation">,</span> s<span class="punctuation">,</span> l.s<span class="punctuation">,</span> b.s<span class="punctuation">,</span> e.lt<span class="punctuation">,</span> x.e<span class="punctuation">,</span> d.be<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 核对一些nodes的conditional probability</span></span><br><span class="line">plist<span class="operator">$</span>tub</span><br><span class="line">plist<span class="operator">$</span>either</span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># 核对一些nodes的conditional probability</span></span><br><span class="line"><span class="operator">&gt;</span> plist<span class="operator">$</span>tub</span><br><span class="line">     asia</span><br><span class="line">tub    yes   no</span><br><span class="line">  yes <span class="number">0.05</span> <span class="number">0.01</span></span><br><span class="line">  no  <span class="number">0.95</span> <span class="number">0.99</span></span><br><span class="line"><span class="operator">&gt;</span> plist<span class="operator">$</span>either</span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> tub <span class="operator">=</span> yes</span><br><span class="line"></span><br><span class="line">      lung</span><br><span class="line">either yes no</span><br><span class="line">   yes   <span class="number">1</span>  <span class="number">1</span></span><br><span class="line">   no    <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> tub <span class="operator">=</span> no</span><br><span class="line"></span><br><span class="line">      lung</span><br><span class="line">either yes no</span><br><span class="line">   yes   <span class="number">1</span>  <span class="number">0</span></span><br><span class="line">   no    <span class="number">0</span>  <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>贝叶斯网络是一个用于表示随机变量及其条件依赖性的概率图模型。在上述结果中，我们有两个表格：一个表示<code>asia</code>和<code>tub</code>之间的关系，另一个表示<code>tub</code>和<code>either</code>以及<code>lung</code>之间的关系。</p>
<ol>
<li><p><strong><code>plist$tub</code> 表</strong>:</p>
<p>这个表展示了给定 <code>asia</code> 的状态时 <code>tub</code> 的条件概率。</p>
<ul>
<li>如果一个人在亚洲（asia&#x3D;yes），他患有肺结核（tub&#x3D;yes）的概率是0.05，不患有的概率是0.95。</li>
<li>如果一个人不在亚洲（asia&#x3D;no），他患有肺结核（tub&#x3D;yes）的概率是0.01，不患有的概率是0.99。</li>
</ul>
</li>
<li><p><strong><code>plist$either</code> 表</strong>:</p>
<p>这个表格展示了给定 <code>tub</code> 和 <code>lung</code> 的状态时 <code>either</code> 的条件概率。它是一个三维的表。</p>
<ul>
<li><p>当 <code>tub=yes</code>：</p>
<ul>
<li>无论 <code>lung</code> 是 <code>yes</code> 还是 <code>no</code>，<code>either=yes</code> 的概率都是1。这意味着，如果一个人确实患有肺结核，那么 <code>either</code> 的值也是肯定的。</li>
</ul>
</li>
<li><p>当 <code>tub=no</code>：</p>
<ul>
<li>如果 <code>lung=yes</code>，那么 <code>either=yes</code> 的概率是1。</li>
<li>如果 <code>lung=no</code>，那么 <code>either=no</code> 的概率是1。这意味着，如果一个人没有患有肺结核并且肺部是健康的，那么 <code>either</code> 的值也是否定的。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>总之，这些表描述了某些医疗状态之间的条件概率关系。尤其是，它们描述了肺结核的存在与否如何依赖于是否在亚洲，以及<code>either</code>状态如何依赖于肺结核和另一个未明确的条件（可能是另一种肺部疾病或状况，由于上下文中没有给出具体描述，所以我们只能推测）。</p>
<p>或者可以这么做：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># 或者可以这么做</span></span><br><span class="line"><span class="operator">&gt;</span> plist<span class="operator">$</span>tub <span class="operator">%&gt;%</span> as.data.frame.table</span><br><span class="line">  tub asia Freq</span><br><span class="line"><span class="number">1</span> yes  yes <span class="number">0.05</span></span><br><span class="line"><span class="number">2</span>  no  yes <span class="number">0.95</span></span><br><span class="line"><span class="number">3</span> yes   no <span class="number">0.01</span></span><br><span class="line"><span class="number">4</span>  no   no <span class="number">0.99</span></span><br></pre></td></tr></table></figure>

<p>上述的结果表示的是：$P(tub &#x3D; yes | asia &#x3D; yes) &#x3D; 0.05$，等等。</p>
<p>接着绘图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(net1)  #无向网络</span><br></pre></td></tr></table></figure>

<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918232441979.png" alt="image-20230918232441979"></p>
<p>接着是有向的网络：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(net1$dag)</span><br></pre></td></tr></table></figure>

<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230918232540916.png" alt="image-20230918232540916"></p>
<p>接着计算边际概率。</p>
<p>边际概率，也叫作边缘概率或简单概率，是一个随机变量在没有给定其他变量的条件下的概率。它表示一个或多个事件发生的概率，而忽略其他事件。边际概率常常通过从联合概率分布中对一个或多个变量求和来获得。</p>
<p>假设我们有两个二进制随机变量$X$和$Y$。给定它们的联合概率分布如下：</p>
<table>
<thead>
<tr>
<th>$X$</th>
<th>$Y$</th>
<th>P$( X, Y )$</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>0.3</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0.1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0.4</td>
</tr>
</tbody></table>
<p>我们可以计算$X$和$Y$的边际概率如下：</p>
<p>$P(X&#x3D;0) &#x3D; P(X&#x3D;0, Y&#x3D;0) + P(X&#x3D;0, Y&#x3D;1) &#x3D; 0.3 + 0.2 &#x3D; 0.5$</p>
<p>$P(X&#x3D;1) &#x3D; P(X&#x3D;1, Y&#x3D;0) + P(X&#x3D;1, Y&#x3D;1) &#x3D; 0.1 + 0.4 &#x3D; 0.5$</p>
<p>$P(Y&#x3D;0) &#x3D; P(X&#x3D;0, Y&#x3D;0) + P(X&#x3D;1, Y&#x3D;0) &#x3D; 0.3 + 0.1 &#x3D; 0.4$</p>
<p>$P(Y&#x3D;1) &#x3D; P(X&#x3D;0, Y&#x3D;1) + P(X&#x3D;1, Y&#x3D;1) &#x3D; 0.2 + 0.4 &#x3D; 0.6$</p>
<p>在这里，$P(X&#x3D;0)$和 $P(X&#x3D;1)$ 就是$X$的边际概率，而$P(Y&#x3D;0)$ 和 $P(Y&#x3D;1)$ 是$Y$的边际概率。</p>
<p>在多维随机变量的情况下，边际概率通常是通过从联合概率分布中对除了目标变量之外的所有变量进行积分（对于连续变量）或求和（对于离散变量）来得到的。</p>
<p>接着计算边际概率：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; # 计算边际概率</span><br><span class="line">&gt; querygrain(net1, nodes = c(&quot;lung&quot;, &quot;bronc&quot;), type = &quot;marginal&quot;)</span><br><span class="line">$lung</span><br><span class="line">lung</span><br><span class="line">  yes    no </span><br><span class="line">0.055 0.945 </span><br><span class="line"></span><br><span class="line">$bronc</span><br><span class="line">bronc</span><br><span class="line"> yes   no </span><br><span class="line">0.45 0.55 </span><br></pre></td></tr></table></figure>

<p>这是一个R代码片段，它使用了<code>querygrain</code>函数（这个函数来自于<code>gRain</code>包，用于查询贝叶斯网络的概率分布）。代码试图查询与<code>lung</code>和<code>bronc</code>这两个节点相关的边际概率。</p>
<p>从给出的结果中，我们可以解释为：</p>
<ol>
<li><p><strong><code>lung</code>节点</strong>:</p>
<p>这是<code>lung</code>的边际概率分布：</p>
<ul>
<li><code>yes</code>：有0.055的概率表示某人患有与此节点相关的肺部疾病或条件（例如肺结核或肺癌等）。</li>
<li><code>no</code>：有0.945的概率表示某人没有患有与此节点相关的肺部疾病或条件。</li>
</ul>
</li>
<li><p><strong><code>bronc</code>节点</strong>:</p>
<p>这是<code>bronc</code>的边际概率分布：</p>
<ul>
<li><code>yes</code>：有0.45的概率表示某人患有支气管炎或与此节点相关的其他呼吸系统条件。</li>
<li><code>no</code>：有0.55的概率表示某人没有患有支气管炎或与此节点相关的其他呼吸系统条件。</li>
</ul>
</li>
</ol>
<p>简而言之，这两个结果提供了<code>lung</code>和<code>bronc</code>两个节点的边际概率分布。这意味着，考虑到网络中所有其他节点的影响，一个随机选择的个体患有某种肺部疾病的概率是0.055，而患有支气管相关疾病的概率是0.45。</p>
<p>然后是联合概率：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># 计算联合概率</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;lung&quot;</span><span class="punctuation">,</span><span class="string">&quot;bronc&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;joint&quot;</span><span class="punctuation">)</span></span><br><span class="line">     bronc</span><br><span class="line">lung     yes     no</span><br><span class="line">  yes <span class="number">0.0315</span> <span class="number">0.0235</span></span><br><span class="line">  no  <span class="number">0.4185</span> <span class="number">0.5265</span></span><br></pre></td></tr></table></figure>

<p>这个R代码片段使用<code>querygrain</code>函数从<code>gRain</code>包中查询了与<code>lung</code>和<code>bronc</code>两个节点相关的联合概率。</p>
<p>给定的结果是一个2x2的表，描述了<code>lung</code>和<code>bronc</code>两个节点可能状态的联合概率分布：</p>
<ol>
<li><p><strong>联合概率</strong>:</p>
<p>$P(lung, bronc)$ 表示<code>lung</code>和<code>bronc</code>两个节点的联合概率分布。</p>
<ul>
<li>$P(lung &#x3D; \text{yes}, bronc &#x3D; \text{yes}) &#x3D; 0.0315$：这意味着，有0.0315的概率一个随机选择的个体同时有肺部疾病（如肺结核或肺癌）并患有支气管炎或与此节点相关的其他呼吸系统条件。</li>
<li>$P(lung &#x3D; \text{yes}, bronc &#x3D; \text{no}) &#x3D; 0.0235$：有0.0235的概率一个随机选择的个体有肺部疾病但没有支气管相关疾病。</li>
<li>$P(lung &#x3D; \text{no}, bronc &#x3D; \text{yes}) &#x3D; 0.4185$：有0.4185的概率一个随机选择的个体没有肺部疾病但患有支气管相关疾病。</li>
<li>$P(lung &#x3D; \text{no}, bronc &#x3D; \text{no}) &#x3D; 0.5265$：有0.5265的概率一个随机选择的个体既没有肺部疾病也没有支气管相关疾病。</li>
</ul>
</li>
</ol>
<p>简而言之，这个表格为我们提供了在考虑<code>lung</code>和<code>bronc</code>两个节点时可能的四种状态组合的联合概率。联合概率给出了两个或多个事件同时发生的概率。在这个情境下，这些事件是与<code>lung</code>和<code>bronc</code>两个节点相关的健康状况或疾病。</p>
<p>计算条件概率：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># conditional probability</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;lung&quot;</span><span class="punctuation">,</span> <span class="string">&quot;bronc&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="operator">+</span>            type <span class="operator">=</span> <span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line">     bronc</span><br><span class="line">lung   yes         no</span><br><span class="line">  yes <span class="number">0.07</span> <span class="number">0.04272727</span></span><br><span class="line">  no  <span class="number">0.93</span> <span class="number">0.95727273</span></span><br></pre></td></tr></table></figure>

<p>开始做各种计算：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># ====================================</span></span><br><span class="line"><span class="operator">&gt;</span> <span class="comment"># P(lung = yes, bronc = yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;lung&quot;</span><span class="punctuation">,</span> <span class="string">&quot;bronc&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;joint&quot;</span><span class="punctuation">)</span></span><br><span class="line">     bronc</span><br><span class="line">lung     yes     no</span><br><span class="line">  yes <span class="number">0.0315</span> <span class="number">0.0235</span></span><br><span class="line">  no  <span class="number">0.4185</span> <span class="number">0.5265</span></span><br></pre></td></tr></table></figure>

<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># p(bronc = yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;bronc&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;marginal&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">$</span>bronc</span><br><span class="line">bronc</span><br><span class="line"> yes   no </span><br><span class="line"><span class="number">0.45</span> <span class="number">0.55</span></span><br></pre></td></tr></table></figure>

<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># p(lung = yes | smoke = yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;lung&quot;</span><span class="punctuation">,</span> <span class="string">&quot;smoke&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line">     smoke</span><br><span class="line">lung  yes   no</span><br><span class="line">  yes <span class="number">0.1</span> <span class="number">0.01</span></span><br><span class="line">  no  <span class="number">0.9</span> <span class="number">0.99</span></span><br></pre></td></tr></table></figure>

<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># p(xray = yes | smoke = yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;xray&quot;</span><span class="punctuation">,</span> <span class="string">&quot;smoke&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line">     smoke</span><br><span class="line">xray        yes         no</span><br><span class="line">  yes <span class="number">0.1517048</span> <span class="number">0.06887528</span></span><br><span class="line">  no  <span class="number">0.8482952</span> <span class="number">0.93112472</span></span><br></pre></td></tr></table></figure>

<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment"># P(xray = yes | smoke = yes, asia = yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;xray&quot;</span><span class="punctuation">,</span> <span class="string">&quot;smoke&quot;</span><span class="punctuation">,</span> <span class="string">&quot;asia&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> asia <span class="operator">=</span> yes</span><br><span class="line"></span><br><span class="line">     smoke</span><br><span class="line">xray      yes       no</span><br><span class="line">  yes <span class="number">0.18485</span> <span class="number">0.105335</span></span><br><span class="line">  no  <span class="number">0.81515</span> <span class="number">0.894665</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> asia <span class="operator">=</span> no</span><br><span class="line"></span><br><span class="line">     smoke</span><br><span class="line">xray      yes       no</span><br><span class="line">  yes <span class="number">0.15137</span> <span class="number">0.068507</span></span><br><span class="line">  no  <span class="number">0.84863</span> <span class="number">0.931493</span></span><br></pre></td></tr></table></figure>

<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230919000312509.png" alt="image-20230919000312509"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment">#P(lung=yes|asia=yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;lung&quot;</span><span class="punctuation">,</span><span class="string">&quot;asia&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">+</span>            <span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line">     asia</span><br><span class="line">lung    yes    no</span><br><span class="line">  yes <span class="number">0.055</span> <span class="number">0.055</span></span><br><span class="line">  no  <span class="number">0.945</span> <span class="number">0.945</span></span><br></pre></td></tr></table></figure>

<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="comment">##P(bronc=yes|smoke=yes, asia=yes)</span></span><br><span class="line"><span class="operator">&gt;</span> querygrain<span class="punctuation">(</span>net1<span class="punctuation">,</span> nodes<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;bronc&quot;</span><span class="punctuation">,</span><span class="string">&quot;smoke&quot;</span><span class="punctuation">,</span><span class="string">&quot;asia&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">+</span>            <span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;conditional&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> asia <span class="operator">=</span> yes</span><br><span class="line"></span><br><span class="line">     smoke</span><br><span class="line">bronc yes  no</span><br><span class="line">  yes <span class="number">0.6</span> <span class="number">0.3</span></span><br><span class="line">  no  <span class="number">0.4</span> <span class="number">0.7</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">,</span> <span class="punctuation">,</span> asia <span class="operator">=</span> no</span><br><span class="line"></span><br><span class="line">     smoke</span><br><span class="line">bronc yes  no</span><br><span class="line">  yes <span class="number">0.6</span> <span class="number">0.3</span></span><br><span class="line">  no  <span class="number">0.4</span> <span class="number">0.7</span></span><br></pre></td></tr></table></figure>



<h2 id="贝叶斯网络学习"><a href="#贝叶斯网络学习" class="headerlink" title="贝叶斯网络学习"></a>贝叶斯网络学习</h2><h3 id="可能的贝叶斯网络关系"><a href="#可能的贝叶斯网络关系" class="headerlink" title="可能的贝叶斯网络关系"></a>可能的贝叶斯网络关系</h3><p>我们研究可能得到这样的一个结果：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230919235412150.png" alt="image-20230919235412150"></p>
<p>我们需要找出各个变量直接的依赖关系，但是存在着多种可能的关系。</p>
<p><strong>为什么存在着多种可能的贝叶斯网络关系</strong></p>
<p>当我们谈论贝叶斯网络（或其他图模型）的结构时，我们实际上是在描述变量之间的依赖关系。在给定一组变量的情况下，可能存在多种方式来表示这些变量之间的依赖关系，因此会有多个网络结构。</p>
<p>以下是为什么存在多个网络结构的几个原因：</p>
<ol>
<li><p><strong>数据的不确定性与噪声</strong>：实际数据可能包含噪声，或者可能没有足够的数据来确定一个唯一的最佳结构。因此，多个结构可能都能很好地适应同一数据集。</p>
</li>
<li><p><strong>变量之间的多重因果关系</strong>：有时，一组变量之间的因果关系可能有多种解释。例如，考虑两个变量A和B。A可能导致B，B可能导致A，或者它们可能都是由第三个未观察到的变量C的结果。这些不同的关系都会导致不同的网络结构。</p>
</li>
<li><p><strong>模型的复杂性选择</strong>：为了避免过度拟合，我们可能希望选择一个更简单的模型，即使它并不是与数据最匹配的模型。这可能会导致选择一个不同的网络结构。</p>
</li>
<li><p><strong>启发式搜索的局限性</strong>：由于搜索最佳结构的问题是NP-hard的，所以我们经常使用启发式方法来近似找到最佳结构。这些方法可能会找到多个接近最佳的结构，但不一定是真正的最佳结构。</p>
</li>
<li><p><strong>不同的先验知识</strong>：在某些情况下，我们可能会根据先验知识对结构进行约束，这可能会导致选择不同的网络结构。例如，如果我们知道某些变量之间不存在因果关系，那么我们可能会排除包含这些关系的结构。</p>
</li>
</ol>
<p>总之，存在多个网络结构的原因与数据、模型选择标准、搜索方法和先验知识等多个因素有关。在实际应用中，通常会根据特定的应用背景和需求来选择最佳的网络结构。</p>
<p>贝叶斯网络（Bayesian Networks）的学习和结构选择的概念如下所示：</p>
<ol>
<li><p><strong>贝叶斯网络</strong>：这是一种概率图模型，它通过有向无环图（DAG，Directed Acyclic Graph）表示变量间的条件依赖关系，并利用这些关系来进行概率推理。</p>
</li>
<li><p><strong>‘Search and score’ approach</strong>：</p>
<ul>
<li>这种方法的目的是从数据中找到最佳的DAG结构。</li>
<li><strong>搜索所有可能的DAGs</strong>：理论上，我们可以检查所有可能的DAG结构来找到最佳的结构。</li>
<li><strong>为每个DAG打分</strong>：这里的“打分”是通过某种评分函数来实现的，该函数基于数据评估DAG的质量。</li>
<li><strong>选择最高分的DAG</strong>：从所有可能的DAGs中选择得分最高的DAG。这个DAG被认为是最适合给定数据的。</li>
</ul>
</li>
<li><p><strong>NP-hard problem</strong>：寻找最佳的DAG结构是一个NP-hard问题，这意味着我们不可能在多项式时间内找到最佳解决方案。这也是为什么实际中很少从头开始搜索所有可能的DAGs的原因。</p>
</li>
<li><p><strong>Constraint-based approach</strong>：</p>
<ul>
<li>这是另一种学习DAG结构的方法。</li>
<li>使用统计测试来评估变量之间的依赖关系。</li>
<li>基于这些统计测试结果，可以确定某些边（关系）是否存在。</li>
<li>这种方法的复杂性是指数级的，因为随着节点数量的增加，需要进行的统计测试的数量也会呈指数增长。</li>
</ul>
</li>
</ol>
<p>总的来说，贝叶斯网络的结构学习是一个复杂的问题，有多种策略可以进行处理。但是，由于问题的困难性，完全的搜索并不总是可行的。因此，实践中经常使用启发式方法、约束方法或结合这些方法的混合方法。</p>
<p><strong>关于Search and Score’s approach。</strong></p>
<p>“Search and Score”方法是在学习贝叶斯网络结构时经常使用的一种方法。该方法的核心思想是在所有可能的有向无环图（DAGs，Directed Acyclic Graphs）中搜索最佳的结构，并使用一个评分函数来为每个DAG打分，以选择那个与数据最匹配的结构。</p>
<ol>
<li><p><strong>搜索（Search）</strong>:</p>
<ul>
<li><strong>所有可能的DAGs</strong>: 理论上，给定n个变量，可能的DAG数量是非常巨大的。这是因为你需要考虑所有可能的边的存在和不存在的组合。</li>
<li>由于搜索空间太大，通常不可能完整地搜索所有可能的DAGs。因此，实际应用中，搜索通常是启发式的，例如使用贪婪算法、遗传算法或其他优化方法来探索和修改当前DAG的结构。</li>
</ul>
</li>
<li><p><strong>打分（Score）</strong>:</p>
<ul>
<li><strong>评分函数</strong>：评分函数的目的是衡量一个给定DAG与数据的匹配程度。高分意味着DAG更能解释观察到的数据。</li>
<li>常用的评分标准包括BIC（Bayesian Information Criterion）、AIC（Akaike Information Criterion）和似然函数等。这些评分标准的选择可能会受到统计性质和数据的影响。</li>
<li>为每个DAG打分: 你将数据带入DAG，并使用上述评分标准之一来为DAG打分。</li>
</ul>
</li>
<li><p><strong>选择最佳DAG</strong>:</p>
<ul>
<li>从所有搜索到的DAGs中选择得分最高的一个。这个DAG被认为是最能代表或解释给定数据的。</li>
</ul>
</li>
</ol>
<p>总的来说，”Search and Score”方法试图在所有可能的网络结构中找到最佳的一个，但由于其计算复杂性，实际操作时往往使用各种启发式算法来近似解决。这种方法的一个主要挑战是，随着变量数量的增加，可能的DAGs数量会指数级增长，导致搜索和评分变得非常复杂。</p>
<p>如下图：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920000518616.png" alt="image-20230920000518616"></p>
<h3 id="贝叶斯网络评分"><a href="#贝叶斯网络评分" class="headerlink" title="贝叶斯网络评分"></a>贝叶斯网络评分</h3><p><strong>我们需要评估多少DAGs？</strong></p>
<p>如下图：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920001511743.png" alt="image-20230920001511743"></p>
<p>这是一个例子：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920000941381.png" alt="image-20230920000941381"></p>
<p><strong>计算需要使用BIC。</strong></p>
<p>Bayesian Information Criterion（BIC）是一种模型选择标准，旨在确定给定数据最佳的模型。BIC不仅考虑模型对数据的拟合度，而且包括一个惩罚项，惩罚模型的复杂性，以避免过拟合。它是基于贝叶斯概率原则的，但其实际计算更像是经验方法。</p>
<p>BIC定义为：<br>$ \text{BIC} &#x3D; -2 \cdot \ln(\text{likelihood of the data under the model}) + k \cdot \ln(n) $<br>其中：</p>
<ul>
<li>$ \ln $ 是自然对数函数。</li>
<li>$\text{likelihood of the data under the model}$ 是给定模型下数据的似然值。</li>
<li>$ k $ 是模型中的参数数量。</li>
<li>$ n $ 是数据集中的样本数量。</li>
</ul>
<p>BIC的两部分有各自的解释：</p>
<ol>
<li><p>**-2 $\cdot \ln(\text{likelihood})$**：这部分衡量模型如何拟合数据。较小的值意味着模型更好地拟合数据。</p>
</li>
<li><p>**$k \cdot \ln(n)$**：这部分惩罚模型的复杂性。增加更多的参数可以使模型更好地拟合数据，但可能导致过拟合。这个惩罚项确保只有当新参数确实提供了显著的改进时，才会添加到模型中。</p>
</li>
</ol>
<p>在使用BIC进行模型选择时，我们通常选择具有最小BIC值的模型，因为较小的BIC值意味着对于其复杂性，模型为数据提供了更好的拟合。</p>
<p>BIC在许多统计建模的上下文中都有应用，包括线性回归、聚类和贝叶斯网络结构学习等。</p>
<p>或者可以采用这个：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002446331.png" alt="image-20230920002446331"></p>
<p>这是一个例子：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002649499.png" alt="image-20230920002649499"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002703834.png" alt="image-20230920002703834"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002713916.png" alt="image-20230920002713916"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920002727198.png" alt="image-20230920002727198"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920003453577.png" alt="image-20230920003453577"></p>
<p><strong>这是另一个例子</strong></p>
<p>好的，让我们使用一个简化的例子来说明如何使用BIC选择最佳的贝叶斯网络。</p>
<p>假设我们有三个随机变量：$A$，$B$，和$C$。我们想要确定这三个变量之间的关系。为简化起见，我们考虑以下两个可能的贝叶斯网络结构：</p>
<ol>
<li>结构1： $A \rightarrow B \rightarrow C$</li>
<li>结构2： $A \leftarrow B \rightarrow C$</li>
</ol>
<p>我们拥有一个数据集，该数据集包含这三个变量的观察值。我们的目标是确定哪一个网络结构更能解释这些观察到的数据。</p>
<p>步骤1：计算每个模型的似然值</p>
<p>首先，我们为每个结构学习其参数（比如条件概率表）。然后，我们计算给定这些参数下数据的似然值。</p>
<p>假设：</p>
<ul>
<li>结构1的数据似然值为 $L_1$</li>
<li>结构2的数据似然值为 $L_2$</li>
</ul>
<p>步骤2：计算BIC值</p>
<p>对于每个结构，我们使用BIC公式计算其值：</p>
<p>$ \text{BIC} &#x3D; -2 \cdot \ln(\text{likelihood}) + k \cdot \ln(n) $</p>
<p>其中:</p>
<ul>
<li>$ k $ 是模型的参数数量。</li>
<li>$ n $ 是数据点的数量。</li>
</ul>
<p>假设每个结构都有5个参数（在实际应用中，参数的数量可能会根据网络的连接性和数据的具体性质而变化），且数据集包含100个观察值。</p>
<p>结构1的BIC值为：</p>
<p>$ \text{BIC}_1 &#x3D; -2 \cdot \ln(L_1) + 5 \cdot \ln(100) $</p>
<p>结构2的BIC值为：</p>
<p>$ \text{BIC}_2 &#x3D; -2 \cdot \ln(L_2) + 5 \cdot \ln(100) $</p>
<p>步骤3：选择BIC值较小的结构</p>
<p>我们选择BIC值较小的结构作为最佳结构。假设 $\text{BIC}_1 &lt; \text{BIC}_2$，那么我们会选择结构1作为这三个变量之间关系的最佳模型。</p>
<p>通过这种方式，BIC帮助我们在拟合数据和模型复杂性之间达到平衡，从而选择出最佳的贝叶斯网络结构。</p>
<p><strong>关于似然值的知识点补充。</strong></p>
<p>似然值（Likelihood）是一个统计学上的概念，用于描述一个模型给定数据的合理性或概率。更具体地说，它度量了在给定某个参数值时，观察到现有数据的概率是多少。似然值与概率相关，但它们并不是同一回事。</p>
<p>为了更清晰地理解似然值，我们可以考虑一个简单的例子：</p>
<p>例子：抛硬币</p>
<p>假设我们有一个可能是不均匀的硬币，我们想要估计这个硬币正面朝上的概率$ p $。为此，我们抛这个硬币10次，得到了7次正面和3次反面的结果。</p>
<ol>
<li><p><strong>概率</strong>：如果我们知道硬币是均匀的（即 $ p &#x3D; 0.5 $），那么我们可以计算在10次抛掷中得到7次正面的概率。</p>
</li>
<li><p><strong>似然值</strong>：现在，我们反过来思考。给定我们已经观察到的7次正面和3次反面的数据，我们想知道这些数据在不同的$ p $值下有多合理。例如，如果 $ p &#x3D; 0.4 $，与$ p &#x3D; 0.7 $相比，这些数据的似然值是多少？</p>
</li>
</ol>
<p>数学上，对于上述硬币抛掷实验，似然值可以写作：</p>
<p>$ L(p) &#x3D; p^7(1-p)^3 $</p>
<p>这个函数会告诉我们在不同的$ p $值下，观察到的数据（7次正面和3次反面）有多合理。</p>
<p>似然值在统计学中非常重要，尤其是在估计模型参数（例如最大似然估计）时。在模型选择和统计推断中，似然函数和它的相关概念（如对数似然）也常常被用到。</p>
<p><strong>为什么这里使用似然值</strong></p>
<p>似然值是统计学和模型选择中的核心概念，它衡量了给定模型下数据的“可能性”。在贝叶斯网络结构学习和许多其他统计任务中，似然值用于评估模型对数据的拟合程度。具体来说，似然值描述了在固定模型参数时，观察到数据的概率是多少。</p>
<p>为什么在模型选择中使用似然值有以下几个原因：</p>
<ol>
<li><p><strong>数据的适应度</strong>：似然值直接反映了模型如何拟合数据。较高的似然值意味着数据在给定的模型和参数下更有可能被观察到。因此，似然值为我们提供了衡量模型对数据拟合程度的直观方法。</p>
</li>
<li><p><strong>参数估计的基础</strong>：在统计学中，最大似然估计（MLE）是一种常用的参数估计方法。通过最大化似然函数，我们可以找到使数据最有可能出现的模型参数。</p>
</li>
<li><p><strong>与BIC和其他信息准则的关系</strong>：如前所述，BIC等模型选择标准在其计算中使用似然值。BIC结合了似然值（衡量模型的拟合度）和模型复杂性的惩罚项，以避免过度拟合。</p>
</li>
<li><p><strong>统计推断</strong>：似然值在统计推断中也起到关键作用，例如在假设检验和置信区间的构建中。</p>
</li>
</ol>
<p>总之，似然值是一个描述给定模型和参数时数据被观察到的“可能性”或“合理性”的度量。在模型选择和参数估计中，它提供了一个重要的工具来评估模型如何解释观察到的数据。</p>
<h3 id="两种方法的比较"><a href="#两种方法的比较" class="headerlink" title="两种方法的比较"></a>两种方法的比较</h3><p>“Search and Score”方法和“Constraint-based”方法。详细为你解释这两种方法：</p>
<ol>
<li><p><strong>“Search and Score” Approach（搜索和评分方法）</strong>:</p>
<ul>
<li><strong>Search for all possible DAGs</strong>: 这意味着尝试所有可能的有向无环图（DAG）结构来代表变量之间的关系。</li>
<li><strong>Score each DAG with a scoring function</strong>: 一旦每个DAG被构建，它就会根据某种评分函数被评分。这个评分函数通常基于数据的似然值，但会加入某种形式的模型复杂性的惩罚，例如BIC。</li>
<li><strong>The DAG with the highest score</strong>: 选择得分最高的DAG，因为这个DAG最能解释给定的数据。</li>
<li><strong>NP-hard problem</strong>: 找到最佳的DAG结构是一个NP-hard问题，意味着随着变量数量的增加，解决它的计算复杂性会急剧增加，使得对于大型问题来说，寻找确切的解决方案是不切实际的。</li>
</ul>
</li>
<li><p><strong>Constraint-based Approach（基于约束的方法）</strong>:</p>
<ul>
<li><strong>Use statistical tests to evaluate the dependency between variables</strong>: 这种方法首先测试变量之间是否独立。例如，使用Pearson卡方测试来评估两个离散变量是否条件独立。</li>
<li><strong>Construct the network based on independencies</strong>: 通过确定的独立性和非独立性关系，构建网络。如果两个变量是独立的，则不在它们之间放置边；否则，会加一个边。</li>
<li><strong>Exponential to the number of nodes</strong>: 尽管这种方法避免了直接搜索所有可能的DAGs，但它的复杂性仍然是节点数量的指数函数。这是因为它必须考虑所有可能的变量组合以测试独立性。</li>
</ul>
</li>
</ol>
<p>这两种方法都有其优点和限制。搜索和评分方法通常会产生更加精确的结果，但对于具有大量变量的数据集可能是不切实际的。而基于约束的方法则更快，但可能更容易受到数据中噪声的影响。在实践中，还有其他混合方法和启发式方法用于结构学习，以解决这两种方法的局限性。</p>
<p>当然可以，下面是一个表格，比较了“Search and Score”方法和“Constraint-based”方法：</p>
<table>
<thead>
<tr>
<th>特点&#x2F;方法</th>
<th>Search and Score</th>
<th>Constraint-based</th>
</tr>
</thead>
<tbody><tr>
<td>主要思路</td>
<td>搜索所有可能的DAGs并为其评分</td>
<td>使用统计测试确定变量之间的独立性</td>
</tr>
<tr>
<td>评分&#x2F;评估机制</td>
<td>使用评分函数（如BIC）</td>
<td>使用统计测试（如卡方测试）</td>
</tr>
<tr>
<td>最佳结构的选择</td>
<td>选择得分最高的DAG</td>
<td>基于确定的独立性和非独立性构建网络</td>
</tr>
<tr>
<td>计算复杂性</td>
<td>NP-hard</td>
<td>指数于节点数量</td>
</tr>
<tr>
<td>数据要求</td>
<td>大量数据可以提供更准确的评分</td>
<td>对数据质量和样本量有一定要求，以确保独立性测试的可靠性</td>
</tr>
<tr>
<td>噪声容忍度</td>
<td>中等（取决于评分函数）</td>
<td>通常较低，因为噪声可能导致错误的独立性测试结果</td>
</tr>
<tr>
<td>结果</td>
<td>通常更加精确但计算量大</td>
<td>更快但可能受到数据噪声的影响</td>
</tr>
<tr>
<td>是否考虑模型复杂性</td>
<td>是（通过评分函数中的惩罚项）</td>
<td>不直接，但通过独立性测试间接地考虑</td>
</tr>
</tbody></table>
<p>这个表格提供了两种方法的基本比较。实际上，在选择最合适的方法时，可能还需要考虑其他因素，如数据的大小、质量、可用的计算资源等。</p>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p><strong>什么是相关性</strong></p>
<p>相关性描述了两个变量之间线性关系的强度和方向。</p>
<p>其中，皮尔逊积矩相关系数（Pearson’s correlation coefficient）是最常用的相关性测量方法。它的公式为：</p>
<p>$ r &#x3D; \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}} $</p>
<p>其中：</p>
<ul>
<li>$ x_i $ 和 $ y_i $ 是观测值。</li>
<li>$ \bar{x} $ 和 $ \bar{y} $ 是其对应的均值。</li>
</ul>
<p>皮尔逊相关系数的值范围从-1到1：</p>
<ul>
<li>$ r &#x3D; 1 $ 表示完全正相关。</li>
<li>$ r &#x3D; -1 $ 表示完全负相关。</li>
<li>$ r &#x3D; 0 $ 表示没有线性相关性。</li>
</ul>
<p>考虑以下两组数据：</p>
<p>$ x: [1, 2, 3, 4, 5] $<br>$ y: [2, 3, 4, 5, 6] $</p>
<p>首先，计算均值：</p>
<p>$ \bar{x} &#x3D; \frac{1+2+3+4+5}{5} &#x3D; 3 $<br>$ \bar{y} &#x3D; \frac{2+3+4+5+6}{5} &#x3D; 4 $</p>
<p>使用上述公式，我们可以计算r：</p>
<p>$ r &#x3D; \frac{\sum (x_i - 3)(y_i - 4)}{\sqrt{\sum (x_i - 3)^2 \sum (y_i - 4)^2}} $</p>
<p>当你对每个$ x_i $和$ y_i $进行计算并插入到公式中，你会得到：</p>
<p>$ r &#x3D; 1 $<br>这表示x和y之间有完全正相关。</p>
<p>在实际应用中，为了计算相关性，通常使用统计软件或编程语言的库函数，如Python的NumPy中的<code>corrcoef</code>函数。</p>
<p><strong>相关性图</strong></p>
<p>相关性图是一种图形化表示方法，用于描述一组变量之间的相互关系。在这个图中，每个节点代表一个变量，而连接节点的边则表示这些变量之间的相关性。边的强度或权重通常表示相关性的大小或强度。通过这种图，我们可以直观地看到哪些变量之间存在较强的关系。</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920005430457.png" alt="image-20230920005430457"></p>
<p><strong>真实图与相关性图</strong></p>
<p>“True graph”和”Correlation graph”是在图论、统计和因果推断的上下文中使用的术语。让我们详细解释它们：</p>
<ol>
<li><p><strong>True Graph</strong>:</p>
<ul>
<li><strong>定义</strong>: “True graph”通常指代数据背后的真实因果结构或关系。在因果推断和贝叶斯网络的文献中，这通常指的是正确的、未观察到的结构，我们希望从数据中学习到或推断出来的。</li>
<li><strong>特点</strong>: 它通常是一个有向图（通常是有向无环图或DAG），其中边表示因果关系，从原因节点指向效果节点。</li>
</ul>
</li>
<li><p><strong>Correlation Graph</strong>:</p>
<ul>
<li><strong>定义</strong>: “Correlation graph”是一个无向图，其中节点表示变量，而边则表示变量之间的统计相关性。如果两个变量之间的相关性（如皮尔逊相关系数）超过某个阈值，或者它们之间的相关性是显著的，那么这两个变量之间就有一条边。</li>
<li><strong>特点</strong>: 这种图只捕捉到变量之间的相关性，而不捕捉到它们之间的因果关系。只因为两个变量相关，并不意味着其中一个变量是另一个变量的原因。</li>
</ul>
</li>
<li><p><strong>区别:</strong></p>
<ul>
<li>“True graph”强调的是真实的因果关系，而不仅仅是相关性。它的边是有向的，表示一个变量如何影响另一个变量。</li>
<li>“Correlation graph”仅表示数据中观察到的相关性，而不表示因果关系。它的边是无向的，只表示两个变量之间存在某种关系，但不说明这种关系的方向或性质。</li>
</ul>
</li>
</ol>
<p>在进行因果推断或数据分析时，理解这两种图之间的区别是非常重要的，因为相关性不等于因果性。</p>
<p>要测试B和C是否仅由于共同的原因A而发生关联，我们采用条件独立测试：$I(B, C|A)$。</p>
<p>例如：偏相关系数、卡方检验。</p>
<p>图示：</p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920010614362.png" alt="image-20230920010614362"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920010656984.png" alt="image-20230920010656984"></p>
<p><strong>这是一个计算的例子：</strong></p>
<p>我们使用一个简化的例子，以偏相关系数为例，说明如何测试B和C是否只因为A的共同因素而相关。</p>
<p>假设我们有三个变量：A、B、C。可能的场景是：</p>
<ul>
<li>A是学生的学习时间；</li>
<li>B是学生的作业得分；</li>
<li>C是学生的期末考试得分。</li>
</ul>
<p>我们想要知道：一旦控制了学习时间（A），作业得分（B）和期末考试得分（C）之间的关系是否仍然存在。</p>
<p>首先，我们可以计算B和C的皮尔逊相关系数，这将给出B和C之间的总体相关性。但为了控制A的效应，我们需要计算偏相关系数。</p>
<p><strong>偏相关系数的公式为</strong>：</p>
<p>$ r_{BC.A} &#x3D; \frac{r_{BC} - r_{BA} \times r_{CA}}{\sqrt{(1 - r_{BA}^2)(1 - r_{CA}^2)}} $</p>
<p>其中:</p>
<ul>
<li>$ r_{BC} $ 是B和C之间的皮尔逊相关系数；</li>
<li>$ r_{BA} $ 是B和A之间的皮尔逊相关系数；</li>
<li>$ r_{CA} $ 是C和A之间的皮尔逊相关系数。</li>
</ul>
<p>假设我们得到以下相关系数：</p>
<ul>
<li>$ r_{BC} &#x3D; 0.8 $</li>
<li>$ r_{BA} &#x3D; 0.6 $</li>
<li>$ r_{CA} &#x3D; 0.7 $</li>
</ul>
<p>插入公式，我们得到：</p>
<ul>
<li>$ r_{BC.A} &#x3D; \frac{0.8 - 0.6 \times 0.7}{\sqrt{(1 - 0.6^2)(1 - 0.7^2)}} $</li>
<li>$ r_{BC.A} &#x3D; \frac{0.8 - 0.42}{\sqrt{(1 - 0.36)(1 - 0.49)}} $</li>
<li>$ r_{BC.A} &#x3D; \frac{0.38}{\sqrt{0.64 \times 0.51}} $</li>
<li>$ r_{BC.A} &#x3D; \frac{0.38}{0.5732} $</li>
<li>$ r_{BC.A} \approx 0.663 $</li>
</ul>
<p>结果显示，在控制了学习时间（A）后，作业得分（B）和期末考试得分（C）之间仍然存在强相关性（偏相关系数约为0.663）。</p>
<p>这个计算过程显示了如何使用偏相关系数来控制潜在的混淆变量，并检查在控制这些变量后其他两个变量之间的关系是否仍然存在。</p>
<h3 id="PC算法"><a href="#PC算法" class="headerlink" title="PC算法"></a>PC算法</h3><p><strong>什么是PC算法</strong></p>
<p>PC算法（Peter &amp; Clark算法）是一个经典的算法，用于从观测数据中学习贝叶斯网络结构，特别是在没有先验知识的情况下。它是基于条件独立性测试的，并且旨在确定哪些变量是条件独立的，以此来推断出变量之间的关系。</p>
<p>PC算法的主要步骤可以分为两部分：确定“骨架”(skeleton)和确定边的方向。</p>
<ol>
<li><p><strong>确定骨架</strong>：</p>
<ul>
<li>开始时，所有变量之间都假设存在一个边（即完全图）。</li>
<li>对于每一对变量（A、B），开始时没有其他变量作为条件变量来测试它们的独立性。如果它们是独立的，则删除边A-B。</li>
<li>如果在上一步中A和B是条件独立的，则考虑一个变量集{C}作为条件变量，并再次测试A和B的独立性。如果它们在给定{C}时是独立的，则再次删除边A-B。</li>
<li>增加条件变量的数量，并重复上述步骤，直到所有变量对都被测试过或达到了预先设定的最大条件变量的数量。</li>
<li>完成这些步骤后，剩下的边构成了网络的“骨架”。</li>
</ul>
</li>
<li><p><strong>确定边的方向</strong>：</p>
<ul>
<li>使用一组规则和条件独立性测试来确定骨架中边的方向，从而得到DAG。</li>
</ul>
</li>
</ol>
<p><strong>注意</strong>：PC算法的一个关键特点是，它不需要对所有可能的网络结构进行搜索和评分，从而大大提高了效率。但这种效率的提高是以牺牲一些精确度为代价的，因为算法可能不会找到数据的最佳结构。</p>
<p>总之，“骨架”是PC算法中的一个关键概念，它描述了在考虑条件独立性测试后仍然存在的变量间的关系。这个骨架随后被用于确定边的方向，从而完成整个网络结构的学习过程。</p>
<p>PC算法由两个主要阶段组成：确定骨架(skeleton)和方向化边(orienting edges)。以下是PC算法的简化伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">函数 PC_algorithm(data, significance_level):</span><br><span class="line">    输入:</span><br><span class="line">        data: 观测数据</span><br><span class="line">        significance_level: 显著性水平，用于条件独立性测试</span><br><span class="line">    </span><br><span class="line">    输出:</span><br><span class="line">        DAG: 一个有向无环图</span><br><span class="line"></span><br><span class="line">    1. 初始化一个完全图 G，其中节点表示变量，每对变量之间都有边</span><br><span class="line">    2. 对于G中的每一对非相邻节点 X 和 Y:</span><br><span class="line">        a. 设 S = 空集合</span><br><span class="line">        b. 当 S 是 X 和 Y 的邻居的真子集时:</span><br><span class="line">            i. 如果 X 和 Y 在给定 S 的条件下是独立的:</span><br><span class="line">                - 在 G 中移除边 X-Y</span><br><span class="line">                - 将与该边相关的所有节点添加到禁止集，确保这些边不会再被方向化</span><br><span class="line">                - 跳出循环</span><br><span class="line">            ii. 否则，S = S 的下一个子集</span><br><span class="line"></span><br><span class="line">    3. 使用以下规则方向化 G 中的边:</span><br><span class="line">        a. 如果在 G 中有一个未定向的边 X-Y，并且在禁止集中不存在与之相关的节点:</span><br><span class="line">            i. 如果存在一个节点 Z，使得 Z 是 X 的邻居，但 Z 不是 Y 的邻居，并且 X-Z-Y 不是一个已定向的路径，那么将 X-Y 定向为 X-&gt;Y</span><br><span class="line"></span><br><span class="line">    4. 返回 G</span><br></pre></td></tr></table></figure>

<p>这个伪代码提供了PC算法的一个高级概述。在实践中，可以使用各种条件独立性测试，如χ²测试、Fisher精确测试等。此外，确定边方向的部分可能需要更复杂的逻辑和额外的规则，特别是当处理更复杂的网络和数据时。</p>
<p><strong>PC算法的Python语言实现</strong></p>
<p>该实现将确定变量之间的关系骨架，使用χ²测试作为条件独立性测试。这里我们仅考虑离散变量的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> chi2_contingency</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_independent</span>(<span class="params">X, Y, S, data, alpha=<span class="number">0.05</span></span>):</span><br><span class="line">    <span class="comment"># 创建联合分布表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> S:</span><br><span class="line">        contingency_table = pd.crosstab(data[X], data[Y])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        contingency_table = data.groupby([X, Y] + S).size().unstack().fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    chi2, p_value, _, _ = chi2_contingency(contingency_table)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p_value &gt; alpha</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PC_algorithm</span>(<span class="params">data, alpha=<span class="number">0.05</span></span>):</span><br><span class="line">    variables = <span class="built_in">list</span>(data.columns)</span><br><span class="line">    G = nx.Graph()</span><br><span class="line">    G.add_nodes_from(variables)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(variables)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(variables)):</span><br><span class="line">            X = variables[i]</span><br><span class="line">            Y = variables[j]</span><br><span class="line">            G.add_edge(X, Y)  <span class="comment"># 开始时为完全图</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(variables)):</span><br><span class="line">                <span class="keyword">if</span> k == i <span class="keyword">or</span> k == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                S = [variables[k]]</span><br><span class="line">                <span class="keyword">if</span> is_independent(X, Y, S, data, alpha):</span><br><span class="line">                    <span class="keyword">if</span> G.has_edge(X, Y):</span><br><span class="line">                        G.remove_edge(X, Y)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">data = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;A&#x27;</span>: np.random.choice([<span class="number">0</span>, <span class="number">1</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;B&#x27;</span>: np.random.choice([<span class="number">0</span>, <span class="number">1</span>], <span class="number">1000</span>),</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: np.random.choice([<span class="number">0</span>, <span class="number">1</span>], <span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保一些依赖关系</span></span><br><span class="line">data[<span class="string">&#x27;B&#x27;</span>] = data[<span class="string">&#x27;A&#x27;</span>] ^ data[<span class="string">&#x27;B&#x27;</span>]  <span class="comment"># B is dependent on A</span></span><br><span class="line">data[<span class="string">&#x27;C&#x27;</span>] = data[<span class="string">&#x27;B&#x27;</span>] ^ data[<span class="string">&#x27;C&#x27;</span>]  <span class="comment"># C is dependent on B</span></span><br><span class="line"></span><br><span class="line">G = PC_algorithm(data)</span><br><span class="line"><span class="built_in">print</span>(G.edges())</span><br></pre></td></tr></table></figure>

<p>这个代码实现考虑了两个变量在给定第三个变量时的独立性。为简化起见，这里只考虑了一个条件变量的情况。在真实应用中，你可能需要考虑更大的条件变量集合。这个简化的例子应该给出边 <code>A-B</code> 和 <code>B-C</code>，因为<code>B</code>取决于<code>A</code>，而<code>C</code>取决于<code>B</code>。</p>
<p>直接使用pgmpy库来进行实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pgmpy.estimators <span class="keyword">import</span> PC</span><br><span class="line"><span class="keyword">from</span> pgmpy.utils <span class="keyword">import</span> get_example_model</span><br><span class="line"><span class="keyword">from</span> pgmpy.estimators <span class="keyword">import</span> ParameterEstimator, MaximumLikelihoodEstimator</span><br><span class="line"><span class="keyword">from</span> pgmpy.independencies <span class="keyword">import</span> Independencies</span><br><span class="line"><span class="keyword">from</span> pgmpy.models <span class="keyword">import</span> BayesianModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">data = pd.DataFrame(np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">5000</span>, <span class="number">2</span>)), columns=<span class="built_in">list</span>(<span class="string">&#x27;AB&#x27;</span>))</span><br><span class="line">data[<span class="string">&#x27;C&#x27;</span>] = data[<span class="string">&#x27;A&#x27;</span>] + data[<span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;C&#x27;</span>] = data[<span class="string">&#x27;C&#x27;</span>] % <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 PC 算法</span></span><br><span class="line">est = PC(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 估计骨架 (无向图)</span></span><br><span class="line">max_cond_vars = <span class="number">3</span>  <span class="comment"># 最大的条件变量数</span></span><br><span class="line">best_model = est.estimate(return_type=<span class="string">&quot;dag&quot;</span>, significance_level=<span class="number">0.01</span>, max_cond_vars=max_cond_vars)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Edges:&quot;</span>, best_model.edges())</span><br></pre></td></tr></table></figure>

<p><strong>伪代码和后续推理</strong></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185854370.png" alt="image-20230920185854370"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185904606.png" alt="image-20230920185904606"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185918918.png" alt="image-20230920185918918"></p>
<p><img src="/2023/09/17/%E9%AB%98%E7%AD%89%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7II/image-20230920185933798.png" alt="image-20230920185933798"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" rel="tag"># 数据科学</a>
              <a href="/tags/%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/" rel="tag"># 分析工具</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/08/10/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="prev" title="数据探索与可视化">
                  <i class="fa fa-chevron-left"></i> 数据探索与可视化
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/19/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/" rel="next" title="概率知识回顾">
                  概率知识回顾 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Jiu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
