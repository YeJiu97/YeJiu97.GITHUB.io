<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="概率论相关的知识点回顾">
<meta property="og:type" content="article">
<meta property="og:title" content="概率知识回顾">
<meta property="og:url" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/index.html">
<meta property="og:site_name" content="夜久">
<meta property="og:description" content="概率论相关的知识点回顾">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230919013403303.png">
<meta property="og:image" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230919014229567.png">
<meta property="og:image" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230920193805815.png">
<meta property="og:image" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230920223518238.png">
<meta property="article:published_time" content="2024-02-06T14:12:03.000Z">
<meta property="article:modified_time" content="2024-02-11T03:00:49.431Z">
<meta property="article:author" content="Ye Jiu">
<meta property="article:tag" content="概率">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="概率论">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230919013403303.png">


<link rel="canonical" href="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/","path":"2024/02/07/概率知识回顾/","title":"概率知识回顾"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>概率知识回顾 | 夜久</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">夜久</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/848368468&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/terence-vassallo-415912750" title="Ocelotter" target="_blank" style="color: #cccccc; text-decoration: none;">Ocelotter</a> · <a href="https://soundcloud.com/terence-vassallo-415912750/euphoria" title="Euphoria - 楽園の扉 中日字幕" target="_blank" style="color: #cccccc; text-decoration: none;">Euphoria - 楽園の扉 中日字幕</a></div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535990&auto=1&height=66"></iframe>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.1.</span> <span class="nav-text">随机实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E7%A9%BA%E9%97%B4"><span class="nav-number">1.2.</span> <span class="nav-text">样本空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E4%BA%8B%E4%BB%B6"><span class="nav-number">1.3.</span> <span class="nav-text">随机事件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%A6%82%E7%8E%87%E7%90%86%E8%A7%A3%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">对概率理解的三种方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E7%9A%84%E4%B8%A4%E5%A4%A7%E5%AD%A6%E6%B4%BE"><span class="nav-number">1.5.</span> <span class="nav-text">概率论的两大学派</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E6%B3%95%E5%AE%9A%E7%90%86%E5%92%8C%E4%B9%98%E6%B3%95%E5%AE%9A%E7%90%86"><span class="nav-number">1.6.</span> <span class="nav-text">加法定理和乘法定理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="nav-number">2.</span> <span class="nav-text">条件概率</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%80%A7%E8%B4%A8"><span class="nav-number">2.1.</span> <span class="nav-text">定义与性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="nav-number">2.2.</span> <span class="nav-text">全概率公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-number">2.3.</span> <span class="nav-text">贝叶斯定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E7%9A%84%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">2.4.</span> <span class="nav-text">事件的独立性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">3.</span> <span class="nav-text">随机变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">3.1.</span> <span class="nav-text">什么是随机变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">3.2.</span> <span class="nav-text">离散随机变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">3.3.</span> <span class="nav-text">连续随机变量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%87%A0%E7%A7%8D%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">4.</span> <span class="nav-text">几种概率分布</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83"><span class="nav-number">4.1.</span> <span class="nav-text">二项分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83"><span class="nav-number">4.2.</span> <span class="nav-text">泊松分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%88%86%E5%B8%83"><span class="nav-number">4.3.</span> <span class="nav-text">平均分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83"><span class="nav-number">4.4.</span> <span class="nav-text">指数分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="nav-number">4.5.</span> <span class="nav-text">正态分布</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">5.</span> <span class="nav-text">多维随机变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">5.1.</span> <span class="nav-text">联合概率分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="nav-number">5.2.</span> <span class="nav-text">边缘概率分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-1"><span class="nav-number">5.3.</span> <span class="nav-text">条件概率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%95%B0%E5%AD%97%E7%89%B9%E5%BE%81"><span class="nav-number">6.</span> <span class="nav-text">随机变量的数字特征</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B"><span class="nav-number">6.1.</span> <span class="nav-text">数学期望</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE%E5%92%8C%E6%A0%87%E5%87%86%E5%B7%AE"><span class="nav-number">6.2.</span> <span class="nav-text">方差和标准差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE%E5%92%8C%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0"><span class="nav-number">6.3.</span> <span class="nav-text">协方差和相关系数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E5%92%8C%E4%B8%AD%E5%BF%83%E7%9F%A9"><span class="nav-number">6.4.</span> <span class="nav-text">矩和中心矩</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E4%B8%8E%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86"><span class="nav-number">7.</span> <span class="nav-text">大数定律与中心极限定理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">大数定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86"><span class="nav-number">7.2.</span> <span class="nav-text">中心极限定理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E5%8F%8A%E7%BB%9F%E8%AE%A1%E9%87%8F"><span class="nav-number">8.</span> <span class="nav-text">样本及统计量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC"><span class="nav-number">8.1.</span> <span class="nav-text">样本与随机样本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E9%87%8F%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-number">8.2.</span> <span class="nav-text">统计量的分布</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Jiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Jiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夜久">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="概率知识回顾 | 夜久">
      <meta itemprop="description" content="概率论相关的知识点回顾">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          概率知识回顾
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-02-07 00:42:03" itemprop="dateCreated datePublished" datetime="2024-02-07T00:42:03+10:30">2024-02-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-11 13:30:49" itemprop="dateModified" datetime="2024-02-11T13:30:49+10:30">2024-02-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">数学</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">概率论相关的知识点回顾</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="随机实验"><a href="#随机实验" class="headerlink" title="随机实验"></a>随机实验</h2><p>随机试验（或称随机实验）是概率论中的基本概念。它指的是在相同的条件下重复进行的试验，每次试验的结果不是确定的，而是有一定的不确定性。</p>
<p>随机试验具有以下三个基本特点：</p>
<ol>
<li><p><strong>可重复性</strong>：随机试验可以在相同的条件下重复进行多次。</p>
</li>
<li><p><strong>结果的不确定性</strong>：进行试验前，不能确切地知道每次试验的具体结果，但是可以确定所有可能的结果所组成的集合。</p>
</li>
<li><p><strong>结果的可列举性</strong>：随机试验的所有可能结果可以明确列举出来，这组可能的结果构成了样本空间。</p>
</li>
</ol>
<p>例如，投掷一个公正的六面骰子就是一个随机试验。在投掷前，我们不能预知这次投掷的具体结果，但我们知道可能的结果是1、2、3、4、5或6。而这个试验可以在相同条件下重复多次。</p>
<p>在概率论中，随机试验的概念是建立数学模型的基础，因为它为定义随机事件、概率和随机变量等概念提供了基础。</p>
<h2 id="样本空间"><a href="#样本空间" class="headerlink" title="样本空间"></a>样本空间</h2><p>样本空间（Sample Space）是概率论中的基本概念，它表示随机试验的所有可能结果所组成的集合。记作 $S$ 或 $\Omega$。</p>
<p>换句话说，样本空间包括了随机试验可能出现的每一种情况。</p>
<p>一些简单的例子有助于理解：</p>
<ol>
<li><p><strong>投掷一枚硬币</strong>：只有两种可能的结果，即正面（Head, H）和反面（Tail, T）。因此，该随机试验的样本空间为：$S &#x3D; { H, T }$</p>
</li>
<li><p><strong>投掷一枚六面骰子</strong>：可能的结果为1、2、3、4、5或6。样本空间为：$S &#x3D; { 1, 2, 3, 4, 5, 6 }$</p>
</li>
<li><p><strong>投掷两枚硬币</strong>：可能的结果组合有正正、正反、反正和反反。样本空间为：$S &#x3D; { HH, HT, TH, TT }$</p>
</li>
</ol>
<p>需要注意的是，样本空间的定义取决于研究问题的具体性质。例如，考虑投掷一枚箭靶的随机试验。如果我们只关心箭是否命中靶心，那么样本空间可能只有 { 命中, 未命中 }。但如果我们关心箭在靶上的确切位置，则样本空间会非常复杂，可能包括靶面的每一个点。</p>
<p>当处理复杂的随机现象时，样本空间可能会非常大或者是无限的，但核心概念始终是：样本空间包含了随机试验的所有可能结果。</p>
<h2 id="随机事件"><a href="#随机事件" class="headerlink" title="随机事件"></a>随机事件</h2><p>随机事件（或简称“事件”）是概率论中的一个重要概念。它指的是在某一随机试验中，样本空间的一个子集。简单地说，随机事件是随机试验可能出现的一个或一组结果。</p>
<p>为了更具体地解释随机事件，我们可以基于前面提到的样本空间的例子来描述：</p>
<ol>
<li><p><strong>投掷一枚硬币</strong>：</p>
<ul>
<li>样本空间 $S &#x3D; { H, T }$</li>
<li>一个可能的随机事件 $A$ 是“出现正面”，表示为 $A &#x3D; { H }$</li>
</ul>
</li>
<li><p><strong>投掷一枚六面骰子</strong>：</p>
<ul>
<li>样本空间 $S &#x3D; { 1, 2, 3, 4, 5, 6 }$</li>
<li>一个可能的随机事件 $B$ 是“出现偶数点数”，表示为 $B &#x3D; { 2, 4, 6 }$</li>
</ul>
</li>
<li><p><strong>投掷两枚硬币</strong>：</p>
<ul>
<li>样本空间 $S &#x3D; { HH, HT, TH, TT }$</li>
<li>一个可能的随机事件 $C$是“至少有一个正面”，表示为 $C &#x3D; { HH, HT, TH }$</li>
</ul>
</li>
</ol>
<p>根据事件是否发生，我们可以将其分类为：</p>
<ul>
<li><p><strong>必然事件</strong>：这是一个总是发生的事件。例如，在投掷硬币的实验中，事件“出现正面或反面”是一个必然事件，因为这两种结果涵盖了所有的可能性。</p>
</li>
<li><p><strong>不可能事件</strong>：这是一个永远不会发生的事件。例如，在投掷硬币的实验中，事件“出现三个面”是一个不可能事件，因为硬币只有两个面。</p>
</li>
<li><p><strong>简单事件</strong>：只包含样本空间中的一个元素的事件。例如，在投掷六面骰子的实验中，事件“出现1点”是一个简单事件。</p>
</li>
<li><p><strong>复合事件</strong>：包含样本空间中的多个元素的事件。例如，事件“出现偶数点数”是一个复合事件，因为它包含了样本空间中的多个元素（2, 4, 6）。</p>
</li>
</ul>
<p>随机事件为我们提供了一个描述和理解不确定性现象的框架，使我们能够利用数学方法来研究这些现象的概率性质。</p>
<p><strong>补充知识点一：概率为0是否意味着必然不会发生？</strong></p>
<p>概率为0并不总是意味着某事件必然不会发生。这在某些情况下可能听起来有些反直觉，但在连续型随机变量的上下文中尤其常见。</p>
<p>考虑正态分布（或高斯分布）为例，对于任何特定的值 $x$，该值发生的概率恰好为0。但这并不意味着这些值不可能出现。实际上，连续型随机变量取任意特定值的概率都是0。在这种情境下，我们通常关注的是某个范围或区间内的值发生的概率，而不是单一值。</p>
<p>为了更清晰地理解，考虑一个在区间 $[0,1]$ 上均匀分布的随机变量。该随机变量取值为 $ 0.5 $ 的概率为0，实际上，它取任何特定值的概率都是0。然而，我们可以说，随机变量取值在区间 $[0.4, 0.6]$ 的概率是 $ 0.2 $。</p>
<p>在离散的情境中，概率为0的事件确实是不会发生的。</p>
<p>总结：对于离散型随机变量，概率为0确实意味着事件不会发生；但对于连续型随机变量，概率为0并不意味着事件不可能发生。</p>
<p><strong>补充知识点二：样本空间为空是否意味着必然不会发生？</strong></p>
<p>是的，当样本空间为空时，它确实意味着没有可能的结果，因此随机试验或随机事件必然不会发生。</p>
<p>样本空间代表了随机试验所有可能的结果。一个空的样本空间表示没有可能的结果，这也就意味着随机试验本身是没有意义或定义不当的。</p>
<p>在实际的概率和统计问题中，遇到空的样本空间可能是定义问题或理解上的错误。因为一个有意义的随机试验应该至少有一个可能的结果。</p>
<h2 id="对概率理解的三种方法"><a href="#对概率理解的三种方法" class="headerlink" title="对概率理解的三种方法"></a>对概率理解的三种方法</h2><p>古典概型、频率概型和主观概率是描述和理解概率的三种不同的方法或解释。下面分别介绍这三种观点：</p>
<ol>
<li><p><strong>古典概型（Classical Probability）</strong>:</p>
<ul>
<li>基于对称性或等可能性来定义概率。</li>
<li>如果一个随机试验的所有可能结果都是等可能的，且总共有 $n$ 种可能的结果，那么每一个单独结果发生的概率是 $\frac{1}{n}$。</li>
<li>例如，投掷一枚公平的硬币，其出现正面或反面的概率都是0.5，因为两种结果是等可能的。</li>
<li>古典概型通常适用于可以明确列举所有可能结果，并且所有结果等可能发生的情况。</li>
</ul>
</li>
<li><p><strong>频率概型（Frequentist Probability）</strong>:</p>
<ul>
<li>基于长期频率来定义概率。</li>
<li>如果在相同条件下重复进行随机试验很多次，某事件 $A$ 发生的频率趋向于稳定值，则这个稳定值被称为事件 $A$ 的概率。</li>
<li>这种观点强调的是大量重复试验的经验结果。</li>
<li>例如，通过大量重复试验，我们发现某骰子在600次投掷中有100次出现1点，那么出现1点的概率就是 $\frac{100}{600} &#x3D; \frac{1}{6}$。</li>
</ul>
</li>
<li><p><strong>主观概率（Subjective Probability）</strong>:</p>
<ul>
<li>基于个体的信念或判断来定义概率。</li>
<li>这种概率表示的是某人对某事件发生的信心或确定性程度。</li>
<li>主观概率不仅仅基于过去的数据或经验，还可能基于个人的信仰、信息或其他非客观因素。</li>
<li>例如，一个农民可能根据自己的经验和天气预报来估计明天下雨的概率为70%。</li>
</ul>
</li>
</ol>
<p>这三种概率观点在不同的场合和应用中都有其适用性和局限性。在实际问题中，选择哪种观点通常取决于问题的性质和可用的信息。</p>
<h2 id="概率论的两大学派"><a href="#概率论的两大学派" class="headerlink" title="概率论的两大学派"></a>概率论的两大学派</h2><p>概率论的学派主要分为两大流派：<strong>频率主义</strong>和<strong>贝叶斯主义</strong>。这两个学派的核心区别在于它们如何定义和解释“概率”的概念。</p>
<ol>
<li><p><strong>频率主义（或称为经验主义、频率学派）</strong>:</p>
<ul>
<li>概率被解释为一个长期的频率。也就是说，如果我们重复一个随机试验无数次，某一事件发生的概率等于这一事件在这些重复试验中发生的频率。</li>
<li>频率学派依赖于大量的观测和经验数据。例如，在投掷骰子的例子中，频率学派会通过多次投掷来估计每个面出现的概率。</li>
<li>在统计推断中，频率学派通常使用置信区间、假设检验等方法。</li>
</ul>
</li>
<li><p><strong>贝叶斯主义（Bayesianism）</strong>:</p>
<ul>
<li>概率被解释为对不确定性的主观信念的度量。这种信念可以根据新的数据或信息进行更新。</li>
<li>贝叶斯学派使用贝叶斯定理来更新先验概率（基于先前知识或信念的概率）并得到后验概率（在考虑新数据后的概率）。</li>
<li>在统计推断中，贝叶斯学派使用概率分布来描述不确定性，并利用先验概率和似然函数来得到后验概率分布。</li>
</ul>
</li>
</ol>
<p>两个学派都有其优势和局限性，且在许多现代应用中都有其重要的地位。随着计算能力的增强，贝叶斯方法在许多领域中越来越受欢迎，特别是在那些需要模型复杂性和不确定性结合的应用中。但是，频率学派的方法仍然在许多实际应用和科学研究中占据主导地位。</p>
<p>下面是一个简单的表格来对比频率主义和贝叶斯主义两大学派：</p>
<table>
<thead>
<tr>
<th>特点&#x2F;学派</th>
<th>频率主义</th>
<th>贝叶斯主义</th>
</tr>
</thead>
<tbody><tr>
<td><strong>概率的定义</strong></td>
<td>长期重复试验中的频率</td>
<td>对不确定性的主观信念的度量</td>
</tr>
<tr>
<td><strong>数据依赖性</strong></td>
<td>重依赖观测数据</td>
<td>结合先验信息和观测数据</td>
</tr>
<tr>
<td><strong>统计推断</strong></td>
<td>置信区间、假设检验</td>
<td>后验概率分布</td>
</tr>
<tr>
<td><strong>参数看法</strong></td>
<td>固定但未知的</td>
<td>有概率分布的变量</td>
</tr>
<tr>
<td><strong>优势</strong></td>
<td>在大样本下性质良好，结论往往更为稳健</td>
<td>灵活，可以结合专家知识，对小样本有良好表现</td>
</tr>
<tr>
<td><strong>局限性</strong></td>
<td>对小样本可能不稳健，不容易结合先验知识</td>
<td>选择合适的先验可能有主观性，计算复杂性可能高</td>
</tr>
</tbody></table>
<p>这只是一个简要的对比，实际上两个学派之间的差异和细节更为丰富和复杂。而且在许多现代统计方法中，频率主义和贝叶斯主义的观点可能会交叉或结合。</p>
<h2 id="加法定理和乘法定理"><a href="#加法定理和乘法定理" class="headerlink" title="加法定理和乘法定理"></a>加法定理和乘法定理</h2><p>加法定理和乘法定理是概率论中两个基本的定理，它们用于计算合成事件的概率。</p>
<ol>
<li><p><strong>加法定理</strong>:</p>
<ul>
<li>对于任何两个事件$ A $和$ B $，加法定理描述了它们的并集的概率：<br>$P(A \cup B) &#x3D; P(A) + P(B) - P(A \cap B)$</li>
<li>如果事件$A$和$B$是互斥的（也就是说，两个事件不能同时发生），那么$P(A \cap B) &#x3D; 0$，加法定理可以简化为:<br>$P(A \cup B) &#x3D; P(A) + P(B)$</li>
</ul>
</li>
<li><p><strong>乘法定理</strong>:</p>
<ul>
<li>对于任何两个事件$A$和$B$，乘法定理描述了它们的交集的概率：<br>$P(A \cap B) &#x3D; P(A) \times P(B|A)$<br>其中，$P(B|A)$是在事件$A$发生的条件下事件$B$发生的概率（条件概率）。</li>
<li>如果事件$A$和$B$是独立的（也就是说，一个事件的发生不影响另一个事件的发生），那么$P(B|A) &#x3D; P(B)$，乘法定理可以简化为:<br>$P(A \cap B) &#x3D; P(A) \times P(B)$</li>
</ul>
</li>
</ol>
<p>这两个定理为概率论提供了基本的运算法则，并在概率的计算和推导中起到核心的作用。</p>
<h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><h2 id="定义与性质"><a href="#定义与性质" class="headerlink" title="定义与性质"></a>定义与性质</h2><p><strong>条件概率的定义</strong>：</p>
<p>给定两个事件$A$和$B$，其中$P(B) &gt; 0$，事件$A$在事件$B$已知发生的条件下发生的概率，记作$P(A|B)$，定义为： $P(A|B) &#x3D; \frac{P(A \cap B)}{P(B)}$，其中，$P(A \cap B)$表示事件$A$和$B$同时发生的概率，而$P(B)$是事件$B$发生的概率。</p>
<p><strong>条件概率的性质</strong>：</p>
<ol>
<li><p><strong>非负性</strong>：对于任何事件$A$和$B$（其中$P(B) &gt; 0)$，有 $P(A|B) \geq 0$</p>
</li>
<li><p><strong>规范性</strong>：对于给定的事件$B$（其中$P(B) &gt; 0 )$，样本空间$S$的条件概率为1，即$P(S|B) &#x3D; 1$</p>
</li>
<li><p><strong>可加性</strong>：如果$A_1, A_2, \ldots, A_n$是互斥事件，则对于任何事件$B$（其中$P(B) &gt; 0)$，有$P(A_1 \cup A_2 \cup \ldots \cup A_n|B) &#x3D; P(A_1|B) + P(A_2|B) + \ldots + P(A_n|B)$</p>
</li>
<li><p><strong>乘法定理</strong>：可以从条件概率的定义中推导出乘法定理，即$P(A \cap B) &#x3D; P(B) \times P(A|B)$或$P(A \cap B) &#x3D; P(A) \times P(B|A)$</p>
</li>
<li><p><strong>全概率公式</strong>：如果$B_1, B_2, \ldots, B_n$是互斥事件，且它们的并集是整个样本空间（即$B_1 \cup B_2 \cup \ldots \cup B_n &#x3D; S )$，则对于任何事件$A$，有<br>$P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(B_i) \times P(A|B_i)$</p>
</li>
<li><p><strong>贝叶斯定理</strong>：基于条件概率和全概率公式，可以推导出贝叶斯定理。如果有事件$B_1, B_2, \ldots, B_n$满足上述全概率公式的条件，则对于任何事件$A$（其中$P(A) &gt; 0 )$，有$P(B_j|A) &#x3D; \frac{P(B_j) \times P(A|B_j)}{\sum_{i&#x3D;1}^{n} P(B_i) \times P(A|B_i)}$，其中，$j &#x3D; 1, 2, \ldots, n$。</p>
</li>
</ol>
<p>条件概率为我们提供了在给定某些信息时理解和计算概率的方法，它在概率论和统计中都有着广泛的应用。</p>
<p><strong>可加性</strong></p>
<p>可加性的图示：</p>
<p><img src="/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230919013403303.png" alt="image-20230919013403303"></p>
<h2 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a><strong>全概率公式</strong></h2><p><strong>全概率公式</strong>是概率论中的一个重要公式，它提供了一个在给定一组互斥事件的条件下计算另一个事件概率的方法。</p>
<p>假设有一组互斥事件 $B_1, B_2, \ldots, B_n$，这些事件的并集是整个样本空间，即：$B_1 \cup B_2 \cup \ldots \cup B_n &#x3D; S$，并且每个事件 $B_i$ 的概率 $P(B_i) &gt; 0$。</p>
<p>对于任意事件 $A$，它的概率可以表示为：$P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(A \cap B_i)$，利用乘法定理，我们有：$P(A \cap B_i) &#x3D; P(B_i) \times P(A|B_i)$。</p>
<p>将这两个等式结合，我们得到全概率公式：$P(A) &#x3D; \sum_{i&#x3D;1}^{n} P(B_i) \times P(A|B_i)$</p>
<p>全概率公式实际上提供了一个将复杂事件 $A$ 的概率分解为在不同情境 $B_i$ 下发生的条件概率的方法。这在实际问题中是非常有用的，特别是当直接计算事件 $A$ 的概率很困难，但我们可以通过某种方式分解或划分样本空间来简化问题时。</p>
<h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a><strong>贝叶斯定理</strong></h2><p><img src="/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230919014229567.png" alt="image-20230919014229567"></p>
<p>贝叶斯定理是概率论和统计中的一个核心概念，它描述了在给定新的观测数据（或证据）后如何更新我们的信念或概率。贝叶斯定理的各个组成部分在贝叶斯统计中有特定的名称，这些名称有助于我们理解定理的实质和应用。</p>
<p>给定事件 $A$ 和 $B$，贝叶斯定理可以表示为：</p>
<p>$P(A|B) &#x3D; \frac{P(B|A) \times P(A)}{P(B)}$</p>
<p>其中：</p>
<ol>
<li><p>**Prior Probability (先验概率) P(A)**：</p>
<ul>
<li>这是在观察到新数据 $B$ 之前，我们对事件 $A$ 发生的概率的信念。它基于之前的知识、经验或信念。</li>
</ul>
</li>
<li><p>**Likelihood (似然) P(B|A)**：</p>
<ul>
<li>给定事件 $A$ 已经发生，这是观测到数据 $B$ 的概率。似然度量了在特定假设 $A$ 下数据 $B$ 的兼容性或支持程度。</li>
</ul>
</li>
<li><p>**Marginal Probability (边缘概率) P(B)**：</p>
<ul>
<li>这是观察到数据 $B$ 的总概率，可以通过全概率公式计算：<br>$P(B) &#x3D; \sum_{i} P(B|A_i) \times P(A_i)$<br>其中，$A_i$ 是所有可能的 $A$ 的情境或版本。</li>
</ul>
</li>
<li><p>**Posterior Probability (后验概率) P(A|B)**：</p>
<ul>
<li>给定观察到的数据 $B$ 后，这是事件 $A$ 发生的概率。这是贝叶斯更新的结果，它结合了我们的先验信念和新的观测数据。</li>
</ul>
</li>
</ol>
<p>在贝叶斯统计中，我们通常关心参数或假设的后验概率分布，这可以帮助我们理解参数或假设在给定数据下的不确定性，并为决策提供指导。</p>
<h2 id="事件的独立性"><a href="#事件的独立性" class="headerlink" title="事件的独立性"></a>事件的独立性</h2><p>事件的独立性是概率论中的一个核心概念。两个或多个事件被认为是独立的，如果一个事件的发生不影响另一个事件的发生概率。</p>
<p>具体定义如下：</p>
<ol>
<li><strong>两个事件的独立性</strong>：假设有两个事件 $A$ 和$B$。如果满足以下条件，则称事件 $A$ 和 $B$ 是独立的：<br>$P(A \cap B) &#x3D; P(A) \times P(B)$，其中，$P(A \cap B)$ 是 $A$ 和 $B$ 同时发生的概率。</li>
</ol>
<p>这可以进一步解释为：如果 $A$ 和 $B$ 是独立的，则 $A$ 的发生不会影响 $B$ 的发生概率，反之亦然。这意味着 $P(A|B) &#x3D; P(A)$ 且 $P(B|A) &#x3D; P(B)$。</p>
<ol start="2">
<li><strong>多个事件的独立性</strong>：对于三个或更多的事件，独立性的定义更为复杂。例如，三个事件 $A$、$B$ 和 $C$ 被认为是独立的，如果它们满足以下所有条件：<ul>
<li>$P(A \cap B) &#x3D; P(A) \times P(B)$</li>
<li>$P(A \cap C) &#x3D; P(A) \times P(C)$</li>
<li>$P(B \cap C) &#x3D; P(B) \times P(C)$</li>
<li>$P(A \cap B \cap C) &#x3D; P(A) \times P(B) \times P(C)$</li>
</ul>
</li>
</ol>
<p>事件的独立性是分析和计算概率的基础，它帮助我们简化问题并提供对随机过程的深入理解。然而，值得注意的是，独立性不同于互斥性。两个互斥的事件不能同时发生，而两个独立的事件可以同时发生，但一个事件的发生不会影响另一个事件的发生概率。</p>
<p>在概率论和统计学中，独立性的概念有多个层次和情境。下面是一些常见的独立性的类型和定义：</p>
<ol>
<li><p>**简单事件独立性 (Event Independence)**：</p>
<ul>
<li>如前所述，两个事件 $A$ 和 $B$ 是独立的，如果它们同时发生的概率等于它们各自发生的概率的乘积，即：$P(A \cap B) &#x3D; P(A) \times P(B)$</li>
</ul>
</li>
<li><p>**条件独立性 (Conditional Independence)**：</p>
<ul>
<li>给定一个第三个事件 $C$，如果事件 $A$ 和 $B$ 在 $C$ 已知发生的情况下是独立的，则称 $A$ 和 $B$ 在 $C$ 的条件下是条件独立的。数学上表示为：$P(A \cap B|C) &#x3D; P(A|C) \times P(B|C)$</li>
</ul>
</li>
<li><p>**序列独立性 (Sequential Independence)**：</p>
<ul>
<li>在随机过程中，一个事件序列（如投掷硬币的序列）中的事件可能是序列独立的，但不一定是相互独立的。例如，在马尔科夫链中，一个状态可能只依赖于前一个状态，但不依赖于更早的状态。</li>
</ul>
</li>
<li><p>**变量独立性 (Statistical Independence of Variables)**：</p>
<ul>
<li>在统计学中，两个随机变量 $X$ 和 $Y$ 被认为是独立的，如果它们的联合分布等于它们各自的边缘分布的乘积，即：$f(x,y) &#x3D; f_X(x) \times f_Y(y)$ 其中 $f(x,y)$ 是 $X$ 和 $Y$ 的联合概率密度函数，而 $f_X(x)$ 和 $f_Y(y)$ 是它们各自的边缘概率密度函数。</li>
</ul>
</li>
<li><p>**互斥性 (Mutual Exclusivity)**：</p>
<ul>
<li>虽然这不是独立性的一个类型，但它经常与独立性混淆。两个事件是互斥的，如果它们不能同时发生。互斥的事件永远不是独立的，因为知道一个事件发生意味着另一个事件必然不会发生。</li>
</ul>
</li>
</ol>
<p>理解这些不同类型的独立性对于正确地分析和解释随机现象和数据是至关重要的。</p>
<h1 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a><strong>随机变量</strong></h1><h2 id="什么是随机变量"><a href="#什么是随机变量" class="headerlink" title="什么是随机变量"></a>什么是随机变量</h2><p>随机变量是概率论和统计学中的一个基本概念，它为随机试验的各种可能结果赋予一个实数值，从而使我们能够研究这些结果的数学特性。</p>
<p><strong>定义</strong>：随机变量（Random Variable, 常简写为 R.V.）是一个定义在样本空间 $S$ 上的实值函数，它将每一个样本点映射到一个实数。形式上，如果 $X$ 是一个随机变量，那么 $X: S \rightarrow \mathbb{R}$，其中 $S$ 是样本空间，$\mathbb{R}$ 是实数集。</p>
<p>更直观地，随机变量实际上是对随机试验结果的数值描述。例如：</p>
<ol>
<li><p>投掷一个公正的骰子，我们可以定义一个随机变量 $X$，表示骰子的上面数字。那么，$X$ 可以取值1, 2, 3, 4, 5, 或 6。</p>
</li>
<li><p>投掷两个骰子，我们可以定义随机变量 $Y$ 为两次投掷的数字之和。那么，$Y$ 的可能取值为2（两个1）到12（两个6）。</p>
</li>
</ol>
<p>随机变量可以进一步分类为<strong>离散随机变量</strong>和<strong>连续随机变量</strong>：</p>
<ul>
<li><p><strong>离散随机变量 (Discrete Random Variable)</strong>: 取值是有限的或可数无限的。例如，投掷骰子的结果、家庭中的孩子数量等。</p>
</li>
<li><p><strong>连续随机变量 (Continuous Random Variable)</strong>: 取值范围在某个区间内是无限的。例如，一个人的身高、一个随机选定的时间点到某事件发生的时间等。</p>
</li>
</ul>
<p>每种类型的随机变量都有与之相关的概率分布，这些分布描述了随机变量取其可能值的概率。离散随机变量有概率质量函数 (Probability Mass Function, PMF)，而连续随机变量有概率密度函数 (Probability Density Function, PDF)。</p>
<h2 id="离散随机变量"><a href="#离散随机变量" class="headerlink" title="离散随机变量"></a>离散随机变量</h2><p>离散随机变量是概率论和统计学中的基本概念，其值由一个有限的或可数无限的集合组成。离散随机变量与其可能的结果之间存在明确、可数的映射。</p>
<p>以下是离散随机变量的一些关键特点和定义：</p>
<ol>
<li><p><strong>定义</strong>：离散随机变量的值集是有限的或者可数无限的。例如，骰子的点数、某个时间段内的电话呼叫数量或一本书的页码错误数量都可以由离散随机变量表示。</p>
</li>
<li><p>**概率质量函数 (Probability Mass Function, PMF)**：对于离散随机变量 $X$，其PMF $p(x)$ 给出了 $X$ 取某个特定值的概率。形式上：<br>$p(x) &#x3D; P(X &#x3D; x)$，对于所有的 $x$，我们有 $0 \leq p(x) \leq 1$，且所有可能的 $x$ 值的 $p(x)$ 之和为1。</p>
</li>
<li><p>**累积分布函数 (Cumulative Distribution Function, CDF)**：对于离散随机变量 $X$，其CDF $F(x)$ 表示 $X$ 取小于或等于 $x$ 的值的概率。形式上：<br>$F(x) &#x3D; P(X \leq x) &#x3D; \sum_{t \leq x} p(t)$<br>其中，求和是在所有 $t$ 值上进行的，这些值满足 $t \leq x$。</p>
</li>
<li><p>**期望值 (Expected Value or Mean)**：离散随机变量 $X$ 的期望值，通常表示为 $E[X]$ 或 $\mu$，是其所有可能值的概率加权平均。对于离散随机变量，期望值定义为：$E[X] &#x3D; \sum_{x} x \cdot p(x)$，其中，求和是在所有可能的 $x$ 值上进行的。</p>
</li>
<li><p>**方差 (Variance)**：离散随机变量 $X$ 的方差，通常表示为$(Var(X)$ 或 $\sigma^2$，衡量 $X$ 的值如何围绕其期望值变化。对于离散随机变量，方差定义为：<br>$Var(X) &#x3D; \sum_{x} (x - E[X])^2 \cdot p(x)$</p>
</li>
</ol>
<p>常见的离散概率分布包括伯努利分布、二项分布、几何分布、超几何分布、泊松分布等。</p>
<p>最后，虽然离散随机变量只能取有限或可数的一组值，但这些值可以是整数、有理数或其他形式的数。</p>
<h2 id="连续随机变量"><a href="#连续随机变量" class="headerlink" title="连续随机变量"></a>连续随机变量</h2><p>连续随机变量是概率论和统计学中的一个关键概念。与离散随机变量只能取有限或可数无限的值不同，连续随机变量可以在一个连续的区间（如一个实数区间）内取任意值。</p>
<p>以下是连续随机变量的一些关键特点和定义：</p>
<ol>
<li><p><strong>定义</strong>：连续随机变量的值集是无限的，并且它可以在一个连续的区间内取任意值。例如，一个人的身高、某物体的质量或从一个点到另一个点的距离都可以由连续随机变量表示。</p>
</li>
<li><p>**概率密度函数 (Probability Density Function, PDF)**：连续随机变量 $X$ 的 PDF $f(x)$ 在某个特定值 $x$ 处的值不直接给出 $X$ 取该值的概率（事实上，连续随机变量在任意特定点的取值概率都是0）。而是提供了 $X$ 落在某个小区间内的相对可能性。PDF满足以下性质：</p>
<ul>
<li>$f(x) \geq 0$ 对所有的 $x$</li>
<li>$\int_{-\infty}^{\infty} f(x) , dx &#x3D; 1$</li>
</ul>
</li>
<li><p>**累积分布函数 (Cumulative Distribution Function, CDF)**：连续随机变量 $X$ 的 CDF $F(x)$ 表示 $X$ 取小于或等于 $x$ 的值的概率。对于连续随机变量，CDF 定义为：$ F(x) &#x3D; P(X \leq x) &#x3D; \int_{-\infty}^{x} f(t) , dt $</p>
</li>
<li><p>**期望值 (Expected Value or Mean)**：连续随机变量 $X$ 的期望值，通常表示为 $E[X]$ 或 $\mu$，是其所有可能值的概率加权平均。对于连续随机变量，期望值定义为：$ E[X] &#x3D; \int_{-\infty}^{\infty} x \cdot f(x) , dx $</p>
</li>
<li><p>**方差 (Variance)**：连续随机变量 $X$ 的方差，通常表示为 $Var(X)$ 或 $\sigma^2$，衡量 $X$ 的值如何围绕其期望值变化。对于连续随机变量，方差定义为：$ Var(X) &#x3D; \int_{-\infty}^{\infty} (x - E[X])^2 \cdot f(x) , dx $</p>
</li>
</ol>
<p>常见的连续概率分布包括均匀分布、正态分布、指数分布、贝塔分布和卡方分布等。</p>
<p>最后，值得注意的是，尽管我们经常在实际问题中使用连续概率模型，但真正的“连续性”在许多实际情况中可能是一个近似。例如，虽然我们可能会使用连续模型来描述人的身高，但身高的真实测量值在某种程度上是离散的，因为它们可以被限制到特定的精度或测量单位。</p>
<h1 id="几种概率分布"><a href="#几种概率分布" class="headerlink" title="几种概率分布"></a>几种概率分布</h1><h2 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h2><p>二项分布是离散概率分布的一种，用于描述在一系列独立的、有相同概率的试验中成功的次数。以下是二项分布的主要特点和定义：</p>
<p>假设进行了 $n$ 次独立的 Bernoulli 试验（每次试验只有两个可能的结果：成功或失败），并且每次试验成功的概率都是 $p$，失败的概率是 $1-p$。则随机变量 $X$，表示 $n$ 次试验中成功的次数，服从参数为 $n$ 和 $p$ 的二项分布，记为 $X \sim B(n, p)$。</p>
<p>对于 $k &#x3D; 0, 1, 2, \dots, n$，二项分布的概率质量函数 (PMF) 定义为：$ P(X &#x3D; k) &#x3D; \binom{n}{k} p^k (1-p)^{n-k} $</p>
<p>其中，$\binom{n}{k}$ 是组合数，表示从 $n$ 个对象中选择 $k$ 个的方法数，计算为：</p>
<p>$ \binom{n}{k} &#x3D; \frac{n!}{k!(n-k)!} $</p>
<p>二项分布的期望值和方差分别为：</p>
<p>$ E[X] &#x3D; np $</p>
<p>$ Var(X) &#x3D; np(1-p) $</p>
<p>二项分布是描述重复进行的、独立的、具有相同成功概率的试验中成功次数的分布。例如，掷一个不均匀的硬币 $n$ 次，出现正面的次数就服从一个二项分布。</p>
<ol>
<li>试验是独立的。</li>
<li>每次试验都只有两个可能的结果（通常称为“成功”和“失败”）。</li>
<li>每次试验的成功概率都是 $p$，并且保持不变。</li>
<li>我们关心的是 $n$ 次试验中成功的总次数。</li>
</ol>
<p>二项分布在统计学、质量控制、金融和其他许多领域中都有广泛的应用。</p>
<h2 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h2><p>泊松分布是一种描述稀有事件在固定时间、空间或其他定义好的范围内发生次数的离散概率分布。这些事件在所考虑的区间内是随机且独立发生的。</p>
<p>如果随机变量 $X$ 表示在一个固定的时间、空间或其他范围内稀有事件的发生次数，并且满足以下条件：</p>
<ol>
<li>在任何非常小的时间、空间或其他范围内，事件发生一次的概率与这个区间的大小成正比，而发生两次或更多次的概率是忽略不计的。</li>
<li>在不重叠的区间内的事件发生是独立的。</li>
</ol>
<p>那么，随机变量 $X$ 服从参数为 $\lambda$ （$\lambda &gt; 0$）的泊松分布，记作 $X \sim Poisson(\lambda)$。</p>
<p>对于 $k &#x3D; 0, 1, 2, \dots$，泊松分布的概率质量函数 (PMF) 定义为：</p>
<p>$ P(X &#x3D; k) &#x3D; \frac{e^{-\lambda} \lambda^k}{k!} $</p>
<p>其中，$e$ 是自然对数的底，约等于2.71828。</p>
<p>泊松分布的期望值和方差都是 $\lambda$：</p>
<p>$ E[X] &#x3D; \lambda $<br>$ Var(X) &#x3D; \lambda $</p>
<p>泊松分布经常被用来模拟那些在平均下来很少发生，但偶尔会突然增加的事件。例如，某一地区一天内发生的交通事故数、某一时间段内到达某服务设施的顾客数等，都可以使用泊松分布来描述。</p>
<p>参数 $\lambda$ 表示在给定的时间、空间或其他范围内事件的平均发生率。例如，如果一个电话交换机在每小时平均接收到10次电话，那么 $\lambda &#x3D; 10$。</p>
<p>泊松分布与指数分布关联密切。如果事件之间的时间间隔服从指数分布，那么这些事件的数量就服从泊松分布。</p>
<p>总之，泊松分布是描述稀有事件在一定范围内发生次数的一个非常有用的工具，在许多实际应用中都有广泛的用途。</p>
<h2 id="平均分布"><a href="#平均分布" class="headerlink" title="平均分布"></a>平均分布</h2><p>均匀分布是统计学和概率论中最简单、最直观的概率分布之一。它的主要特点是在其定义域内的所有值都有相同的概率。均匀分布可以是离散的或连续的，但连续均匀分布是最常见的。</p>
<p><strong>连续均匀分布</strong></p>
<p>假设随机变量 $X$ 在区间 $[a, b]$ 内取值，并且在这个区间的任意子区间内的值都有相同的概率。那么，我们说 $X$ 服从区间 $[a, b]$ 上的连续均匀分布。</p>
<p>概率密度函数 (PDF)：<br>$ f(x) &#x3D;<br>\begin{cases}<br>\frac{1}{b-a} &amp; \text{if } a \leq x \leq b \<br>0 &amp; \text{otherwise}<br>\end{cases}<br>$</p>
<p>累积分布函数 (CDF)：<br>$ F(x) &#x3D;<br>\begin{cases}<br>0 &amp; \text{if } x &lt; a \<br>\frac{x-a}{b-a} &amp; \text{if } a \leq x \leq b \<br>1 &amp; \text{if } x &gt; b<br>\end{cases}<br>$</p>
<p><strong>期望值</strong>：$ E[X] &#x3D; \frac{a+b}{2} $</p>
<p><strong>方差</strong>：$ Var(X) &#x3D; \frac{(b-a)^2}{12} $</p>
<p><strong>离散均匀分布</strong></p>
<p>假设一个离散随机变量 $X$ 有 $n$ 个可能的值，且每个值出现的概率都是 $\frac{1}{n}$。那么，我们说 $X$ 有离散均匀分布。</p>
<p>例如，投掷一个公正的六面骰子的结果就服从一个 ${1, 2, 3, 4, 5, 6}$ 上的离散均匀分布，因为每一个面出现的概率都是 $\frac{1}{6}$。</p>
<p><strong>期望值</strong>：$ E[X] &#x3D; \frac{n+1}{2} $，当所有可能的值为1, 2, …, n时。</p>
<p><strong>方差</strong>：$ Var(X) &#x3D; \frac{n^2-1}{12} $，当所有可能的值为1, 2, …, n时。</p>
<p><strong>描述</strong></p>
<p>均匀分布通常用于描述那些所有结果都 equally likely 的随机现象。例如，投掷公正的骰子、抽取随机数等。对于连续均匀分布，它的图形是一个在区间 $[a, b]$ 上的矩形，其高度为常数 $\frac{1}{b-a}$。</p>
<h2 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h2><p>指数分布是一种连续概率分布，常用于描述某一事件发生的时间间隔。它是一种“无记忆”的分布，这意味着如果某事件的发生遵循指数分布，那么无论我们已经等待了多长时间，从现在开始到下一次事件发生的预期时间仍然是固定的。</p>
<p><strong>定义与性质</strong></p>
<p>假设随机变量 $X$ 表示两次连续事件之间的时间间隔，如果 $X$ 的概率密度函数 (PDF) 为：</p>
<p>$<br>f(x;\lambda) &#x3D;<br>\begin{cases}<br>\lambda e^{-\lambda x} &amp; \text{if } x \geq 0 \<br>0 &amp; \text{if } x &lt; 0<br>\end{cases}<br>$</p>
<p>那么，我们说 $X$ 服从参数为 $\lambda$ 的指数分布。</p>
<p>这里的 $\lambda &gt; 0$ 称为该分布的率参数(rate parameter)。事件的平均发生率为 $\lambda$，所以两次事件之间的平均时间间隔为 $\frac{1}{\lambda}$。</p>
<p><strong>累积分布函数 (CDF)</strong></p>
<p>对于 $x \geq 0$，指数分布的累积分布函数为：<br>$ F(x;\lambda) &#x3D; 1 - e^{-\lambda x} $</p>
<p><strong>期望值与方差</strong></p>
<p>指数分布的期望值和方差分别为：<br>$ E[X] &#x3D; \frac{1}{\lambda} $<br>$ Var(X) &#x3D; \frac{1}{\lambda^2} $</p>
<p>“无记忆”性质</p>
<p>指数分布具有一个特别的性质，称为“无记忆”性质。具体来说，对于所有非负的 $s$ 和 $t$，有：</p>
<p>$ P(X &gt; s + t | X &gt; s) &#x3D; P(X &gt; t) $</p>
<p>这意味着，如果我们知道某事件的发生遵循指数分布，并且已经等待了 $s$ 时间但事件仍未发生，那么从现在开始到事件下一次发生的预期时间仍然是 $\frac{1}{\lambda}$。</p>
<p><strong>应用</strong></p>
<p>指数分布在各种实际应用中都有出现，如：</p>
<ol>
<li>顾客到达服务站的时间间隔。</li>
<li>电子元件的失效时间。</li>
<li>电话呼叫之间的时间间隔。</li>
</ol>
<p>指数分布与泊松分布关联密切：如果某一事件在单位时间内发生的次数服从泊松分布，那么两次连续事件之间的时间间隔就服从指数分布。</p>
<h2 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h2><p>正态分布，也称为高斯分布（Gaussian distribution），是统计学和自然科学中最常见和最重要的连续概率分布之一。正态分布在形状上呈钟形，由两个参数决定：均值（$\mu$）和方差（$\sigma^2$）。</p>
<p><strong>定义与性质</strong></p>
<p>一个随机变量 $X$，如果其概率密度函数 (PDF) 为：</p>
<p>$<br>f(x; \mu, \sigma^2) &#x3D; \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}<br>$</p>
<p>其中，$\mu$ 是均值，$\sigma^2$ 是方差，则我们说 $X$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的正态分布。常用记法为 $X \sim N(\mu, \sigma^2)$。</p>
<p><strong>主要特性</strong></p>
<ol>
<li><strong>钟形曲线</strong>：正态分布的概率密度函数呈钟形，关于其均值 $\mu$ 对称。</li>
<li><strong>均值、中位数和众数相同</strong>：对于正态分布，均值、中位数和众数都是相同的，且都等于 $\mu$。</li>
<li><strong>两个参数</strong>：正态分布由两个参数决定：均值 $\mu$ 和标准差 $\sigma$（方差为 $\sigma^2$）。</li>
<li><strong>概率计算</strong>：在 $\mu \pm \sigma$ 的范围内，约有68.27%的数据；在 $\mu \pm 2\sigma$ 的范围内，约有95.45%的数据；在 $\mu \pm 3\sigma$ 的范围内，约有99.73%的数据。</li>
</ol>
<p><strong>应用</strong></p>
<p>正态分布在许多自然和社会科学领域中都有广泛应用：</p>
<ol>
<li><strong>中心极限定理</strong>：大量独立随机变量的和或平均值，即使原随机变量不是正态分布的，通常也会趋近于正态分布。</li>
<li><strong>质量控制</strong>：例如，制造中的产品尺寸分布。</li>
<li><strong>社会科学</strong>：如智商分布、考试分数等。</li>
<li><strong>自然科学</strong>：例如测量误差。</li>
<li><strong>金融和经济学</strong>：资产收益、股价变动等多种金融变量的模型常假定其遵循正态分布（尽管实际中可能有偏离）。</li>
</ol>
<p>尽管正态分布在许多领域中非常有用，但不是所有数据都遵循正态分布。在实际应用中，始终需要检验数据是否真正满足正态分布的假设。</p>
<h1 id="多维随机变量"><a href="#多维随机变量" class="headerlink" title="多维随机变量"></a><strong>多维随机变量</strong></h1><h2 id="联合概率分布"><a href="#联合概率分布" class="headerlink" title="联合概率分布"></a>联合概率分布</h2><p>联合概率分布描述了两个或多个随机变量同时取特定值的概率。换句话说，它提供了一种量化随机变量之间的相互关系的方式。联合概率分布可以是离散的或连续的，这取决于相关随机变量的性质。</p>
<ol>
<li><p><strong>离散联合概率分布</strong>：</p>
<p>如果 $X$ 和 $Y$ 是两个离散随机变量，则其联合概率分布 $P(X &#x3D; x_i, Y &#x3D; y_j)$ 给出了同时有 $X &#x3D; x_i$ 和 $Y &#x3D; y_j$ 的概率。通常，我们可以通过一个概率表来表示这些概率，其中行可能代表 $X$ 的值，列代表 $Y$ 的值。</p>
<p>性质：所有概率的总和为1，即：$\sum_i \sum_j P(X &#x3D; x_i, Y &#x3D; y_j) &#x3D; 1$</p>
</li>
<li><p><strong>连续联合概率分布</strong>：</p>
<p>如果 $X$ 和 $Y$ 是连续随机变量，则其联合概率分布由联合概率密度函数 (joint PDF) $f_{X,Y}(x, y)$ 描述。这个函数的值不是概率，但是函数在某一区域上的积分给出了随机变量 $X$ 和 $Y$ 落入该区域的概率。</p>
<p>性质：联合PDF在其定义域上的总积分为1，即：$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y}(x, y) , dx , dy &#x3D; 1$</p>
</li>
</ol>
<p>无论是离散还是连续，联合概率分布都可以用来找到边缘概率分布和条件概率分布：</p>
<ul>
<li><p><strong>边缘概率分布</strong>：描述一个随机变量不考虑其他随机变量的分布。例如，从联合概率分布 $P(X, Y)$ 中，我们可以得到 $X$ 的边缘概率分布 $P(X)$。</p>
</li>
<li><p><strong>条件概率分布</strong>：描述一个随机变量在给定另一个随机变量特定值的条件下的分布。</p>
</li>
</ul>
<p>联合概率分布在统计学、机器学习、信号处理等领域中都有广泛应用，它为分析随机变量之间的关系提供了基本框架。</p>
<h2 id="边缘概率分布"><a href="#边缘概率分布" class="headerlink" title="边缘概率分布"></a>边缘概率分布</h2><p>边缘概率分布描述了从联合概率分布中得到单个随机变量的分布。换句话说，边缘概率分布是对其他变量进行“求和”或“积分”的结果，以便只考虑我们关心的某个特定随机变量。</p>
<p><strong>离散随机变量</strong></p>
<p>对于两个离散随机变量 $X$ 和 $Y$，如果我们已知它们的联合概率分布 $P(X &#x3D; x_i, Y &#x3D; y_j)$，则 $X$ 的边缘概率分布为： $ P(X &#x3D; x_i) &#x3D; \sum_j P(X &#x3D; x_i, Y &#x3D; y_j) $</p>
<p>同样地，$Y$ 的边缘概率分布为：$ P(Y &#x3D; y_j) &#x3D; \sum_i P(X &#x3D; x_i, Y &#x3D; y_j) $</p>
<p><strong>连续随机变量</strong></p>
<p>对于两个连续随机变量 $X$ 和 $Y$ 且已知它们的联合概率密度函数 $f_{X,Y}(x, y)$，则 $X$ 的边缘概率密度函数 $f_X(x)$ 为： $ f_X(x) &#x3D; \int_{-\infty}^{\infty} f_{X,Y}(x, y) , dy $</p>
<p>同样地，$Y$ 的边缘概率密度函数 $f_Y(y)$ 为： $ f_Y(y) &#x3D; \int_{-\infty}^{\infty} f_{X,Y}(x, y) , dx $</p>
<p><strong>重要性</strong></p>
<p>边缘概率分布在统计学和概率论中很重要，因为它们允许我们从复杂的多变量系统中提取出单个变量的信息。在实际问题中，我们可能对所有的随机变量都感兴趣，但有时只关注其中的一个或几个随机变量。在这种情况下，我们可以使用边缘概率分布来考虑这些特定变量，而不是整个系统。</p>
<h2 id="条件概率-1"><a href="#条件概率-1" class="headerlink" title="条件概率"></a>条件概率</h2><p>条件概率分布描述了在给定某些条件下，一个或多个随机变量的概率分布。它提供了在已知其他随机变量取特定值时，某个随机变量取特定值的概率。</p>
<p>对于两个随机变量 $X$ 和 $Y$，$Y &#x3D; y$ 的条件下 $X$ 的条件概率分布由以下公式给出：</p>
<p>$ P(X &#x3D; x | Y &#x3D; y) &#x3D; \frac{P(X &#x3D; x, Y &#x3D; y)}{P(Y &#x3D; y)} $</p>
<p>其中 $P(X &#x3D; x, Y &#x3D; y)$ 是 $X$ 和 $Y$ 的联合概率分布，而 $P(Y &#x3D; y)$ 是 $Y$ 的边缘概率分布。</p>
<p><strong>离散随机变量</strong></p>
<p>对于离散随机变量，条件概率可以看作是给定 $Y &#x3D; y$ 的条件下，$X$ 取特定值 $x$ 的概率。这通常可以通过对联合概率表进行归一化来得到。</p>
<p><strong>连续随机变量</strong></p>
<p>对于连续随机变量，我们考虑条件概率密度函数 (conditional PDF)。给定 $Y &#x3D; y$ 的条件下，$X$ 的条件概率密度函数由以下公式给出：</p>
<p>$ f_{X|Y}(x|y) &#x3D; \frac{f_{X,Y}(x, y)}{f_Y(y)} $</p>
<p>其中 $f_{X,Y}(x, y)$ 是 $X$ 和 $Y$ 的联合概率密度函数，而 $f_Y(y)$ 是 $Y$ 的边缘概率密度函数。</p>
<p><strong>重要性</strong></p>
<p>条件概率分布在许多统计和机器学习应用中都很重要。例如，贝叶斯统计就是基于条件概率的概念。在机器学习中，给定输入 $X$ 预测输出 $Y$ 的问题可以看作是寻找 $Y$ 的条件概率分布 $P(Y|X)$。</p>
<p>此外，条件概率还与独立性有关。如果 $P(X &#x3D; x | Y &#x3D; y) &#x3D; P(X &#x3D; x)$ 对所有的 $x$ 和 $y$ 都成立，则我们说 $X$ 和 $Y$ 是条件独立的。</p>
<h1 id="随机变量的数字特征"><a href="#随机变量的数字特征" class="headerlink" title="随机变量的数字特征"></a>随机变量的数字特征</h1><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a><strong>数学期望</strong></h2><p>数学期望，通常称为期望值或均值，是随机变量可能取值的加权平均值。它提供了一个中心位置的度量，以描述随机变量的长期平均行为。数学期望是概率论和统计学中的基本概念，经常用符号 $E$ 表示。</p>
<ol>
<li><p><strong>离散随机变量</strong>：</p>
<p>对于一个离散随机变量 $X$，其可能的取值为 $x_1, x_2, \dots, x_n$，且对应的概率为 $P(X &#x3D; x_1), P(X &#x3D; x_2), \dots, P(X &#x3D; x_n)$，数学期望 $E(X)$ 计算为：</p>
<p>$ E(X) &#x3D; \sum_{i&#x3D;1}^{n} x_i P(X &#x3D; x_i) $</p>
</li>
<li><p><strong>连续随机变量</strong>：</p>
<p>对于一个连续随机变量 $X$，其概率密度函数为 $f(x)$，数学期望 $E(X)$ 计算为：</p>
<p>$ E(X) &#x3D; \int_{-\infty}^{\infty} x f(x) , dx $</p>
</li>
</ol>
<p><strong>重要性</strong></p>
<p>数学期望是用于描述随机变量中心趋势的关键参数。在实际应用中，期望值可以提供关于随机过程或随机试验结果的平均或预期值的信息。例如，投掷一个均匀的六面骰子的期望值是3.5，这意味着如果我们投掷骰子很多次并计算平均得分，那么这个平均值将接近3.5。</p>
<p>此外，期望有许多有用的性质，如线性性：对于任意的常数 $a$ 和 $b$ 和随机变量 $X$ 和 $Y$，有</p>
<p>$ E(aX + bY) &#x3D; aE(X) + bE(Y) $</p>
<p>这使得期望在概率和统计学中的计算变得相对简单。</p>
<h2 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a>方差和标准差</h2><p>方差和标准差都是描述随机变量或一组数据的分散性或变异性的度量。它们为我们提供了关于数据点相对于平均值或期望的偏离程度的信息。</p>
<ol>
<li><p><strong>方差 (Variance)</strong>:</p>
<p>方差表示随机变量或一组数据的离散程度。对于随机变量 $X$，其方差记作 $Var(X)$ 或 $ \sigma^2_X $，定义为：$ \sigma^2_X &#x3D; Var(X) &#x3D; E[(X - E(X))^2] $</p>
<p>具体计算公式如下：</p>
<ul>
<li><p>对于离散随机变量:$ \sigma^2_X &#x3D; \sum_{i} (x_i - E(X))^2 P(X &#x3D; x_i) $</p>
</li>
<li><p>对于连续随机变量:$ \sigma^2_X &#x3D; \int_{-\infty}^{\infty} (x - E(X))^2 f(x) , dx $</p>
</li>
</ul>
<p>如果我们有一个数据样本 ${x_1, x_2, \dots, x_n}$，样本的方差是：</p>
<p>$ s^2 &#x3D; \frac{1}{n-1} \sum_{i&#x3D;1}^{n} (x_i - \bar{x})^2 $</p>
<p>其中，$\bar{x}$ 是样本的平均值。</p>
</li>
<li><p><strong>标准差 (Standard Deviation)</strong>:</p>
<p>标准差是方差的平方根，它的单位与原始数据相同，这使得其在解释和分析时更为直观。对于随机变量 $X$，其标准差记作 $\sigma_X$，定义为：$ \sigma_X &#x3D; \sqrt{Var(X)} $</p>
<p>对于上述数据样本，样本的标准差是：$ s &#x3D; \sqrt{s^2} $</p>
</li>
</ol>
<p><strong>重要性</strong></p>
<p>方差和标准差为我们提供了关于数据的重要信息：</p>
<ul>
<li>当方差或标准差较大时，这意味着数据点大部分都离其平均值较远。</li>
<li>当方差或标准差较小时，这意味着数据点较为集中，大部分数据都接近其平均值。</li>
</ul>
<p>在许多统计和科学研究中，方差和标准差都被用作关键的数据分析工具，帮助研究者理解数据的特性和分布。</p>
<h2 id="协方差和相关系数"><a href="#协方差和相关系数" class="headerlink" title="协方差和相关系数"></a>协方差和相关系数</h2><p>协方差和相关系数都是用于描述两个随机变量之间关系的度量，但它们提供的信息和解释方式有所不同。</p>
<ol>
<li><p><strong>协方差 (Covariance)</strong>:</p>
<p>协方差描述了两个随机变量的联合变异性。对于随机变量 $X$ 和 $Y$，协方差定义为：</p>
<p>$ \text{Cov}(X, Y) &#x3D; E[(X - E(X))(Y - E(Y))] $</p>
<p>具体计算公式如下：</p>
<ul>
<li><p>对于离散随机变量:<br>$ \text{Cov}(X, Y) &#x3D; \sum_{i}\sum_{j} (x_i - E(X))(y_j - E(Y)) P(X &#x3D; x_i, Y &#x3D; y_j) $</p>
</li>
<li><p>对于连续随机变量:<br>$ \text{Cov}(X, Y) &#x3D; \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} (x - E(X))(y - E(Y)) f_{X,Y}(x, y) , dx , dy $</p>
</li>
</ul>
<p>其中，$f_{X,Y}(x, y)$ 是 $X$ 和 $Y$ 的联合概率密度函数。</p>
<p>当协方差为正时，表示当一个变量增大时，另一个变量也倾向于增大；反之，如果协方差为负，表示一个变量增大时，另一个变量倾向于减小。</p>
</li>
<li><p><strong>相关系数 (Correlation Coefficient)</strong>:</p>
<p>相关系数是协方差的标准化版本，用于测量两个随机变量之间线性关系的强度和方向。皮尔逊相关系数，通常表示为 $ \rho $ 或 $ r $，定义为：</p>
<p>$ \rho_{XY} &#x3D; r &#x3D; \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} $</p>
<p>其中，$\sigma_X$ 和 $\sigma_Y$ 分别是 $X$ 和 $Y$ 的标准差。</p>
<p>相关系数的值总是在 -1 和 1 之间：</p>
<ul>
<li>$ \rho &#x3D; 1 $ 表示完全的正线性关系。</li>
<li>$ \rho &#x3D; -1 $ 表示完全的负线性关系。</li>
<li>$ \rho &#x3D; 0 $ 表示 $X$ 和 $Y$ 之间没有线性关系。</li>
</ul>
</li>
</ol>
<p><strong>重要性</strong></p>
<p>协方差和相关系数提供了量化两个随机变量或数据集之间关系的方法。尽管协方差可以给出关系的方向，但其大小并不容易解释。相关系数，由于其被标准化，提供了一个清晰、一致的测量方式，使得不同的数据集之间的比较变得容易。</p>
<h2 id="矩和中心矩"><a href="#矩和中心矩" class="headerlink" title="矩和中心矩"></a>矩和中心矩</h2><p>矩 (moment) 和中心矩 (central moment) 是随机变量分布特性的重要数学描述。它们为我们提供了关于随机变量或一组数据的分布形态、集中趋势和分散程度的信息。</p>
<ol>
<li><p>**矩 (Moment)**： 对于随机变量 $X$ 和非负整数 $k$，随机变量 $X$ 的 $k$ 阶原点矩定义为： $ \mu_k’ &#x3D; E[X^k] $</p>
<p>具体计算公式如下：</p>
<ul>
<li><p>对于离散随机变量: $ \mu_k’ &#x3D; \sum_{i} x_i^k P(X &#x3D; x_i) $</p>
</li>
<li><p>对于连续随机变量: $ \mu_k’ &#x3D; \int_{-\infty}^{\infty} x^k f(x) , dx $</p>
</li>
</ul>
<p>其中，$f(x)$ 是 $X$ 的概率密度函数。</p>
<p>$k$ 阶矩描述了随机变量 $X$ 的分布形状。例如，当 $k &#x3D; 1$ 时，第一原点矩就是随机变量 $X$ 的期望或均值。</p>
</li>
<li><p>**中心矩 (Central Moment)**： 中心矩是关于随机变量的均值或期望值的矩。对于随机变量 $X$ 和非负整数 $k$，随机变量 $X$ 的 $k$ 阶中心矩定义为：</p>
<p>$ \mu_k &#x3D; E[(X - E(X))^k] $</p>
</li>
</ol>
<p>具体计算公式如下：</p>
<ul>
<li>对于离散随机变量: $ \mu_k &#x3D; \sum_{i} (x_i - E(X))^k P(X &#x3D; x_i) $</li>
<li>对于连续随机变量: $ \mu_k &#x3D; \int_{-\infty}^{\infty} (x - E(X))^k f(x) , dx $</li>
</ul>
<p>$k$ 阶中心矩描述了随机变量 $X$ 相对于其均值的分布形状。例如，第二阶中心矩就是方差。</p>
<p><strong>重要性</strong></p>
<p>矩和中心矩为我们提供了关于随机变量或数据集分布的深入信息：</p>
<ul>
<li>第一原点矩给出了均值。</li>
<li>第二阶中心矩给出了方差。</li>
<li>第三阶中心矩与偏度 (skewness) 相关，描述了分布的不对称性。</li>
<li>第四阶中心矩与峰度 (kurtosis) 相关，描述了分布的峰态。</li>
</ul>
<p>通常，矩和中心矩被用作描述和比较不同分布的工具，尤其是在更高级的统计分析中。</p>
<h1 id="大数定律与中心极限定理"><a href="#大数定律与中心极限定理" class="headerlink" title="大数定律与中心极限定理"></a><strong>大数定律与中心极限定理</strong></h1><h2 id="大数定理"><a href="#大数定理" class="headerlink" title="大数定理"></a>大数定理</h2><p>大数定理是概率论中的一个基础定理，描述了随机样本的平均值在样本容量趋于无穷大时的行为。其核心思想是，随着试验次数的增加，样本平均值会趋向于总体的期望值。这为统计学提供了坚实的基础，特别是在描述样本统计量与总体参数之间的关系时。</p>
<p>常见的大数定理有两种：弱大数定理和强大数定理。</p>
<ol>
<li><p><strong>弱大数定理 (Weak Law of Large Numbers, WLLN)</strong>: 假设 ${ X_n }$ 是一系列独立同分布的随机变量，且具有有限的期望值 $E[X_n] &#x3D; \mu$ 和有限的方差 $\text{Var}(X_n) &#x3D; \sigma^2$。那么对于任何 $ \epsilon &gt; 0 $：$ \lim_{n \to \infty} P \left( \left| \frac{1}{n} \sum_{i&#x3D;1}^{n} X_i - \mu \right| &gt; \epsilon \right) &#x3D; 0 $</p>
</li>
<li><p><strong>强大数定理 (Strong Law of Large Numbers, SLLN)</strong>: 同样的假设条件下，我们有：$ P \left( \lim_{n \to \infty} \frac{1}{n} \sum_{i&#x3D;1}^{n} X_i &#x3D; \mu \right) &#x3D; 1 $</p>
</li>
</ol>
<p><strong>以下是弱大数定理的一个简单证明：</strong></p>
<p>设 ${ X_n }$ 是一系列独立同分布的随机变量，且 $E[X_n] &#x3D; \mu$ 和 $\text{Var}(X_n) &#x3D; \sigma^2 &lt; \infty$。</p>
<p>考虑随机变量的平均值：$ S_n &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} X_i $</p>
<p>我们想证明：$ \lim_{n \to \infty} P(|S_n - \mu| &gt; \epsilon) &#x3D; 0 $</p>
<p>由切比雪夫不等式 (Chebyshev’s Inequality)，我们有：$ P(|X - E[X]| \geq k\sigma) \leq \frac{1}{k^2} $</p>
<p>应用切比雪夫不等式给 $S_n$，我们得到：$ P(|S_n - \mu| \geq \epsilon) \leq \frac{\text{Var}(S_n)}{\epsilon^2} $</p>
<p>因为 $\text{Var}(S_n) &#x3D; \frac{\sigma^2}{n}$，所以：$ P(|S_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} $</p>
<p>当 $n \to \infty$，我们有：$ \lim_{n \to \infty} P(|S_n - \mu| &gt; \epsilon) &#x3D; 0 $</p>
<p>这就完成了弱大数定理的证明。</p>
<p><strong>使用Python语言展示大数定理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子以保持结果的一致性</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抛硬币的模拟实验次数</span></span><br><span class="line">num_trials = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用numpy进行模拟，其中1表示正面，0表示反面</span></span><br><span class="line">coin_flips = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=num_trials)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算正面出现的累计平均频率</span></span><br><span class="line">cumulative_avg = np.cumsum(coin_flips) / (np.arange(num_trials) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(cumulative_avg, label=<span class="string">&quot;Cumulative Average of Coin Flips&quot;</span>)</span><br><span class="line">plt.axhline(<span class="number">0.5</span>, color=<span class="string">&quot;red&quot;</span>, linestyle=<span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;Expected Value (0.5)&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Number of Trials&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average Frequency of Heads&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Law of Large Numbers - Coin Flip Example&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<p><img src="/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230920193805815.png" alt="image-20230920193805815"></p>
<h2 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h2><p><strong>什么是中心极限定理</strong></p>
<p>中心极限定理（Central Limit Theorem, CLT）是统计学和概率论中的一个关键定理，它说明了为什么许多自然现象都呈正态分布。基本上，CLT指出，如果你取一个任意的分布（给定其有有限的均值和方差）并多次从中抽取样本，然后计算这些样本的平均值，这些平均值的分布将趋近于正态分布。此外，这种趋势并不取决于原始分布的形状。</p>
<p>更具体地，假设 $X_1, X_2, \ldots, X_n$ 是从一个均值为 $\mu$ 和方差为 $\sigma^2$ 的任意分布中抽取的n个独立同分布样本。定义样本均值为：</p>
<p>$ \bar{X} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n X_i $</p>
<p>那么，当 $n$ 趋于无穷大时，$\sqrt{n} (\bar{X} - \mu)$ 将按分布趋近于一个均值为0和方差为 $\sigma^2$ 的正态分布。</p>
<p>简言之，对于大的 $n$，有：</p>
<p>$ \bar{X} \sim N \left( \mu, \frac{\sigma^2}{n} \right) $</p>
<p><strong>中心极限定理的证明</strong></p>
<p>中心极限定理（Central Limit Theorem, CLT）是概率论中的一个深入且基本的结果。完整的证明超出了这个平台的能力范围，但我会尝试为您提供一个概述和其中的关键步骤。具体的证明通常涉及特征函数或者生成函数。</p>
<p>中心极限定理（Lindeberg-Levy版本）</p>
<p>假设 $X_1, X_2, \ldots$ 是一组独立同分布的随机变量，它们有共同的期望 $\mu$ 和方差 $\sigma^2$（其中 $0 &lt; \sigma^2 &lt; \infty$）。定义部分和为：</p>
<p>$ S_n &#x3D; X_1 + X_2 + \ldots + X_n $</p>
<p>则对任意实数 x，当 n 趋向于无穷时，我们有：</p>
<p>$ P\left( \frac{S_n - n\mu}{\sigma \sqrt{n}} \leq x \right) \to \Phi(x) $</p>
<p>其中 $\Phi(x)$ 是标准正态分布的累积分布函数。</p>
<p><strong>证明概述</strong>：</p>
<ol>
<li><p><strong>特征函数</strong>: 首先，我们使用特征函数，这是随机变量分布的一个有用工具。随机变量X的特征函数定义为：$ \phi_X(t) &#x3D; E[e^{itX}] $</p>
</li>
<li><p><strong>计算独立随机变量的和的特征函数</strong>: 如果 $X_1, \ldots, X_n$ 是独立的随机变量，那么它们的和 $S_n &#x3D; X_1 + \ldots + X_n$ 的特征函数是它们各自特征函数的乘积：$ \phi_{S_n}(t) &#x3D; \phi_{X_1}(t) \ldots \phi_{X_n}(t) $</p>
</li>
<li><p><strong>标准化</strong>: 为了考虑中心极限定理，我们将 $S_n$ 标准化：$ Z_n &#x3D; \frac{S_n - n\mu}{\sigma \sqrt{n}} $</p>
</li>
<li><p><strong>利用特征函数的连续性</strong>：我们知道，如果随机变量序列 $Y_n$ 收敛于 $Y$，且其特征函数 $\phi_{Y_n}$ 收敛于 $\phi_Y$ 于每一点，则 $Y_n$ 在分布上收敛于 $Y$。</p>
</li>
<li><p><strong>计算极限</strong>：我们计算 $Z_n$ 的特征函数，并证明当 $n$ 趋近于无穷时，它收敛到标准正态分布的特征函数。</p>
</li>
</ol>
<p>通过上述步骤，我们可以得到当 $n$ 趋向于无穷时， $Z_n$ 的分布趋近于标准正态分布。</p>
<p>这只是一个证明的高度概括。具体的证明涉及复杂的数学计算和对特征函数的深入了解。如果您需要更详细的证明，建议查阅高级的概率论教材或论文。</p>
<p><strong>中心极限定理的Python语言实现</strong></p>
<p>使用Python可以方便地模拟并可视化中心极限定理（CLT）。以下是一个例子，我们将模拟掷骰子的实验，并显示随着我们取样本均值的数量增加，分布逐渐趋近于正态分布：</p>
<ol>
<li>我们假设掷一个六面骰子，每面的概率都是1&#x2F;6。</li>
<li>进行多次实验，每次实验投掷骰子n次并取平均值。</li>
<li>重复上述实验很多次，并查看平均值的分布。</li>
</ol>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dice_roll</span>():</span><br><span class="line">    <span class="keyword">return</span> np.random.randint(<span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_mean</span>(<span class="params">sample_size</span>):</span><br><span class="line">    <span class="keyword">return</span> np.mean([dice_roll() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_size)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate_clt</span>(<span class="params">num_samples, sample_size</span>):</span><br><span class="line">    <span class="keyword">return</span> [sample_mean(sample_size) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">num_samples = <span class="number">10000</span></span><br><span class="line">sample_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">30</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, sample_size <span class="keyword">in</span> <span class="built_in">enumerate</span>(sample_sizes, <span class="number">1</span>):</span><br><span class="line">    means = simulate_clt(num_samples, sample_size)</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, idx)</span><br><span class="line">    plt.hist(means, bins=<span class="number">50</span>, density=<span class="literal">True</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">    plt.title(<span class="string">f&quot;Sample Size: <span class="subst">&#123;sample_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>此代码将产生四个子图，分别表示样本大小为1, 2, 6和30的样本均值的分布。随着样本大小的增加，分布越来越接近正态分布，这恰好是中心极限定理的预测结果。</p>
<p><img src="/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/image-20230920223518238.png" alt="image-20230920223518238"></p>
<h1 id="样本及统计量"><a href="#样本及统计量" class="headerlink" title="样本及统计量"></a>样本及统计量</h1><h2 id="样本与随机样本"><a href="#样本与随机样本" class="headerlink" title="样本与随机样本"></a>样本与随机样本</h2><p>样本和随机样本是统计学和概率论中的基本概念。</p>
<ol>
<li><p><strong>样本 (Sample)</strong>:</p>
<ul>
<li>样本是从一个较大的集合（通常被称为“总体”）中选择的一个子集。</li>
<li>在统计研究中，通常不可能对整个总体进行研究，因此研究者会选择一个代表性的样本来进行研究。</li>
<li>样本的性质（例如平均值、方差等）用于估计总体的相应性质。</li>
</ul>
</li>
<li><p><strong>随机样本 (Random Sample)</strong>:</p>
<ul>
<li>随机样本是以随机方式从总体中抽取的样本，这意味着每个成员被选中的概率是已知的（不一定非要是相等的）。</li>
<li>随机样本的关键特点是它能够提供对总体的无偏估计。</li>
<li>当每个成员被选中的概率都相等时，我们经常说这是一个简单随机样本。</li>
<li>在实际研究中，可能还会使用其他抽样方法，如分层抽样、整群抽样等，但随机性仍然是关键要素。</li>
</ul>
</li>
</ol>
<p>简单来说，样本是总体的子集，而随机样本是确保每个成员被选中的概率都是已知的，以便我们可以对其进行统计推断。</p>
<p>使用Python来模拟以下情境：</p>
<ol>
<li>总体是一个集合，其中包含1到100的所有整数。</li>
<li>我们首先简单地从总体中抽取一个非随机样本（例如，只选取前10个数字）。</li>
<li>从总体中随机抽取一个样本，这是一个简单随机样本，因为每个数字被选中的概率都是相等的。</li>
</ol>
<p>以下是Python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义总体</span></span><br><span class="line">population = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 非随机样本：选择前10个数字</span></span><br><span class="line">non_random_sample = population[:<span class="number">10</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;非随机样本:&quot;</span>, non_random_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机样本：从总体中随机选择10个数字</span></span><br><span class="line">random_sample = random.sample(population, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机样本:&quot;</span>, random_sample)</span><br></pre></td></tr></table></figure>

<p>运算的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">非随机样本: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">随机样本: [<span class="number">77</span>, <span class="number">30</span>, <span class="number">48</span>, <span class="number">32</span>, <span class="number">79</span>, <span class="number">37</span>, <span class="number">76</span>, <span class="number">15</span>, <span class="number">40</span>, <span class="number">70</span>]</span><br></pre></td></tr></table></figure>

<h2 id="统计量的分布"><a href="#统计量的分布" class="headerlink" title="统计量的分布"></a>统计量的分布</h2><p>当我们从一个总体中抽取样本并计算样本的某个性质（如平均值、方差等）时，这个性质的值会随着我们抽取的样本的不同而变化。我们称这种性质为统计量。由于统计量是基于样本而计算的，它自身也是一个随机变量，并有其自己的分布，这就是所谓的“统计量的分布”。</p>
<p>以下是一些常见的统计量及其分布的简要描述：</p>
<ol>
<li><p><strong>样本均值 ($ \bar{X} $)</strong>:</p>
<ul>
<li>当总体分布是正态分布时，样本均值的分布也是正态分布。</li>
<li>当样本量足够大时（通常 n&gt;30 被视为大样本），根据中心极限定理，无论总体分布是什么，样本均值的分布都会接近正态分布。</li>
<li>样本均值的期望值是总体均值，其方差是总体方差除以样本大小。</li>
</ul>
</li>
<li><p><strong>样本方差 (S^2)</strong>:</p>
<ul>
<li>对于正态分布的总体，样本方差的分布与卡方分布有关。</li>
</ul>
</li>
<li><p><strong>样本比例 (p̂)</strong>:</p>
<ul>
<li>当总体分布是二项分布时（例如，成功&#x2F;失败、是&#x2F;否的情况），样本比例的分布在大样本下近似正态分布。</li>
</ul>
</li>
<li><p><strong>样本标准差 (S)</strong>:</p>
<ul>
<li>当总体是正态分布时，样本标准差与卡方分布有关。</li>
</ul>
</li>
<li><p><strong>t统计量</strong>:</p>
<ul>
<li>当我们想从小样本中推断一个正态分布总体的均值但总体的方差未知时，我们使用t统计量。这个统计量的分布遵循 t 分布。</li>
</ul>
</li>
<li><p><strong>F统计量</strong>:</p>
<ul>
<li>当我们想比较两个正态分布总体的方差时，我们使用F统计量。这个统计量遵循 F 分布。</li>
</ul>
</li>
</ol>
<p>这些只是最常见的统计量和相关分布的简短概述。每种分布都有其自己的性质和应用，而在统计推断中，了解这些分布对于进行假设检验和构建置信区间等任务至关重要。</p>
<p>Python语言实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm, t, chi2, f</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 样本均值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_mean_distribution</span>(<span class="params">population, sample_size, num_samples</span>):</span><br><span class="line">    means = [np.mean(np.random.choice(population, sample_size)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]</span><br><span class="line">    plt.hist(means, bins=<span class="number">50</span>, density=<span class="literal">True</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Distribution of Sample Means&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">population = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10000</span>)  <span class="comment"># Assume a normal distribution for population</span></span><br><span class="line">sample_mean_distribution(population, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 样本方差</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_variance_distribution</span>(<span class="params">population, sample_size, num_samples</span>):</span><br><span class="line">    variances = [np.var(np.random.choice(population, sample_size), ddof=<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]</span><br><span class="line">    plt.hist(variances, bins=<span class="number">50</span>, density=<span class="literal">True</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Distribution of Sample Variances&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">sample_variance_distribution(population, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 样本比例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample_proportion_distribution</span>(<span class="params">population, sample_size, num_samples</span>):</span><br><span class="line">    proportions = [(np.<span class="built_in">sum</span>(np.random.choice(population, sample_size) == <span class="number">1</span>) / sample_size) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]</span><br><span class="line">    plt.hist(proportions, bins=<span class="number">50</span>, density=<span class="literal">True</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Distribution of Sample Proportions&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">population_proportion = np.array([<span class="number">1</span>]*<span class="number">5000</span> + [<span class="number">0</span>]*<span class="number">5000</span>)  <span class="comment"># Assume 50% success in population</span></span><br><span class="line">sample_proportion_distribution(population_proportion, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. t统计量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">t_stat_distribution</span>(<span class="params">population, sample_size, num_samples</span>):</span><br><span class="line">    t_stats = [(np.mean(np.random.choice(population, sample_size)) - np.mean(population)) / (np.std(np.random.choice(population, sample_size), ddof=<span class="number">1</span>)/np.sqrt(sample_size)) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]</span><br><span class="line">    x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line">    plt.hist(t_stats, bins=<span class="number">50</span>, density=<span class="literal">True</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">    plt.plot(x, t.pdf(x, df=sample_size-<span class="number">1</span>), label=<span class="string">&#x27;t-distribution&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;t-statistic Distribution&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">t_stat_distribution(population, <span class="number">30</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%A6%82%E7%8E%87/" rel="tag"># 概率</a>
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"># 数学</a>
              <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag"># 概率论</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/01/05/%E8%AE%BA%E8%AF%81%E6%98%AF%E4%B8%80%E9%97%A8%E5%AD%A6%E9%97%AE/" rel="prev" title="论证是一门学问">
                  <i class="fa fa-chevron-left"></i> 论证是一门学问
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/02/09/%E7%95%AA%E5%A4%96%EF%BC%9A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AF%86%E6%95%99%E8%82%B2/" rel="next" title="通识教育">
                  通识教育 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Jiu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
