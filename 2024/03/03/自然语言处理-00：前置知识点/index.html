<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="自然语言处理的前置知识点">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理 00：前置知识点">
<meta property="og:url" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/index.html">
<meta property="og:site_name" content="夜久">
<meta property="og:description" content="自然语言处理的前置知识点">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303153844956.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303192849562.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303224742692.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303231225050.png">
<meta property="article:published_time" content="2024-03-03T12:01:29.000Z">
<meta property="article:modified_time" content="2024-03-03T12:52:45.410Z">
<meta property="article:author" content="Ye Jiu">
<meta property="article:tag" content="自然语言处理">
<meta property="article:tag" content="NPL">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303153844956.png">


<link rel="canonical" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/","path":"2024/03/03/自然语言处理-00：前置知识点/","title":"自然语言处理 00：前置知识点"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>自然语言处理 00：前置知识点 | 夜久</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">夜久</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/848368468&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/terence-vassallo-415912750" title="Ocelotter" target="_blank" style="color: #cccccc; text-decoration: none;">Ocelotter</a> · <a href="https://soundcloud.com/terence-vassallo-415912750/euphoria" title="Euphoria - 楽園の扉 中日字幕" target="_blank" style="color: #cccccc; text-decoration: none;">Euphoria - 楽園の扉 中日字幕</a></div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535990&auto=1&height=66"></iframe>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E9%9C%80%E6%B1%82"><span class="nav-number">1.</span> <span class="nav-text">前置知识需求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90"><span class="nav-number">2.</span> <span class="nav-text">相关资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E5%90%88%E3%80%81%E5%87%BD%E6%95%B0%E3%80%81%E6%B1%82%E5%92%8C"><span class="nav-number">3.</span> <span class="nav-text">集合、函数、求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B"><span class="nav-number">4.</span> <span class="nav-text">线性代数与线性方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-number">5.</span> <span class="nav-text">描述性统计知识点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-number">6.</span> <span class="nav-text">概率论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">7.</span> <span class="nav-text">朴素贝叶斯与分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PCA"><span class="nav-number">8.</span> <span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KNN"><span class="nav-number">9.</span> <span class="nav-text">KNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">10.</span> <span class="nav-text">梯度下降</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Jiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Jiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夜久">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="自然语言处理 00：前置知识点 | 夜久">
      <meta itemprop="description" content="自然语言处理的前置知识点">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          自然语言处理 00：前置知识点
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-03-03 22:31:29 / Modified: 23:22:45" itemprop="dateCreated datePublished" datetime="2024-03-03T22:31:29+10:30">2024-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">自然语言处理的前置知识点</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="前置知识需求"><a href="#前置知识需求" class="headerlink" title="前置知识需求"></a>前置知识需求</h2><p>编程语言需求：Python语言。</p>
<p>数学知识：数据科学的数学基础。</p>
<ul>
<li>基础知识：集合、函数、求和</li>
<li>概率</li>
<li>朴素贝叶斯定理和分类器</li>
<li>用矩阵表示数据</li>
<li>解线性方程</li>
<li>降维：主成分分析（PCA）</li>
<li>微积分和梯度下降</li>
</ul>
<p>机器学习与人工智能知识。</p>
<ul>
<li>线性回归，成本&#x2F;损失函数，均方误差。 </li>
<li>分类和交叉验证：贝叶斯分类、k-NN（k近邻）、决策树、感知器训练、逻辑回归、支持向量机（SVM）。 </li>
<li>聚类：k均值。</li>
</ul>
<h2 id="相关资源"><a href="#相关资源" class="headerlink" title="相关资源"></a>相关资源</h2><p><strong>Python和R</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.w3schools.com/python/">W3 Python 教程到外部站点的链接。</a>是刷新 Python 知识的好资源。本教程包含一组页面，涉及 Python 编程中的许多主题，并提供了一个简单的环境来试验 Python 代码。</p>
<p><a target="_blank" rel="noopener" href="https://www.w3schools.com/r/">W3 R教程到外部站点的链接。</a>或者Katya Ognyanova对<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/77838/files/11103244/download">R 基础知识和可视化工具</a>的精彩介绍。还有一个附带的<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/77838/files/11103243/download">R 代码</a>。</p>
<p>以下资源将帮助建立&#x2F;修改统计知识。这些是课程中许多主题中使用的重要概念。</p>
<p><strong>统计</strong></p>
<p>总结<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540179?wrap=1">_</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540179/download?download_frd=1">下载摘要</a>的描述性统计。</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303153844956.png" alt="image-20240303153844956"></p>
<p>DS 摩尔和 GP 麦凯布 (1989)。<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2246282641530001811">统计实践简介到外部站点的链接。</a>。WH 弗里曼&#x2F;时代图书&#x2F;亨利·霍尔特公司</p>
<p>有关统计的其他资源：</p>
<p>其他资源：</p>
<ul>
<li>描述性统计： <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Five-number_summary">https://en.wikipedia.org/wiki/Five-number_summary到外部站点的链接。</a></li>
<li>汇总数据<a target="_blank" rel="noopener" href="https://www.youtube.com/channel/UCxdR3vGHDUYIOm6RqUJxPPg/videos">https://www.youtube.com/channel/UCxdR3vGHDUYIOm6RqUJxPPg/videos到外部站点的链接。</a></li>
<li>形状、分布、异常值<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Q8b1_dOM8LU">https://www.youtube.com/watch?v=Q8b1_dOM8LU到外部站点的链接。</a></li>
<li>箱形图<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bhCGIUmZeDE">https://www.youtube.com/watch?v=bhCGIUmZeDE到外部站点的链接。</a></li>
</ul>
<p><strong>概率</strong></p>
<p>以下资源将帮助建立&#x2F;修改有关概率的知识。这些是贝叶斯方法中的重要概念，例如朴素贝叶斯算法。</p>
<p><a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2246282644410001811">机器学习数学到外部站点的链接。</a>，第 1.2 章。剑桥大学出版社。</p>
<p><strong>线性代数和微积分</strong></p>
<p>对于本模块，需要熟悉几个数学主题，其中包括：</p>
<ul>
<li>矩阵求逆和转置</li>
<li>矩阵-矩阵运算</li>
<li>矩阵向量运算</li>
<li>向量点积</li>
<li>偏导数</li>
</ul>
<p>以下<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540175?wrap=1"><strong>幻灯片</strong></a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540175/download?download_frd=1"> 下载幻灯片</a>总结这些基本的数学概念。</p>
<p>这是一个关于矩阵向量乘法的视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aV-P3mDgK_E">https://www.youtube.com/watch?v=aV-P3mDgK_E到外部站点的链接。</a></p>
<p> 可以在这里找到很好的数学参考：<a target="_blank" rel="noopener" href="http://cs229.stanford.edu/summer2020/cs229-linalg.pdf">http://cs229.stanford.edu/summer2020/cs229-linalg.pdf到外部站点的链接。</a></p>
<p>以下书籍是深入了解数据科学中使用的数学方法的良好学习来源： Sheldon Axler，<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2242923306420001811">线性代数做得正确到外部站点的链接。</a>，第三版，施普林格。</p>
<p><strong>机器学习和神经网络简介</strong></p>
<p>以下书籍是一本很好的机器学习入门读物，可能有助于复习 ML 概念。</p>
<p>Rebala, G.、Ravi, A. 和 Churiwala, S. (2019)。<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2242923306410001811">机器学习简介到外部站点的链接。</a>。施普林格。</p>
<p>如果您只喜欢摘要，这里有关于经典机器学习各种主题的幻灯片：</p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540176?wrap=1">机器学习方法论</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540176/download?download_frd=1">下载机器学习方法</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540180?wrap=1">k-最近邻</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540180/download?download_frd=1">下载 k 最近邻</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540191?wrap=1">朴素贝叶斯方法</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540191/download?download_frd=1">下载朴素贝叶斯方法</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540184?wrap=1"><strong>幻灯片</strong></a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540184/download?download_frd=1"> 下载幻灯片</a>总结神经网络的基础知识。</p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178?wrap=1"><strong>另外，Python代码</strong></a>的示例<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178?wrap=1"> </a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178/download?download_frd=1"> 下载代码</a>使用这个**<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540204?wrap=1">数据集</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540204/download?download_frd=1"> 下载数据集</a>**用于测试各种机器学习方法。</p>
<p>使用不同的机器学习方法来进行训练范例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support, accuracy_score,classification_report</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> BernoulliNB</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line">Data=pd.read_csv(<span class="string">&#x27;titanic.csv&#x27;</span>)</span><br><span class="line">Data1 = Data[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>,<span class="string">&quot;Survived&quot;</span>]]</span><br><span class="line"><span class="keyword">if</span> Data1.Age <span class="keyword">is</span> <span class="literal">None</span>: Data1[<span class="string">&quot;Age&quot;</span>] = Data1.Age.mean()</span><br><span class="line">Data1 = Data1.dropna()</span><br><span class="line">Data2 = Data1.copy()</span><br><span class="line">Data2[<span class="string">&#x27;Sex&#x27;</span>]=Data1[<span class="string">&#x27;Sex&#x27;</span>].replace([<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;female&#x27;</span>],[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">Y=Data2.Survived</span><br><span class="line">X=Data2</span><br><span class="line">X.drop([<span class="string">&#x27;Survived&#x27;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=<span class="number">0.2</span>, random_state=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Nauve Bayes</span></span><br><span class="line">classifier=BernoulliNB(fit_prior=<span class="literal">True</span>, alpha=<span class="number">1.0</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BernoulliNB\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Decision Tree</span></span><br><span class="line">classifier = tree.DecisionTreeClassifier(min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">    min_samples_split=<span class="number">2</span>, max_depth=<span class="literal">None</span>,criterion=<span class="string">&#x27;entropy&#x27;</span>,random_state=<span class="number">0</span>)</span><br><span class="line">model = classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;DecisionTreeClassifier \n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; sklearn NN &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># p is the parameter for Minkowski distance</span></span><br><span class="line"><span class="comment"># weights can be also &#x27;distance&#x27;</span></span><br><span class="line">classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, p=<span class="number">2</span>, weights=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNeighborsClassifier\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">classifier = MLPClassifier(solver=<span class="string">&#x27;lbfgs&#x27;</span>, alpha=<span class="number">1e-5</span>, max_iter=<span class="number">300</span>,</span><br><span class="line">                    hidden_layer_sizes=(<span class="number">5</span>, <span class="number">2</span>), random_state=<span class="number">1</span>,</span><br><span class="line">                    validation_fraction=<span class="number">0.2</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MLPClassifier\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; keras NN &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="comment"># keras.layers.Dense(units, activation=None, use_bias=True, </span></span><br><span class="line"><span class="comment"># kernel_initializer=&#x27;glorot_uniform&#x27;, </span></span><br><span class="line"><span class="comment"># bias_initializer=&#x27;zeros&#x27;, kernel_regularizer=None, </span></span><br><span class="line"><span class="comment"># bias_regularizer=None, activity_regularizer=None, </span></span><br><span class="line"><span class="comment"># kernel_constraint=None, bias_constraint=None)</span></span><br><span class="line"></span><br><span class="line">classifier = Sequential()</span><br><span class="line"><span class="comment"># Input layer with 5 inputs neurons</span></span><br><span class="line">classifier.add(Dense(<span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>, input_dim = <span class="number">5</span>))</span><br><span class="line"><span class="comment">#Hidden layer</span></span><br><span class="line">classifier.add(Dense(<span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#output layer with 1 output neuron which will predict 1 or 0</span></span><br><span class="line"><span class="comment"># this is why we use sigmoid</span></span><br><span class="line">classifier.add(Dense(<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">classifier.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;adam&#x27;</span>, \</span><br><span class="line">    loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">classifier.fit(X_train, Y_train, batch_size = <span class="number">10</span>, epochs = <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#getting predictions of test data</span></span><br><span class="line">predicted_y_cont = classifier.predict(X_test).tolist()</span><br><span class="line">predicted_y = [<span class="built_in">round</span>(x[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> predicted_y_cont]</span><br><span class="line"><span class="comment"># print(predicted_y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Keras classifier \n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这段代码是一个完整的机器学习项目示例，展示了如何使用不同的分类算法对数据进行训练和测试，以解决泰坦尼克号乘客生存预测问题。</p>
<p><strong>注意</strong></p>
<p>上述内容我已经下载到传到了Github上面。</p>
<h2 id="集合、函数、求和"><a href="#集合、函数、求和" class="headerlink" title="集合、函数、求和"></a>集合、函数、求和</h2><p>当涉及到集合、函数和求和时，我们可以通过一个具体的例子来解释它们的概念和关系。</p>
<p>假设我们有一个集合 $A$，包含了一些整数：</p>
<p>$$<br>A &#x3D; {1, 2, 3, 4, 5}<br>$$</p>
<p>现在，我们定义一个函数 $f$，这个函数将集合 $A$ 中的每个元素映射到它的平方。也就是说，如果 $x$ 是集合 $A$ 中的一个元素，则 $f(x)$ 是 $x$ 的平方。我们可以写成如下的函数表达式：</p>
<p>$$<br>f(x) &#x3D; x^2<br>$$</p>
<p>现在，我们来计算函数 $f$ 应用在集合 $A$ 中每个元素上的结果。</p>
<ul>
<li><p>对于 $x &#x3D; 1$，我们有 $f(1) &#x3D; 1^2 &#x3D; 1$。</p>
</li>
<li><p>对于 $x &#x3D; 2$，我们有 $f(2) &#x3D; 2^2 &#x3D; 4$。</p>
</li>
<li><p>对于 $x &#x3D; 3$，我们有 $f(3) &#x3D; 3^2 &#x3D; 9$。</p>
</li>
<li><p>对于 $x &#x3D; 4$，我们有 $f(4) &#x3D; 4^2 &#x3D; 16$。</p>
</li>
<li><p>对于 $x &#x3D; 5$，我们有 $f(5) &#x3D; 5^2 &#x3D; 25$。</p>
</li>
</ul>
<p>现在，我们可以将这些结果加起来得到一个总和。这就是求和的过程。我们将函数 $f$ 应用到集合 $A$ 中的每个元素上，然后将这些结果相加。</p>
<p>$$<br>\text{总和} &#x3D; f(1) + f(2) + f(3) + f(4) + f(5) &#x3D; 1 + 4 + 9 + 16 + 25 &#x3D; 55<br>$$</p>
<p>所以，这里的总和是 $55$。</p>
<p>在这个例子中，我们看到了集合 $A$，函数 $f$，以及将函数应用于集合元素并对结果求和的过程。这展示了集合、函数和求和之间的关系。</p>
<p>求和符号通常用来表示对一组数值进行求和的过程。在数学中，求和符号通常用希腊字母 sigma（Σ）来表示。具体而言，如果我们有一个序列或集合 $a_1, a_2, …, a_n$，求和符号的使用方式如下：<br>$$<br>\sum_{i&#x3D;1}^{n} a_i<br>$$</p>
<p>这个符号的意思是对 $i$ 从 $1$ 到 $n$ 的每个 $a_i$ 求和。换句话说，它表示对序列中的每个元素求和。</p>
<p>现在让我们通过一个具体的例子来解释：</p>
<p>假设我们有一个序列 $1, 2, 3, 4, 5$，我们想要对这个序列中的所有数进行求和。那么我们可以写成：<br>$$<br>\sum_{i&#x3D;1}^{5} i<br>$$</p>
<p>这里 $i$ 是从 $1$ 到 $5$ 的整数。因此，这个求和符号表示对 $i$ 从 $1$ 到 $5$ 的每个 $i$ 求和。</p>
<p>计算过程如下：</p>
<p>$$<br>\sum_{i&#x3D;1}^{5} i &#x3D; 1 + 2 + 3 + 4 + 5 &#x3D; 15<br>$$</p>
<p>因此，序列 $1, 2, 3, 4, 5$ 的总和是 $15$。</p>
<p>现在，让我们看一个具有多个求和符号的示例。假设我们有一个二维数组 $a_{ij}$，其中 $i$ 表示行，$j$ 表示列。我们想要计算所有元素的总和。</p>
<p>$$<br>\sum_{i&#x3D;1}^{m} \sum_{j&#x3D;1}^{n} a_{ij}<br>$$</p>
<p>这个符号的意思是对所有 $i$ 从 $1$ 到 $m$，以及对所有 $j$ 从 $1$ 到 $n$ 的 $a_{ij}$ 求和。</p>
<p>举例来说，如果我们有一个 $2 \times 2$ 的矩阵：</p>
<p>$$<br>\begin{pmatrix}<br>1 &amp; 2 \<br>3 &amp; 4 \<br>\end{pmatrix}<br>$$</p>
<p>那么总和可以计算为：</p>
<p>$$<br>\sum_{i&#x3D;1}^{2} \sum_{j&#x3D;1}^{2} a_{ij} &#x3D; (1 + 2) + (3 + 4) &#x3D; 10<br>$$</p>
<p>这里，我们首先对每一行的元素进行求和，然后再对行求和的结果进行总和。</p>
<h2 id="线性代数与线性方程"><a href="#线性代数与线性方程" class="headerlink" title="线性代数与线性方程"></a>线性代数与线性方程</h2><p>线性代数是数学的一个分支，主要研究向量空间、线性变换和线性方程组等概念与性质的代数理论。</p>
<p>线性代数的主要内容包括以下几个方面：</p>
<ol>
<li><p><strong>向量空间</strong>：向量空间是线性代数的核心概念之一。向量空间是一个集合，其中的元素称为向量，并满足一定的线性运算规则，例如向量的加法和数乘。向量空间的性质和结构对于理解线性代数的许多概念和定理具有重要意义。</p>
</li>
<li><p><strong>线性变换</strong>：线性变换是指一个向量空间到另一个向量空间的映射，它保持向量空间的线性性质。线性变换可以用矩阵表示，并且矩阵的性质和运算与线性变换之间存在密切的联系。</p>
</li>
<li><p><strong>矩阵理论</strong>：矩阵是线性代数中的重要工具，它们用于表示线性变换、解决线性方程组和进行线性变换的分析。矩阵的性质和运算规则是线性代数理论的基础之一。</p>
</li>
<li><p><strong>线性方程组</strong>：线性方程组是线性代数的一个重要应用领域，它涉及求解一组线性方程的未知数，通常使用矩阵运算和向量运算的方法来求解。</p>
</li>
<li><p><strong>特征值和特征向量</strong>：特征值和特征向量是线性变换的重要性质，它们提供了一种分析线性变换的方法，例如确定矩阵的对角化形式和分解矩阵。</p>
</li>
<li><p><strong>内积空间和正交性</strong>：内积空间是向量空间的一种特殊形式，其中定义了一个内积运算。正交性是内积空间中的重要概念，它描述了向量之间的垂直关系。</p>
</li>
</ol>
<p>线性代数涉及许多常见的计算公式和性质，下面列举了一些常见的线性代数计算公式：</p>
<ol>
<li><p><strong>向量和矩阵的加法和数乘</strong>：</p>
<ul>
<li>$\mathbf{v} + \mathbf{w} &#x3D; (v_1 + w_1, v_2 + w_2, \ldots, v_n + w_n) $</li>
<li>$ k \mathbf{v} &#x3D; (k v_1, k v_2, \ldots, k v_n) $</li>
</ul>
</li>
<li><p><strong>向量的点积（内积）</strong>：</p>
<ul>
<li>$ \mathbf{v} \cdot \mathbf{w} &#x3D; v_1 w_1 + v_2 w_2 + \ldots + v_n w_n $</li>
</ul>
</li>
<li><p><strong>向量的长度（模）</strong>：</p>
<ul>
<li>$ ||\mathbf{v}|| &#x3D; \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2} $</li>
</ul>
</li>
<li><p><strong>矩阵乘法</strong>：</p>
<ul>
<li>若 $ A $ 是一个 $ m \times n $ 的矩阵，$ B $ 是一个 $ n \times p $ 的矩阵，则它们的乘积 $ C &#x3D; AB $ 是一个 $ m \times p $ 的矩阵，其中 $ c_{ij} $ 元素为：</li>
<li>$ c_{ij} &#x3D; a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{in}b_{nj} $</li>
</ul>
</li>
<li><p><strong>矩阵转置</strong>：</p>
<ul>
<li>$ (A^T)<em>{ij} &#x3D; A</em>{ji} $</li>
</ul>
</li>
<li><p><strong>矩阵的逆</strong>：</p>
<ul>
<li>对于一个可逆的 $ n \times n $ 矩阵 $ A $，它的逆矩阵记作 $ A^{-1} $，满足 $ AA^{-1} &#x3D; A^{-1}A &#x3D; I $，其中 $ I $ 是单位矩阵。</li>
</ul>
</li>
<li><p><strong>特征值和特征向量</strong>：</p>
<ul>
<li>对于一个 $ n \times n $ 矩阵 $ A $，如果存在标量 $ \lambda $ 和非零向量 $ \mathbf{v} $ 使得 $ A\mathbf{v} &#x3D; \lambda \mathbf{v} $，则 $ \lambda $ 是矩阵的特征值，$ \mathbf{v} $ 是对应的特征向量。</li>
</ul>
</li>
<li><p><strong>行列式</strong>：</p>
<ul>
<li>对于一个 $ n \times n $ 的方阵 $ A $，其行列式 $ \text{det}(A) $ 表示一个与矩阵相关的标量值，它具有一些重要的性质，如矩阵可逆性和特征值计算等。</li>
</ul>
</li>
</ol>
<p>以上是线性代数中的一些常见的计算公式和性质，它们在理解和应用线性代数的过程中起着重要的作用。</p>
<h2 id="描述性统计知识点"><a href="#描述性统计知识点" class="headerlink" title="描述性统计知识点"></a>描述性统计知识点</h2><p>当涉及描述性统计时，”平均数”，”中位数”和”众数”是常用的统计指标，用于了解数据集的集中趋势。以下是它们的定义和用途：</p>
<ol>
<li><p><strong>平均数（算术平均数）</strong>：</p>
<ul>
<li>定义：平均数是一组数据之和除以数据的个数。它是最常用的描述数据集中心位置的指标。</li>
<li>公式：对于数据集 $x_1, x_2, …, x_n$，平均数 $\bar{x}$ 计算公式为：$\bar{x} &#x3D; \frac{x_1 + x_2 + \ldots + x_n}{n}$</li>
<li>用途：平均数可以提供数据集的集中趋势，但在数据存在极端值（异常值）时，可能会受到影响。</li>
</ul>
</li>
<li><p><strong>中位数</strong>：</p>
<ul>
<li>定义：中位数是将数据集按顺序排列后位于中间位置的数值。如果数据集包含偶数个数据，则中位数是中间两个数的平均值。</li>
<li>计算方法：将数据集从小到大排列，中间位置的数即为中位数。</li>
<li>用途：中位数不受极端值的影响，更能反映数据集的典型值。</li>
</ul>
</li>
<li><p><strong>众数</strong>：</p>
<ul>
<li>定义：众数是数据集中出现频率最高的数值，即数据集中的模式值。</li>
<li>用途：众数可以帮助识别数据集中最常见的值，尤其适用于分类数据。</li>
</ul>
</li>
</ol>
<p>在实际应用中，这三种指标通常会一起使用，以便全面了解数据的分布情况和集中趋势。根据数据的特点和分布，选择合适的指标进行分析和解释。</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>平均数</th>
<th>中位数</th>
<th>众数</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>数据的总和除以个数</td>
<td>排序后位于中间的值</td>
<td>出现频率最高的值</td>
</tr>
<tr>
<td>优点</td>
<td>- 能够反映数据的集中趋势；<br>- 使用范围广泛。</td>
<td>- 不受极端值影响；<br>- 对偏态数据有良好的表示。</td>
<td>- 简单易懂；<br>- 对分类数据适用。</td>
</tr>
<tr>
<td>缺点</td>
<td>- 受极端值影响；<br>- 不能反映数据的分布情况。</td>
<td>- 不利于计算；<br>- 无法反映数据的具体取值。</td>
<td>- 可能存在多个众数；<br>- 对连续型数据不够精确。</td>
</tr>
<tr>
<td>使用的场合</td>
<td>- 数据分布近似对称；<br>- 数据没有极端值。</td>
<td>- 数据存在极端值；<br>- 数据分布不对称。</td>
<td>- 对分类数据分析；<br>- 识别数据集中的模式。</td>
</tr>
</tbody></table>
<p>下面是关于变异程度的常见统计指标的具体讲解和例子：</p>
<ol>
<li><p><strong>标准差</strong>：</p>
<ul>
<li>定义：标准差是一组数据与其平均数之间差异的平方的平均值的平方根。它衡量了数据集的离散程度。</li>
<li>计算方法：标准差的计算公式为数据与平均数的差的平方和的平均值的平方根。<br>  $\text{标准差} &#x3D; \sqrt{\frac{\sum_{i&#x3D;1}^{n}(x_i - \bar{x})^2}{n - 1}}$</li>
<li>例子：假设有一组数据集 {3, 6, 9, 12, 15}，平均数 $\bar{x} &#x3D; \frac{3 + 6 + 9 + 12 + 15}{5} &#x3D; 9$。计算标准差，即$\sqrt{\frac{(3-9)^2 + (6-9)^2 + (9-9)^2 + (12-9)^2 + (15-9)^2}{5}}$，结果为标准差为 4.24。</li>
</ul>
</li>
<li><p><strong>方差</strong>：</p>
<ul>
<li>定义：方差是一组数据与其平均数之间差异的平方的平均值，是标准差的平方。</li>
<li>计算方法：方差的计算公式与标准差类似，只是不取平方根。<br>  $方差 &#x3D; \frac{\sum_{i&#x3D;1}^{n}(x_i - \bar{x})^2}{n}$</li>
<li>例子：在上面的例子中，标准差的平方即为方差，所以方差为 $4.24^2 &#x3D; 18$。</li>
</ul>
</li>
<li><p><strong>范围</strong>：</p>
<ul>
<li>定义：范围是数据集中最大值与最小值之间的差异，用来衡量数据的波动范围。</li>
<li>计算方法：最大值减去最小值即可得到范围。</li>
<li>例子：对于数据集 {3, 6, 9, 12, 15}，范围为 $ 15 - 3 &#x3D; 12 $。</li>
</ul>
</li>
<li><p><strong>四分位数和四分位差</strong>：</p>
<ul>
<li>定义：四分位数将数据集分成四等份，分别是最小值、下四分位数、中位数、上四分位数和最大值。四分位差是上四分位数和下四分位数之间的差异。</li>
<li>计算方法：四分位数可通过将数据集按大小排序后找到对应位置的值得到。四分位差即上四分位数减去下四分位数。</li>
<li>例子：对于数据集 {3, 6, 9, 12, 15}，上四分位数为 9，下四分位数为 6，四分位差为 $ 9 - 6 &#x3D; 3 $。</li>
</ul>
</li>
<li><p><strong>百分位数</strong>：</p>
<ul>
<li>定义：百分位数是数据集中所有观测值中对应百分比的值。例如，中位数就是50th百分位数。</li>
<li>计算方法：将数据按大小排序后，找到相应百分比位置的值即可。</li>
<li>例子：中位数是50th百分位数。对于数据集 {3, 6, 9, 12, 15}，中位数是 9。</li>
</ul>
</li>
</ol>
<p>这些指标提供了关于数据集变异程度的不同方面的信息，有助于更全面地了解数据的分布和离散程度。</p>
<p>为什么方差要n-1：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102043269">https://zhuanlan.zhihu.com/p/102043269</a></p>
<p>数据分布是描述数据集中数据分布情况的统计概念，正态分布参数、偏度和峰度是常用于描述数据分布形态的指标。</p>
<ol>
<li><p><strong>正态分布参数（均值、标准差）</strong>：</p>
<ul>
<li>正态分布是统计学中最常见的分布形式，也称为钟形曲线。其特点是具有对称的形态，均值和标准差决定了分布的中心位置和数据的离散程度。</li>
<li>均值：正态分布的均值代表数据集的中心位置，位于分布的中心点。</li>
<li>标准差：正态分布的标准差决定了数据点相对于均值的分布集中程度，标准差越大，数据分布越分散。</li>
</ul>
</li>
<li><p><strong>偏度（Skewness）</strong>：</p>
<ul>
<li>偏度是描述数据分布偏斜程度的统计量。当数据集的分布向左偏或向右偏时，偏度值会偏离零。</li>
<li>正偏态（右偏）数据分布的尾部向右延伸，均值位于中位数右侧。</li>
<li>负偏态（左偏）数据分布的尾部向左延伸，均值位于中位数左侧。</li>
<li>偏度为0表示数据分布对称。</li>
</ul>
</li>
<li><p><strong>峰度（Kurtosis）</strong>：</p>
<ul>
<li>峰度用于描述数据集中数据分布形态的尖峰程度。它与正态分布进行比较。</li>
<li>正态分布的峰度为3，称为Mesokurtic，表示与正态分布的峰度相同。</li>
<li>高峰度（Leptokurtic）数据分布具有比正态分布更尖锐的峰度，峰度大于3。</li>
<li>低峰度（Platykurtic）数据分布具有比正态分布更平坦的峰度，峰度小于3。</li>
</ul>
</li>
</ol>
<p>这些统计指标帮助我们了解数据的分布情况、偏斜程度以及尖峰程度，从而更好地进行数据分析和解释。在应用中，可以使用这些指标来评估数据集的形态并做出相应的推断。</p>
<p>频率分布是描述数据集中各个数值或类别出现次数的统计概念。在频率分布中，常见的指标包括频数、相对频率和累积频率。</p>
<ol>
<li><p><strong>频数</strong>：</p>
<ul>
<li>频数是指某个数值或类别在数据集中出现的次数。它表示了数据集中各个数值或类别的出现情况。</li>
<li>例如，对于一个骰子投掷的结果，1至6的频数分别表示投掷到1、2、3、4、5、6的次数。</li>
</ul>
</li>
<li><p><strong>相对频率</strong>：</p>
<ul>
<li>相对频率是指某个数值或类别在数据集中出现的频数与总样本量之比。它表示了每个数值或类别在整体中的占比情况。</li>
<li>相对频率可以通过将频数除以样本量得到。</li>
<li>例如，如果一个数值的频数为10，总样本量为100，则该数值的相对频率为10%。</li>
</ul>
</li>
<li><p><strong>累积频率</strong>：</p>
<ul>
<li>累积频率是指累计到某个数值或类别的频数之和。它表示了数据集中小于或等于某个数值或类别的所有数值的总频数。</li>
<li>例如，如果数据集中小于或等于3的频数为20，小于或等于4的频数为40，则小于或等于4的累积频率为40。</li>
</ul>
</li>
</ol>
<p>频率分布可以通过这些指标来分析数据集中各个数值或类别的分布情况，以及它们在整体中的相对重要性。通过频率分布，可以更好地理解数据集的特征和结构，从而进行更深入的数据分析和解释。</p>
<p>集中度与离散度是描述数据分布或变异程度的统计概念。常见的指标包括变异系数、相对离散度和相对标准差。</p>
<ol>
<li><p><strong>变异系数</strong>：</p>
<ul>
<li>变异系数是数据标准差与数据平均值的比值，通常用来比较不同数据集之间的离散程度，尤其是当数据集的均值大小差异较大时。</li>
<li>公式：变异系数（CV）等于标准差（SD）除以均值（Mean），再乘以100%。</li>
<li>通常以百分比的形式表示，可以用来比较不同单位或量级的数据集的离散程度。</li>
</ul>
</li>
<li><p><strong>相对离散度</strong>：</p>
<ul>
<li>相对离散度是指标准差与均值的比值，用来衡量数据集中的离散程度。</li>
<li>公式：相对离散度等于标准差（SD）除以均值（Mean）。</li>
<li>相对离散度的值越大，表示数据集中的波动程度越大，相对离散度的值越小，表示数据集中的波动程度越小。</li>
</ul>
</li>
<li><p><strong>相对标准差</strong>：</p>
<ul>
<li>相对标准差是标准差与均值的比值，类似于相对离散度。</li>
<li>公式：相对标准差等于标准差（SD）除以均值（Mean）。</li>
<li>相对标准差的计算方式与相对离散度相同，但是它不以百分比形式表示。</li>
</ul>
</li>
</ol>
<p>这些指标帮助我们了解数据集的离散程度和集中趋势，以及不同数据集之间的比较。通过这些指标，可以更好地理解数据集的特征和结构，从而进行更深入的数据分析和解释。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个示例数据集</span></span><br><span class="line">data = np.array([<span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的均值和标准差</span></span><br><span class="line">mean_value = np.mean(data)</span><br><span class="line">std_deviation = np.std(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算变异系数</span></span><br><span class="line">coefficient_of_variation = (std_deviation / mean_value) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相对离散度</span></span><br><span class="line">relative_dispersion = std_deviation / mean_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相对标准差</span></span><br><span class="line">relative_standard_deviation = std_deviation / mean_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集的均值：&quot;</span>, mean_value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集的标准差：&quot;</span>, std_deviation)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;变异系数：&quot;</span>, coefficient_of_variation)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相对离散度：&quot;</span>, relative_dispersion)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相对标准差：&quot;</span>, relative_standard_deviation)</span><br></pre></td></tr></table></figure>

<p>计算结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据集的均值： <span class="number">25.0</span></span><br><span class="line">数据集的标准差： <span class="number">7.0710678118654755</span></span><br><span class="line">变异系数： <span class="number">28.284271247461902</span></span><br><span class="line">相对离散度： <span class="number">0.282842712474619</span></span><br><span class="line">相对标准差： <span class="number">0.282842712474619</span></span><br></pre></td></tr></table></figure>

<p>如何使用变异系数来比较两个数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个示例数据集</span></span><br><span class="line">data1 = np.array([<span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>])</span><br><span class="line">data2 = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的均值和标准差</span></span><br><span class="line">mean_value1 = np.mean(data1)</span><br><span class="line">mean_value2 = np.mean(data2)</span><br><span class="line">std_deviation1 = np.std(data1)</span><br><span class="line">std_deviation2 = np.std(data2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的变异系数</span></span><br><span class="line">coefficient_of_variation1 = (std_deviation1 / mean_value1) * <span class="number">100</span></span><br><span class="line">coefficient_of_variation2 = (std_deviation2 / mean_value2) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集1的变异系数：&quot;</span>, coefficient_of_variation1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集2的变异系数：&quot;</span>, coefficient_of_variation2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个数据集的变异系数</span></span><br><span class="line"><span class="keyword">if</span> coefficient_of_variation1 &lt; coefficient_of_variation2:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据集1的变异程度较小&quot;</span>)</span><br><span class="line"><span class="keyword">elif</span> coefficient_of_variation1 &gt; coefficient_of_variation2:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据集2的变异程度较小&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;两个数据集的变异程度相等&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据集<span class="number">1</span>的变异系数： <span class="number">28.284271247461902</span></span><br><span class="line">数据集<span class="number">2</span>的变异系数： <span class="number">47.14045207910317</span></span><br><span class="line">数据集<span class="number">1</span>的变异程度较小</span><br></pre></td></tr></table></figure>

<p>当说数据集1的变异程度较小时，意味着数据集1中的数据点相对于其平均值的分布更为集中，波动性较小。换句话说，数据集1中的数据点更趋向于围绕着其均值附近波动，而不是偏离均值太远。</p>
<p>这对于数据分析和解释来说是有意义的，因为一个变异程度较小的数据集通常意味着数据点之间的差异性较小，数据集的稳定性较高。这样的数据集更容易预测和理解，因为数据点的波动不会太大，更接近数据集的中心趋势。</p>
<p>以下是直方图、箱线图、散点图、折线图和饼图的对比表格：</p>
<table>
<thead>
<tr>
<th>图形类型</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>直方图</td>
<td>- 显示数据分布的频率或频数；<br>- x轴表示数据范围，y轴表示频数或频率。</td>
<td>- 描述数据分布的形态；<br>- 比较不同组之间的差异。</td>
</tr>
<tr>
<td>箱线图</td>
<td>- 显示数据的分散情况和离群值；<br>- 展示了数据的中位数、四分位数和异常值。</td>
<td>- 比较数据集的分布；<br>- 发现异常值和离群点。</td>
</tr>
<tr>
<td>散点图</td>
<td>- 显示两个变量之间的关系；<br>- 每个数据点表示为平面上的一个点。</td>
<td>- 描述变量之间的相关性；<br>- 观察数据的分布规律。</td>
</tr>
<tr>
<td>折线图</td>
<td>- 通过连线表示数据随时间或有序类别的变化趋势；<br>- x轴表示时间或类别，y轴表示数值。</td>
<td>- 描述数据随时间的变化；<br>- 比较不同时间点的趋势。</td>
</tr>
<tr>
<td>饼图</td>
<td>- 用圆形将数据分成各个部分；<br>- 表示各个部分占总体的比例。</td>
<td>- 展示数据组成的比例；<br>- 显示分类数据的比例关系。</td>
</tr>
</tbody></table>
<p>这些图形类型各有特点，适用于不同类型的数据分析和展示。根据数据的特点和分析目的，可以选择合适的图形类型来呈现数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 生成示例数据</span><br><span class="line">x = np.linspace(0, 10, 100)</span><br><span class="line">y1 = np.random.normal(0, 1, 100)</span><br><span class="line">y2 = np.random.normal(2, 1, 100)</span><br><span class="line">y3 = np.random.normal(5, 2, 100)</span><br><span class="line"></span><br><span class="line"># 创建画布和子图</span><br><span class="line">fig, axs = plt.subplots(2, 2, figsize=(12, 8))</span><br><span class="line"></span><br><span class="line"># 绘制直方图</span><br><span class="line">axs[0, 0].hist(y1, bins=20, color=&#x27;blue&#x27;, alpha=0.7, label=&#x27;Histogram&#x27;)</span><br><span class="line">axs[0, 0].set_title(&#x27;Histogram&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制箱线图</span><br><span class="line">axs[0, 1].boxplot([y1, y2, y3], labels=[&#x27;Dataset 1&#x27;, &#x27;Dataset 2&#x27;, &#x27;Dataset 3&#x27;])</span><br><span class="line">axs[0, 1].set_title(&#x27;Boxplot&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制散点图</span><br><span class="line">axs[1, 0].scatter(x, y1, color=&#x27;red&#x27;, label=&#x27;Scatter&#x27;)</span><br><span class="line">axs[1, 0].set_title(&#x27;Scatter Plot&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制折线图</span><br><span class="line">axs[1, 1].plot(x, y1, color=&#x27;green&#x27;, label=&#x27;Line&#x27;)</span><br><span class="line">axs[1, 1].set_title(&#x27;Line Plot&#x27;)</span><br><span class="line"></span><br><span class="line"># 添加图例</span><br><span class="line">axs[0, 0].legend()</span><br><span class="line">axs[1, 0].legend()</span><br><span class="line">axs[1, 1].legend()</span><br><span class="line"></span><br><span class="line"># 调整布局</span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line"># 显示图形</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303192849562.png" alt="image-20240303192849562"></p>
<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><p>概率论知识点回顾这篇博客：<a target="_blank" rel="noopener" href="https://yejiu97.github.io/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/">https://yejiu97.github.io/2024/02/07/%E6%A6%82%E7%8E%87%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/</a></p>
<h2 id="朴素贝叶斯与分类器"><a href="#朴素贝叶斯与分类器" class="headerlink" title="朴素贝叶斯与分类器"></a>朴素贝叶斯与分类器</h2><p><strong>贝叶斯定理</strong></p>
<p>贝叶斯定理是概率论中的一个基本定理，它描述了在已知先验信息的情况下，如何通过新的观察数据来更新我们对事件的概率估计。贝叶斯定理在统计学、机器学习和人工智能等领域中都有广泛的应用。</p>
<p>贝叶斯定理的数学表达如下：</p>
<p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<p>其中：</p>
<ul>
<li>$P(A|B)$ 表示在给定B发生的条件下A发生的概率，也称为后验概率。</li>
<li>$P(B|A)$ 表示在给定A发生的条件下B发生的概率，也称为似然度。</li>
<li>$P(A)$ 和 $P(B)$ 分别是A和B的边缘概率，也称为先验概率和边缘似然度。</li>
</ul>
<p>拓展视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1R7411a76r/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1R7411a76r/?spm_id_from=333.337.search-card.all.click</a></p>
<p>下面是一个简单的例子，展示了如何使用贝叶斯定理进行推断：</p>
<p>假设某地区有一种罕见疾病，该疾病的患病率很低，只有1%的人口患病。现在，我们有一种测试方法，该测试在患病人群中能够正确诊断出99%的患者，但在健康人群中也有1%的误诊率。</p>
<p>现在有一个人进行了测试，测试结果为阳性。我们想知道这个人实际上患病的概率是多少。</p>
<p>我们用事件A表示这个人患病，事件B表示测试结果为阳性。</p>
<p>根据题设可知：</p>
<ul>
<li>$P(A) &#x3D; 0.01$（先验概率，即患病率）</li>
<li>$P(B|A) &#x3D; 0.99$ （在患病的情况下，测试结果为阳性的概率）</li>
<li>$P(B|\neg A) &#x3D; 0.01$ （在健康的情况下，测试结果为阳性的概率）</li>
</ul>
<p>我们需要计算后验概率$P(A|B)$，即在测试结果为阳性的情况下，这个人实际上患病的概率。</p>
<p>根据贝叶斯定理：</p>
<p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<p>其中，$P(B) &#x3D; P(B|A)P(A) + P(B|\neg A)P(\neg A)$</p>
<p>$P(B) &#x3D; 0.99 \times 0.01 + 0.01 \times 0.99 &#x3D; 0.0099 + 0.0099 &#x3D; 0.0198$</p>
<p>因此</p>
<p>$P(A|B) &#x3D; \frac{0.99 \times 0.01}{0.0198} \approx \frac{0.0099}{0.0198} \approx 0.5$</p>
<p>即在测试结果为阳性的情况下，这个人实际上患病的概率约为50%。这个例子展示了贝叶斯定理在推断中的应用。</p>
<p><strong>朴素贝叶斯与朴素贝叶斯分类器</strong></p>
<p>朴素贝叶斯（Naive Bayes）是基于贝叶斯定理和特征条件独立假设的一种简单且高效的分类算法。朴素贝叶斯分类器是基于朴素贝叶斯算法构建的分类模型。</p>
<p>朴素贝叶斯算法的关键假设是，给定类别，每个特征之间是条件独立的，也就是说，特征之间的存在概率不受其他特征的影响。尽管这个假设在实际情况中很少成立，但在实践中，朴素贝叶斯仍然表现出令人满意的性能，特别是在文本分类和垃圾邮件过滤等任务中。</p>
<p>朴素贝叶斯分类器的工作原理如下：</p>
<ol>
<li><p><strong>训练阶段</strong>：通过已知分类标签的数据集，计算每个特征在每个类别下的概率。</p>
</li>
<li><p><strong>预测阶段</strong>：对于新的数据点，计算在每个类别下的后验概率，然后选择具有最高后验概率的类别作为预测结果。</p>
</li>
</ol>
<p>在文本分类任务中，朴素贝叶斯常常被使用。例如，给定一些文档，我们想要将它们归类到不同的类别中，比如“体育”、“政治”、“科技”等等。我们可以将每个文档表示为特征向量，其中特征是词汇表中的词，特征的值可以是词出现的频率或者TF-IDF等。然后，利用朴素贝叶斯分类器来对这些特征向量进行分类。</p>
<p>下面是一个简单的朴素贝叶斯分类器的示例：</p>
<p>假设我们有一个数据集，包含两个类别（”是”和”否”），每个样本有两个特征（特征1和特征2）。</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>特征1</th>
<th>特征2</th>
<th>类别</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>高</td>
<td>高</td>
<td>是</td>
</tr>
<tr>
<td>2</td>
<td>高</td>
<td>低</td>
<td>是</td>
</tr>
<tr>
<td>3</td>
<td>低</td>
<td>高</td>
<td>否</td>
</tr>
<tr>
<td>4</td>
<td>低</td>
<td>低</td>
<td>否</td>
</tr>
</tbody></table>
<p>我们想要对一个新样本（特征1&#x3D;高，特征2&#x3D;低）进行分类。</p>
<p>首先，计算类别的先验概率：</p>
<ul>
<li>$P(\text{“是”}) &#x3D; \frac{2}{4} &#x3D; 0.5$</li>
<li>$P(\text{“否”}) &#x3D; \frac{2}{4} &#x3D; 0.5$</li>
</ul>
<p>然后，计算条件概率：</p>
<ul>
<li>$P(\text{“高”}|\text{“是”}) &#x3D; \frac{2}{2} &#x3D; 1$</li>
<li>$P(\text{“低”}|\text{“是”}) &#x3D; \frac{0}{2} &#x3D; 0$</li>
<li>$P(\text{“高”}|\text{“否”}) &#x3D; \frac{1}{2} &#x3D; 0.5$</li>
<li>$P(\text{“低”}|\text{“否”}) &#x3D; \frac{1}{2} &#x3D; 0.5$</li>
</ul>
<p>计算后验概率：</p>
<p>对于新样本（特征1&#x3D;高，特征2&#x3D;低），我们计算在每个类别下的后验概率：</p>
<ul>
<li><p>对于类别”是”： </p>
<ul>
<li>$P(\text{“是”}|\text{“高”}, \text{“低”}) &#x3D; P(\text{“高”}|\text{“是”}) \times P(\text{“低”}|\text{“是”}) \times P(\text{“是”}) &#x3D; 1 \times 0 \times 0.5 &#x3D; 0$</li>
</ul>
</li>
<li><p>对于类别”否”： </p>
<ul>
<li>$P(\text{“否”}|\text{“高”}, \text{“低”}) &#x3D; P(\text{“高”}|\text{“否”}) \times P(\text{“低”}|\text{“否”}) \times P(\text{“否”}) &#x3D; 0.5 \times 0.5 \times 0.5 &#x3D; 0.125$</li>
</ul>
</li>
</ul>
<p>因此，我们预测新样本属于类别”否”。</p>
<p>Python语言代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NaiveBayesClassifier</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.classes = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        self.class_feature_counts = defaultdict(<span class="keyword">lambda</span>: defaultdict(<span class="built_in">int</span>))</span><br><span class="line">        self.class_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        self.total_samples = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="keyword">for</span> features, label <span class="keyword">in</span> <span class="built_in">zip</span>(X, y):</span><br><span class="line">            self.classes[label] += <span class="number">1</span></span><br><span class="line">            self.class_counts[label] += <span class="built_in">sum</span>(features)</span><br><span class="line">            <span class="keyword">for</span> idx, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">                self.class_feature_counts[label][idx] += feature</span><br><span class="line">            self.total_samples += <span class="built_in">sum</span>(features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        predictions = []</span><br><span class="line">        <span class="keyword">for</span> features <span class="keyword">in</span> X:</span><br><span class="line">            max_prob, max_class = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> label <span class="keyword">in</span> self.classes:</span><br><span class="line">                log_prob = <span class="number">0</span></span><br><span class="line">                class_count = self.classes[label]</span><br><span class="line">                class_total = self.class_counts[label]</span><br><span class="line">                <span class="keyword">for</span> idx, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">                    <span class="comment"># Laplace smoothing for unseen features</span></span><br><span class="line">                    feature_count = self.class_feature_counts[label][idx] + <span class="number">1</span></span><br><span class="line">                    log_prob += feature * (class_count / class_total)</span><br><span class="line">                <span class="keyword">if</span> log_prob &gt; max_prob:</span><br><span class="line">                    max_prob, max_class = log_prob, label</span><br><span class="line">            predictions.append(max_class)</span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X_train = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">]</span><br><span class="line">y_train = [<span class="string">&quot;yes&quot;</span>, <span class="string">&quot;yes&quot;</span>, <span class="string">&quot;no&quot;</span>, <span class="string">&quot;no&quot;</span>]</span><br><span class="line"></span><br><span class="line">X_test = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并训练朴素贝叶斯分类器</span></span><br><span class="line">clf = NaiveBayesClassifier()</span><br><span class="line">clf.train(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">predictions = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predictions:&quot;</span>, predictions)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predictions: [<span class="string">&#x27;no&#x27;</span>, <span class="string">&#x27;no&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><strong>什么是PCA</strong></p>
<p>PCA是主成分分析（Principal Component Analysis）的缩写。它是一种常用的数据降维技术，用于发现数据集中的主要特征并将其转换为一个新的坐标系，以便更好地理解数据的结构和减少数据的维度。</p>
<p>主成分分析的目标是通过线性变换将原始数据映射到一个新的坐标系上，使得在新的坐标系下，数据的方差被最大化。通过这种方式，PCA找到了数据中最重要的方向，也就是最重要的主成分。这些主成分是原始特征的线性组合，它们按照数据中的方差递减的顺序排列，因此第一个主成分包含了最大方差，第二个主成分包含了第二大方差，以此类推。</p>
<p>PCA在数据预处理、特征提取、数据可视化等领域被广泛应用，它可以帮助我们减少数据的维度、去除数据中的噪声、发现数据中的模式和结构，从而更好地理解数据并进行有效的分析。</p>
<p><strong>一个例子</strong></p>
<p>有一个包含两个特征的数据集，分别是房子的面积和房子的价格。你想要使用PCA来降低数据的维度，以便更好地理解数据并进行可视化。</p>
<p>原始数据集示例（每行代表一个样本，每列代表一个特征）：</p>
<table>
<thead>
<tr>
<th>面积（平方米）</th>
<th>价格（万元）</th>
</tr>
</thead>
<tbody><tr>
<td>70</td>
<td>300</td>
</tr>
<tr>
<td>90</td>
<td>450</td>
</tr>
<tr>
<td>110</td>
<td>500</td>
</tr>
<tr>
<td>60</td>
<td>250</td>
</tr>
</tbody></table>
<p>首先，通过PCA，我们可以找到这两个特征的主成分。假设找到的主成分为PC1和PC2。</p>
<p>PC1和PC2是原始特征的线性组合，按照方差的递减顺序排列。在新的坐标系中，我们可以得到降维后的数据：</p>
<table>
<thead>
<tr>
<th>PC1</th>
<th>PC2</th>
</tr>
</thead>
<tbody><tr>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr>
<td>2.5</td>
<td>0.5</td>
</tr>
<tr>
<td>3.0</td>
<td>0.8</td>
</tr>
<tr>
<td>1.0</td>
<td>0.1</td>
</tr>
</tbody></table>
<p>在这个例子中，我们将原始数据集的两个特征通过PCA转换成了两个主成分。这样，我们可以用这两个主成分来表示原始数据，而不是使用原始的面积和价格。这有助于减少维度，提取数据中的主要结构，并且在新的坐标系中更容易进行可视化和分析。</p>
<p><strong>一个更加复杂的例子</strong></p>
<p>假设我们有一个包含多个特征的数据集，表示不同学生在不同科目上的成绩。我们想使用PCA来降低数据的维度，并尝试用两个主成分来表示学生的成绩情况。</p>
<p>首先，让我们用Python来实现PCA：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">class PCA:</span><br><span class="line">    def __init__(self, n_components):</span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components = None</span><br><span class="line">        self.mean = None</span><br><span class="line"></span><br><span class="line">    def fit(self, X):</span><br><span class="line">        # 计算均值</span><br><span class="line">        self.mean = np.mean(X, axis=0)</span><br><span class="line">        # 中心化数据</span><br><span class="line">        X = X - self.mean</span><br><span class="line">        # 计算协方差矩阵</span><br><span class="line">        cov_matrix = np.cov(X.T)</span><br><span class="line">        # 计算特征值和特征向量</span><br><span class="line">        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)</span><br><span class="line">        # 对特征向量进行排序</span><br><span class="line">        eigenvectors = eigenvectors.T</span><br><span class="line">        idxs = np.argsort(eigenvalues)[::-1]</span><br><span class="line">        self.components = eigenvectors[idxs][:self.n_components]</span><br><span class="line"></span><br><span class="line">    def transform(self, X):</span><br><span class="line">        # 中心化数据</span><br><span class="line">        X = X - self.mean</span><br><span class="line">        # 转换数据</span><br><span class="line">        return np.dot(X, self.components.T)</span><br><span class="line"></span><br><span class="line"># 测试PCA</span><br><span class="line">data = np.array([[80, 85, 90],</span><br><span class="line">                 [75, 82, 80],</span><br><span class="line">                 [90, 91, 95],</span><br><span class="line">                 [60, 70, 80]])</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=2)</span><br><span class="line">pca.fit(data)</span><br><span class="line">transformed_data = pca.transform(data)</span><br><span class="line">print(&quot;降维后的数据：&quot;)</span><br><span class="line">print(transformed_data)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，我们首先导入了NumPy库，然后定义了一个名为PCA的类，其中包含了fit方法用于拟合数据和transform方法用于转换数据。在fit方法中，我们计算了数据的均值、中心化数据、协方差矩阵、特征值和特征向量，并对特征向量进行了排序以得到主成分。在transform方法中，我们对数据进行了中心化和转换操作。</p>
<p>在测试部分，我们定义了一个包含四个学生在三个科目上成绩的数据集，并使用PCA将其降维到两个主成分。最后，我们打印了降维后的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">降维后的数据：</span><br><span class="line">[[ -<span class="number">5.90164697</span>   <span class="number">1.42807004</span>]</span><br><span class="line"> [  <span class="number">3.46997375</span>  -<span class="number">5.3463793</span> ]</span><br><span class="line"> [-<span class="number">18.56233444</span>   <span class="number">1.39753199</span>]</span><br><span class="line"> [ <span class="number">20.99400765</span>   <span class="number">2.52077727</span>]]</span><br></pre></td></tr></table></figure>

<p>这些数据表示了原始数据集中每个学生在新的两个主成分上的投影。通过降维，我们将原始的三个特征（科目成绩）转换成了两个主成分，从而减少了数据的维度，但仍然保留了大部分的信息。</p>
<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>KNN是K-Nearest Neighbors（K近邻）的缩写，是一种常用的机器学习算法，用于分类和回归问题。在KNN算法中，给定一个新的数据点，它的类别或数值输出是由其最接近的K个邻居的类别或数值输出决定的。</p>
<p>KNN算法的工作原理很简单：对于给定的新数据点，计算它与训练数据集中所有数据点的距离，然后选取与之最近的K个数据点。对于分类问题，K个最近邻居中最常见的类别将被赋予新数据点。对于回归问题，K个最近邻居的输出的平均值或加权平均值将作为新数据点的输出。</p>
<p>确定K值的数量对于KNN的性能和泛化能力至关重要。K值太小可能会导致模型过拟合，而K值太大可能会导致模型欠拟合。以下是一些确定K值的常用方法：</p>
<ol>
<li><p><strong>经验法则：</strong> 通常来说，K值的选择可以通过经验法则进行初步确定。一般来说，选择较小的K值（如3、5或7）是一个不错的起点，然后通过交叉验证等方法进行调整。</p>
</li>
<li><p><strong>交叉验证（Cross-Validation）：</strong> 使用交叉验证来评估不同K值的性能。通过将数据集分成训练集和验证集，然后在验证集上评估模型的性能，选择性能最好的K值。常用的交叉验证方法包括k折交叉验证和留一法交叉验证。</p>
</li>
<li><p><strong>网格搜索（Grid Search）：</strong> 通过网格搜索技术来自动化地尝试不同的K值并选择最优的K值。网格搜索将指定的K值列表中的每个值都尝试一次，并根据交叉验证的结果选择最佳的K值。</p>
</li>
<li><p><strong>启发式方法：</strong> 有时可以根据数据集的特性和问题的性质来选择K值。例如，如果数据集具有明显的类别边界，则较小的K值可能更适合；如果数据集的类别之间有较多的重叠，则较大的K值可能更适合。</p>
</li>
<li><p><strong>调试和验证：</strong> 最后，还可以通过尝试不同的K值并评估模型的性能来进行手动调试和验证。</p>
</li>
</ol>
<p>总之，选择合适的K值需要结合经验、交叉验证、网格搜索等方法，以及对数据集和问题的理解，来找到最适合的K值。</p>
<p>在KNN算法中，常见的距离度量方法包括：</p>
<ol>
<li><p>欧氏距离（Euclidean Distance）：欧氏距离是最常见的距离度量方法，也是我们直觉上理解的距离。在二维空间中，欧氏距离可以通过勾股定理计算得出。在n维空间中，欧氏距离的计算公式为：$d(\mathbf{p}, \mathbf{q}) &#x3D; \sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \ldots + (q_n - p_n)^2}$</p>
</li>
<li><p>曼哈顿距离（Manhattan Distance）：曼哈顿距离也称为城市街区距离或L1距离，它是两个点在标准坐标系上的绝对轴距总和。在二维空间中，曼哈顿距离可以通过横纵坐标的差的绝对值之和计算得出。在n维空间中，曼哈顿距离的计算公式为：$d(\mathbf{p}, \mathbf{q}) &#x3D; |q_1 - p_1| + |q_2 - p_2| + \ldots + |q_n - p_n|$</p>
</li>
<li><p>切比雪夫距离（Chebyshev Distance）：切比雪夫距离是两个点在n维空间中的各坐标数值差的绝对值的最大值。在二维空间中，切比雪夫距离就是两点在坐标系上横、纵坐标差值的最大值。</p>
</li>
</ol>
<p>除了这些常见的距离度量方法之外，还有其他一些特定领域或问题所使用的距离度量方法。在KNN算法中，选择合适的距离度量方法对算法的性能和结果影响很大。</p>
<p>可以观看：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Fv4y1G7rB/?spm_id_from=333.337.search-card.all.click&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV1Fv4y1G7rB/?spm_id_from=333.337.search-card.all.click&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></p>
<p>这是一个Python代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一组数据</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">100</span>, n_features=<span class="number">2</span>, n_classes=<span class="number">3</span>, n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           n_informative=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_repeated=<span class="number">0</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个网格来绘制决策边界</span></span><br><span class="line">h = <span class="number">.02</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建颜色映射</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">&#x27;#FFAAAA&#x27;</span>, <span class="string">&#x27;#AAFFAA&#x27;</span>, <span class="string">&#x27;#AAAAFF&#x27;</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">&#x27;#FF0000&#x27;</span>, <span class="string">&#x27;#00FF00&#x27;</span>, <span class="string">&#x27;#0000FF&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建subplot</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原始数据</span></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold, edgecolor=<span class="string">&#x27;k&#x27;</span>, s=<span class="number">20</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">&quot;Original Data&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同距离度量方法进行循环</span></span><br><span class="line"><span class="keyword">for</span> i, distance <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&#x27;euclidean&#x27;</span>, <span class="string">&#x27;manhattan&#x27;</span>, <span class="string">&#x27;chebyshev&#x27;</span>]):</span><br><span class="line">    <span class="comment"># 使用KNN分类器</span></span><br><span class="line">    clf = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, metric=distance)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制决策边界</span></span><br><span class="line">    axes[(i+<span class="number">1</span>)//<span class="number">2</span>, (i+<span class="number">1</span>)%<span class="number">2</span>].pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line">    axes[(i+<span class="number">1</span>)//<span class="number">2</span>, (i+<span class="number">1</span>)%<span class="number">2</span>].scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold, edgecolor=<span class="string">&#x27;k&#x27;</span>, s=<span class="number">20</span>)</span><br><span class="line">    axes[(i+<span class="number">1</span>)//<span class="number">2</span>, (i+<span class="number">1</span>)%<span class="number">2</span>].set_title(<span class="string">f&quot;KNN with <span class="subst">&#123;distance.capitalize()&#125;</span> Distance&quot;</span>)</span><br><span class="line">    axes[(i+<span class="number">1</span>)//<span class="number">2</span>, (i+<span class="number">1</span>)%<span class="number">2</span>].set_xlabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">    axes[(i+<span class="number">1</span>)//<span class="number">2</span>, (i+<span class="number">1</span>)%<span class="number">2</span>].set_ylabel(<span class="string">&quot;Feature 2&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>这是结果：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303224742692.png" alt="image-20240303224742692"></p>
<p>实现一下knn算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KNN</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k</span>):</span><br><span class="line">        self.k = k</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X_train, y_train</span>):</span><br><span class="line">        self.X_train = X_train</span><br><span class="line">        self.y_train = y_train</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X_test</span>):</span><br><span class="line">        y_pred = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X_test:</span><br><span class="line">            distances = np.sqrt(np.<span class="built_in">sum</span>((self.X_train - x)**<span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line">            nearest_neighbors = np.argsort(distances)[:self.k]</span><br><span class="line">            nearest_labels = self.y_train[nearest_neighbors]</span><br><span class="line">            unique_labels, counts = np.unique(nearest_labels, return_counts=<span class="literal">True</span>)</span><br><span class="line">            y_pred.append(unique_labels[np.argmax(counts)])</span><br><span class="line">        <span class="keyword">return</span> np.array(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试KNN算法</span></span><br><span class="line"><span class="comment"># 假设我们有一个简单的数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;X1&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    <span class="string">&#x27;X2&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    <span class="string">&#x27;Y&#x27;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分特征和标签</span></span><br><span class="line">X = df[[<span class="string">&#x27;X1&#x27;</span>, <span class="string">&#x27;X2&#x27;</span>]].values</span><br><span class="line">y = df[<span class="string">&#x27;Y&#x27;</span>].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化KNN模型</span></span><br><span class="line">knn = KNN(k=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">knn.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测新样本</span></span><br><span class="line">X_test = np.array([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果:&quot;</span>, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是一种优化算法，常用于机器学习和深度学习中的模型训练过程中，通过最小化损失函数来寻找模型参数的最优解。</p>
<p>梯度下降的基本思想是沿着损失函数梯度的反方向更新参数，以使损失函数的值逐渐减小。梯度下降的过程可以描述为沿着损失函数的梯度方向迭代调整参数，直至达到损失函数的局部最小值或全局最小值。</p>
<p>根据梯度下降的变化步长和更新方式，可以将其分为几种不同的类型：</p>
<ol>
<li><p><strong>批量梯度下降（Batch Gradient Descent）</strong>：在每一次迭代中，批量梯度下降都会使用整个训练数据集来计算梯度。这意味着在大型数据集上训练时，计算梯度会很昂贵，但是它能够保证收敛到全局最小值（如果损失函数是凸的）或局部最小值。</p>
</li>
<li><p><strong>随机梯度下降（Stochastic Gradient Descent）</strong>：与批量梯度下降相反，随机梯度下降在每次迭代中仅使用一个训练样本来计算梯度。由于随机梯度下降的更新是基于单个样本，它的计算速度更快，但可能会导致参数更新的方向不稳定，从而引入更多的噪声，使得收敛过程更不稳定。</p>
</li>
<li><p><strong>小批量梯度下降（Mini-batch Gradient Descent）</strong>：小批量梯度下降是批量梯度下降和随机梯度下降的折中方案。在每次迭代中，它使用一个介于整个数据集和单个样本之间的小批量样本来计算梯度。这种方法结合了两者的优点，既可以提高计算效率，又能减少更新方向的随机性。</p>
</li>
</ol>
<p>梯度下降方法的选择取决于许多因素，包括数据集的大小、模型的复杂度、计算资源的限制等。以下是它们的优缺点：</p>
<ul>
<li><p><strong>批量梯度下降</strong>：</p>
<ul>
<li>优点：在参数更新过程中，能够保证收敛到全局最小值（如果损失函数是凸的）或局部最小值。</li>
<li>缺点：计算代价高，特别是在大型数据集上。</li>
</ul>
</li>
<li><p><strong>随机梯度下降</strong>：</p>
<ul>
<li>优点：计算速度快，适用于大型数据集。</li>
<li>缺点：参数更新的方向不稳定，收敛过程更不稳定。</li>
</ul>
</li>
<li><p><strong>小批量梯度下降</strong>：</p>
<ul>
<li>优点：结合了批量梯度下降和随机梯度下降的优点，既能够提高计算效率，又能够减少更新方向的随机性。</li>
<li>缺点：需要调整批量大小，可能会引入一些噪声。</li>
</ul>
</li>
</ul>
<p>在实践中，根据具体问题和资源限制，可以选择合适的梯度下降方法来训练模型。</p>
<table>
<thead>
<tr>
<th>梯度下降类型</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>批量梯度下降 (BGD)</td>
<td>- 收敛到全局最优解（如果损失函数是凸的）</td>
<td>- 计算代价高，不适用于大规模数据集</td>
</tr>
<tr>
<td>随机梯度下降 (SGD)</td>
<td>- 计算速度快，适用于大规模数据集</td>
<td>- 参数更新方向不稳定，收敛过程不稳定</td>
</tr>
<tr>
<td>小批量梯度下降 (MBGD)</td>
<td>- 提高计算效率，减少参数更新方向的随机性</td>
<td>- 需要调整批量大小，可能会引入一些噪声</td>
</tr>
</tbody></table>
<p>Python语言实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成简单的线性数据集</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加偏置项</span></span><br><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义批量梯度下降算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_gradient_descent</span>(<span class="params">X, y, eta=<span class="number">0.01</span>, n_iterations=<span class="number">1000</span></span>):</span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 随机初始化参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">        gradients = <span class="number">2</span>/m * X.T.dot(X.dot(theta) - y)</span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义随机梯度下降算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">X, y, eta=<span class="number">0.01</span>, n_iterations=<span class="number">100</span></span>):</span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 随机初始化参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            random_index = np.random.randint(m)</span><br><span class="line">            xi = X[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            gradients = <span class="number">2</span> * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">            theta = theta - eta * gradients</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义小批量梯度下降算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mini_batch_gradient_descent</span>(<span class="params">X, y, eta=<span class="number">0.01</span>, batch_size=<span class="number">20</span>, n_iterations=<span class="number">100</span></span>):</span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 随机初始化参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">        shuffled_indices = np.random.permutation(m)</span><br><span class="line">        X_shuffled = X[shuffled_indices]</span><br><span class="line">        y_shuffled = y[shuffled_indices]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, m, batch_size):</span><br><span class="line">            xi = X_shuffled[i:i+batch_size]</span><br><span class="line">            yi = y_shuffled[i:i+batch_size]</span><br><span class="line">            gradients = <span class="number">2</span>/<span class="built_in">len</span>(xi) * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">            theta = theta - eta * gradients</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行批量梯度下降</span></span><br><span class="line">theta_batch = batch_gradient_descent(X_b, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行随机梯度下降</span></span><br><span class="line">theta_stochastic = stochastic_gradient_descent(X_b, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行小批量梯度下降</span></span><br><span class="line">theta_mini_batch = mini_batch_gradient_descent(X_b, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原始数据</span></span><br><span class="line">plt.scatter(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制批量梯度下降的拟合线</span></span><br><span class="line">plt.plot(X, X_b.dot(theta_batch), label=<span class="string">&#x27;Batch GD&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制随机梯度下降的拟合线</span></span><br><span class="line">plt.plot(X, X_b.dot(theta_stochastic), label=<span class="string">&#x27;Stochastic GD&#x27;</span>, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制小批量梯度下降的拟合线</span></span><br><span class="line">plt.plot(X, X_b.dot(theta_mini_batch), label=<span class="string">&#x27;Mini-batch GD&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>可视化结果：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/image-20240303231225050.png" alt="image-20240303231225050"></p>
<p>这是一个动态呈现的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.animation <span class="keyword">import</span> FuncAnimation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成简单的线性数据集</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加偏置项</span></span><br><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义批量梯度下降算法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_gradient_descent</span>(<span class="params">X, y, eta=<span class="number">0.01</span>, n_iterations=<span class="number">200</span></span>):</span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 随机初始化参数</span></span><br><span class="line">    theta_history = [theta]  <span class="comment"># 记录参数历史</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(n_iterations):</span><br><span class="line">        gradients = <span class="number">2</span>/m * X.T.dot(X.dot(theta) - y)</span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line">        theta_history.append(theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_history</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动画函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">frame</span>):</span><br><span class="line">    plt.cla()</span><br><span class="line">    plt.scatter(X, y)</span><br><span class="line">    y_predict = X_b.dot(theta_history[frame])</span><br><span class="line">    plt.plot(X, y_predict, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">f&#x27;Iteration: <span class="subst">&#123;frame&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行批量梯度下降</span></span><br><span class="line">theta_history = batch_gradient_descent(X_b, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建动画</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ani = FuncAnimation(fig, update, frames=<span class="built_in">len</span>(theta_history), interval=<span class="number">50</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"># 自然语言处理</a>
              <a href="/tags/NPL/" rel="tag"># NPL</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/02/15/Learning-how-to-learn/" rel="prev" title="Learning how to learn">
                  <i class="fa fa-chevron-left"></i> Learning how to learn
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/" rel="next" title="自然语言处理 01：自然语言处理介绍">
                  自然语言处理 01：自然语言处理介绍 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Jiu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
