<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="自然语言处理介绍">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理 01：自然语言处理介绍">
<meta property="og:url" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/index.html">
<meta property="og:site_name" content="夜久">
<meta property="og:description" content="自然语言处理介绍">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303154755596.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303162518422.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303162702975.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303163722854.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303164126043.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310180649774.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310181038872.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310235135975.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000144503.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000200054.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000339387.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000402355.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310180013521.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310174834750.png">
<meta property="article:published_time" content="2024-03-03T14:33:35.000Z">
<meta property="article:modified_time" content="2024-03-10T13:34:47.757Z">
<meta property="article:author" content="Ye Jiu">
<meta property="article:tag" content="自然语言处理">
<meta property="article:tag" content="NPL">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303154755596.png">


<link rel="canonical" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/","path":"2024/03/03/自然语言处理-01：自然语言处理介绍/","title":"自然语言处理 01：自然语言处理介绍"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>自然语言处理 01：自然语言处理介绍 | 夜久</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">夜久</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/848368468&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/terence-vassallo-415912750" title="Ocelotter" target="_blank" style="color: #cccccc; text-decoration: none;">Ocelotter</a> · <a href="https://soundcloud.com/terence-vassallo-415912750/euphoria" title="Euphoria - 楽園の扉 中日字幕" target="_blank" style="color: #cccccc; text-decoration: none;">Euphoria - 楽園の扉 中日字幕</a></div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535990&auto=1&height=66"></iframe>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E5%91%A8%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.</span> <span class="nav-text">本周任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">2.</span> <span class="nav-text">词向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7%E4%B8%8E%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">工具与算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%85%B7%E4%BD%93%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">4.</span> <span class="nav-text">一些具体的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">6.</span> <span class="nav-text">一些问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A6%82%E5%BF%B5"><span class="nav-number">7.</span> <span class="nav-text">自然语言处理概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%98%AF%E5%9B%B0%E9%9A%BE%E7%9A%84"><span class="nav-number">8.</span> <span class="nav-text">为什么自然语言处理是困难的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E5%AD%A6"><span class="nav-number">9.</span> <span class="nav-text">语言学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP%EF%BC%8CNLU%E5%92%8CNLG"><span class="nav-number">10.</span> <span class="nav-text">NLP，NLU和NLG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8"><span class="nav-number">11.</span> <span class="nav-text">NLP的一些应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">12.</span> <span class="nav-text">语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E8%AF%8D%E8%A1%A8%E7%A4%BA%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="nav-number">13.</span> <span class="nav-text">单词表示和相似度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E8%AF%AD%E8%A1%A8%E7%A4%BA%EF%BC%9APMI"><span class="nav-number">14.</span> <span class="nav-text">词语表示：PMI</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">15.</span> <span class="nav-text">词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98-1"><span class="nav-number">16.</span> <span class="nav-text">一些问题</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Jiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Jiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夜久">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="自然语言处理 01：自然语言处理介绍 | 夜久">
      <meta itemprop="description" content="自然语言处理介绍">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          自然语言处理 01：自然语言处理介绍
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-03 22:33:35" itemprop="dateCreated datePublished" datetime="2024-03-03T22:33:35+08:00">2024-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-10 21:34:47" itemprop="dateModified" datetime="2024-03-10T21:34:47+08:00">2024-03-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">自然语言处理介绍</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="本周任务"><a href="#本周任务" class="headerlink" title="本周任务"></a>本周任务</h2><p>到本周末，将能够：</p>
<ul>
<li>解释基本语言术语及其之间的关系</li>
<li>解释如何创建词向量模型 </li>
<li>讨论词向量的可能应用 </li>
<li>使用预先训练的词向量来查找语义相关的词。</li>
</ul>
<p>下面列出了需要完成的任务。</p>
<ol>
<li><strong>完成<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/assignments/382141">介绍性测验</a></strong></li>
<li>在参加讲座之前，请查看本模块中的在线学习内容并按照指示完成活动。</li>
<li>阅读评估要求。</li>
<li>将的任何问题发布到 Piazza 讨论区。</li>
<li>参加互动讲座。</li>
<li>完成每周测验</li>
</ol>
<p>测试结果：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303154755596.png" alt="image-20240303154755596"></p>
<p>要理解本模块中的概念，需要应用以下主题的知识。</p>
<p><strong>机器学习：</strong></p>
<ul>
<li>分类：分类是一种机器学习任务，旨在将数据实例划分为预定义的类别或标签。分类算法根据已知的特征将数据分配到离散的类别中。</li>
<li>回归（逻辑回归）：回归是一种机器学习任务，旨在预测连续型变量的输出。逻辑回归是一种用于二分类问题的回归算法，通过将输入特征的线性组合传递给一个逻辑函数来预测输出的概率。</li>
<li>聚类：聚类是一种无监督学习任务，旨在将数据集中的数据分成不同的组（或簇），使得同一组内的数据之间的相似度高于不同组之间的相似度。</li>
<li>监督机器学习：监督机器学习是一种机器学习范式，其中模型从带有标签的训练数据中学习，并尝试预测未标记数据的标签。监督学习的目标是根据输入特征预测输出标签或值。</li>
<li>无监督机器学习：无监督机器学习是一种机器学习范式，其中模型从不带标签的数据中学习，并试图发现数据中的结构或模式。无监督学习的目标是发现数据中的隐藏关系或结构，而不是预测标签或值。</li>
<li>成本函数：成本函数是用来衡量机器学习模型预测与实际目标之间差异的函数。在训练过程中，优化算法尝试最小化成本函数，以使模型的预测结果与真实值尽可能接近。</li>
</ul>
<p><strong>机器学习方法论：</strong></p>
<ul>
<li>训练、验证和测试：训练、验证和测试是机器学习模型开发过程中的三个关键阶段。训练阶段用于训练模型参数，验证阶段用于调整模型超参数和评估模型性能，测试阶段用于评估模型的泛化性能。</li>
<li>交叉验证：交叉验证是一种评估模型性能和选择模型超参数的技术。它将数据集分成 k 个子集，每次使用其中的一个子集作为验证集，其余的作为训练集，重复 k 次后取平均值作为最终性能评估指标。</li>
<li>精确率、召回率和 F1 指标：精确率衡量的是模型预测为正类别的样本中实际为正类别的比例，召回率衡量的是实际为正类别的样本中被模型正确预测为正类别的比例，F1 指标是精确率和召回率的调和平均数，综合评估了模型的预测性能。</li>
<li>模型欠拟合和过拟合：模型欠拟合指模型对训练数据和测试数据都表现不佳，未能捕获数据中的趋势和模式；模型过拟合指模型在训练数据上表现很好，但在测试数据上表现较差，过度拟合了训练数据中的噪声或特定特征。</li>
<li>模型评估中的偏差和方差：在模型评估中，偏差是指模型的预测值与真实值之间的差异，反映了模型的拟合能力；方差是指模型在不同数据集上预测结果的变化程度，反映了模型的稳定性。偏差-方差权衡是指在模型选择和优化中需要平衡偏差和方差之间的关系，以获得更好的泛化性能。</li>
</ul>
<p>交叉验证是在机器学习中评估模型性能和选择模型超参数的常用技术。以下是使用交叉验证的情况和原因：</p>
<ol>
<li><strong>评估模型性能</strong>：交叉验证可用于评估模型在未见过的数据上的性能。通过将数据集划分为训练集和验证集，可以多次训练模型并评估其性能，从而得到更稳健和可靠的性能评估。</li>
<li><strong>选择模型超参数</strong>：交叉验证可用于选择模型的超参数。通过尝试不同的超参数组合并使用交叉验证评估每个组合的性能，可以找到最佳的超参数设置，以提高模型的泛化能力。</li>
<li><strong>防止过拟合</strong>：交叉验证可以帮助检测和减轻过拟合问题。通过在不同的数据子集上训练和验证模型，可以更好地了解模型对数据的泛化能力，避免模型过度拟合训练数据。</li>
<li><strong>数据利用率高</strong>：交叉验证充分利用了数据集中的所有样本，尤其在数据量较小的情况下，可以提供更可靠的模型评估结果。</li>
</ol>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>词向量是预先训练的模型，可根据词在向量空间中的距离查找语义相关的词。训练结果是一组向量。每个向量都有 N 个数字，它们是 N 维空间中的坐标。语义相似的向量在这个空间中很接近。</p>
<p>一个演示网站：<a target="_blank" rel="noopener" href="http://vectors.nlpl.eu/explore/embeddings/en/#%E3%80%82">http://vectors.nlpl.eu/explore/embeddings/en/#。</a></p>
<p>另外一个有趣的工具：<a target="_blank" rel="noopener" href="https://ronxin.github.io/wevi/">https://ronxin.github.io/wevi/</a></p>
<p>使用最广泛的 WV 模型，特别注意它们的训练方式：Skip-Gram 和 GloVe。</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>Skip-Gram</th>
<th>GloVe</th>
</tr>
</thead>
<tbody><tr>
<td>训练方式</td>
<td>基于中心词预测上下文词</td>
<td>基于全局词共现矩阵</td>
</tr>
<tr>
<td>模型原理</td>
<td>基于神经网络</td>
<td>基于全局词共现统计信息</td>
</tr>
<tr>
<td>计算效率</td>
<td>训练过程可能较耗时</td>
<td>可能更高效，基于矩阵分解</td>
</tr>
<tr>
<td>数据需求</td>
<td>需要大量数据训练良好的词向量</td>
<td>对全局统计信息依赖较大，但对小数据集也适用</td>
</tr>
<tr>
<td>适用场景</td>
<td>大型语料库，复杂语义表示</td>
<td>语料库规模不大，需求较简单的语义表示</td>
</tr>
</tbody></table>
<p>词向量是词的表示。每个单词都是 N 维向量空间中的一个点。最重要的是，意义相近的单词彼此之间的距离也很近。当我们想要找到相似的单词时，单词向量的这个属性非常有用，例如与未知单词相似的单词，我们将在下一个模块中探索这些单词。</p>
<p>关于词向量：</p>
<blockquote>
<p>词向量是一种将单词映射到高维向量空间中的表示方法。在这个向量空间中，每个单词都被表示为一个向量，而这些向量的维度通常由预先定义的模型决定，比如100维或300维等。</p>
<p>这种表示方法的核心思想是利用单词在上下文中的分布来确定其向量表示，即假设在语言中，具有相似上下文的单词也会有相似的向量表示。</p>
</blockquote>
<p>举例来说，假设我们有一个简单的语料库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;I love natural language processing&quot;</span><br></pre></td></tr></table></figure>

<p>我们可以将每个单词表示为一个向量，例如：</p>
<ul>
<li>“I” -&gt; [0.2, 0.3, 0.5]</li>
<li>“love” -&gt; [0.1, 0.4, 0.8]</li>
<li>“natural” -&gt; [0.7, 0.9, 0.2]</li>
<li>“language” -&gt; [0.6, 0.3, 0.1]</li>
<li>“processing” -&gt; [0.5, 0.7, 0.6]</li>
</ul>
<p>在这个例子中，每个单词都被表示为一个3维向量。这些向量可以捕捉到单词之间的语义和语法关系，比如”love”和”language”可能在向量空间中更接近，而”natural”和”processing”可能也会有一定的相似性。</p>
<p>通过将单词表示为向量，我们可以在向量空间中进行各种操作，比如计算词语之间的相似度、查找与给定词语最相似的词语等。这种表示方法在自然语言处理中被广泛应用，尤其是在词嵌入（Word Embedding）技术中，如Word2Vec、GloVe等。</p>
<p><strong>如何确定单词对应的向量：</strong></p>
<blockquote>
<p>词向量中每个数字的确定通常是通过训练模型来实现的，其中最常见的模型包括Word2Vec、GloVe、FastText等。这些模型使用了大量的文本数据，并利用神经网络或其他统计方法来学习单词的向量表示。</p>
<p>具体地说，这些模型会考虑单词在上下文中的分布情况，通过优化某种损失函数，将单词的向量表示调整到使得在语料库中频繁共现的单词在向量空间中距离更接近的情况。换句话说，如果两个单词在语料库中经常出现在相似的上下文中，那么它们在向量空间中的表示也应该更加接近。</p>
<p>训练过程中，模型会不断调整单词向量中每个数字的值，直到达到最优的表示。这些值可能没有特定的物理含义，但它们能够捕捉到单词之间的语义和语法关系，从而在许多自然语言处理任务中都能取得良好的效果。</p>
</blockquote>
<p>在完成在线学习部分的阅读和观看视频时，请注意下面列出的概念，因为它们对于成功完成在线测验和作业非常重要。</p>
<p>下面嵌入的视频提供了 NLP 和语言学的有用介绍：<a target="_blank" rel="noopener" href="https://youtu.be/MPOVoIB4EGw">https://youtu.be/MPOVoIB4EGw</a></p>
<p><strong>NPL金字塔的层次通常如下所示（自下而上）：</strong></p>
<ol>
<li><strong>语言学基础（Linguistic Foundations）</strong>：这是金字塔的基础层。它涵盖了语言学的基本原理，如词法学（词汇分析）、句法学（语法分析）、语音学（声音学）等。</li>
<li><strong>词法分析（Morphological Analysis）</strong>：这一层包括词法分析，即将词汇分解成词干和词缀等基本单位。</li>
<li><strong>句法分析（Syntactic Analysis）</strong>：句法分析涉及对句子结构的分析，包括词语之间的语法关系和句子的句法结构。</li>
<li><strong>语义分析（Semantic Analysis）</strong>：在这一层，系统试图理解句子的意义，包括词语之间的语义关系以及句子的整体含义。</li>
<li><strong>语用分析（Pragmatic Analysis）</strong>：语用分析涉及到语言使用的背景和语境，以及言语行为的目的和意图。</li>
</ol>
<p>在NPL金字塔的顶部，还可以包括更高级的任务，如对话系统、情感分析、文本生成等。</p>
<h2 id="工具与算法"><a href="#工具与算法" class="headerlink" title="工具与算法"></a>工具与算法</h2><p>自然语言处理（NLP）领域有许多库和工具可供使用，涵盖了各种不同的任务和技术。以下是一些常用的NLP库和工具，包括但不限于：</p>
<ol>
<li><strong>NLTK  Natural Language Toolkit</strong>：NLTK是Python中最常用的NLP库之一，提供了丰富的工具和资源，用于词性标注、句法分析、语义分析等任务。</li>
<li><strong>spaCy</strong>：spaCy是另一个流行的Python NLP库，提供了快速的词法分析和句法分析等功能，支持多种自然语言处理任务。</li>
<li><strong>Gensim</strong>：Gensim是用于文本处理和主题建模的Python库，支持词向量表示、文档相似性计算等功能。</li>
<li><strong>Stanford NLP</strong>：Stanford NLP是斯坦福大学开发的一组NLP工具，包括分词器、词性标注器、句法分析器等，提供了Java和Python的接口。</li>
<li><strong>OpenNLP</strong>：OpenNLP是Apache基金会的一个项目，提供了一系列NLP工具，包括分词器、词性标注器、命名实体识别器等。</li>
<li><strong>CoreNLP</strong>：CoreNLP是斯坦福大学的一个NLP工具包，提供了一系列NLP任务的功能，包括句法分析、语义分析、情感分析等。</li>
<li><strong>Spacy</strong>：Spacy是一个用于自然语言处理任务的Python库，具有高效的句法分析和实体识别功能。</li>
<li><strong>TextBlob</strong>：TextBlob是一个简单易用的Python库，提供了文本处理和情感分析等功能。</li>
<li><strong>Word2Vec</strong>：Word2Vec是Google开发的一个词向量表示工具，用于将词语转换为向量表示，常用于词语相似性计算和文本表示。</li>
<li><strong>FastText</strong>：FastText是Facebook开发的一个词向量表示工具，支持快速训练和使用大规模词向量模型。</li>
</ol>
<p><strong>以下是一些常见的NLP算法：</strong></p>
<ol>
<li><strong>维特比算法（Viterbi Algorithm）</strong>：维特比算法是一种动态规划算法，用于在给定隐马尔可夫模型的情况下，寻找最有可能产生观测序列的隐藏状态序列。这个算法通过在状态空间中动态地计算每个时刻的最佳路径来实现。维特比算法通常用于诸如词性标注、语音识别等序列标注任务中。</li>
<li><strong>BOUND-WELCH算法（Baum-Welch Algorithm）</strong>：BOUND-WELCH算法是一种用于隐马尔可夫模型参数估计的期望最大化（EM）算法的特例。它用于通过观测序列学习隐马尔可夫模型的参数，包括状态转移概率和观测概率。BOUND-WELCH算法通过迭代地调整模型参数，使得观测序列出现的概率最大化。这个算法常用于无监督学习的问题，其中隐藏状态是未知的，但可以通过观测序列进行估计。</li>
<li><strong>词袋模型（Bag of Words，BoW）</strong>：将文本表示为单词的集合，忽略单词的顺序和语法结构，常用于文本分类和情感分析等任务。</li>
<li><strong>TF-IDF（Term Frequency-Inverse Document Frequency）</strong>：用于衡量单词在文档中的重要性，通过词频和逆文档频率来计算单词的权重，常用于文本分类和信息检索等任务。</li>
<li><strong>词嵌入（Word Embedding）</strong>：将单词映射到低维向量空间中，以捕捉单词之间的语义关系，常用的词嵌入模型包括Word2Vec、GloVe、FastText等。</li>
<li><strong>序列标注算法（Sequence Labeling）</strong>：用于给定输入序列中的每个单词标注一个标签，常见的序列标注任务包括词性标注、命名实体识别等。</li>
<li><strong>机器翻译算法（Machine Translation）</strong>：将一种自然语言翻译成另一种自然语言的算法，常用的机器翻译模型包括统计机器翻译（SMT）和神经机器翻译（NMT）等。</li>
<li><strong>情感分析算法（Sentiment Analysis）</strong>：用于分析文本中的情感倾向，常见的情感分析技术包括基于词典的方法、机器学习方法和深度学习方法等。</li>
<li><strong>命名实体识别算法（Named Entity Recognition，NER）</strong>：用于识别文本中具有特定意义的命名实体，如人名、地名、组织机构名等。</li>
<li><strong>文本生成算法（Text Generation）</strong>：用于生成符合语法和语义规则的文本，常见的文本生成技术包括基于规则的方法、统计语言模型和深度学习模型等。</li>
<li><strong>关键词提取算法（Keyword Extraction）</strong>：从文本中自动提取关键词或关键短语，常用于文本摘要、信息检索等应用。</li>
<li><strong>情境理解（Discourse Analysis）</strong>：用于理解文本中句子之间的逻辑关系和语义关联，常见的技术包括共指消解、指代消解等。</li>
</ol>
<p>这些算法和技术在NLP领域的不同任务和应用中起着重要作用，可以根据具体的需求和场景选择合适的算法来解决问题。</p>
<h2 id="一些具体的例子"><a href="#一些具体的例子" class="headerlink" title="一些具体的例子"></a>一些具体的例子</h2><p><strong>一个语言学+深度学习的例子：</strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303162518422.png" alt="image-20240303162518422"></p>
<p><strong>一个依存树的例子：</strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303162702975.png" alt="image-20240303162702975"></p>
<p>更加详细的笔记查看：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/mantch/p/12327735.html">https://www.cnblogs.com/mantch/p/12327735.html</a></p>
<p><strong>另外一个是Constituency trees（构成树）。</strong></p>
<p>Constituency trees（构成树）是自然语言处理中一种常用的句法结构表示方法，用于描述句子的成分结构。构成树将句子分解为不同的成分（或短语），并显示它们之间的层次关系。在构成树中，句子被分解为若干个成分，每个成分可以是单词或者更大的短语。这些成分通过树形结构相互连接，其中树的叶子节点对应于句子中的单词，而内部节点表示短语结构。</p>
<p>例如，考虑以下句子：”The cat sat on the mat.”（猫坐在垫子上。）。</p>
<p>构成树的示例可能如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">        Sentence</span><br><span class="line">           |</span><br><span class="line">     ______|______</span><br><span class="line">    |             |</span><br><span class="line">  NP (The)       VP (sat)</span><br><span class="line">    |             |</span><br><span class="line">   / \           / \</span><br><span class="line">Det  N         V   PP</span><br><span class="line"> |   |         |   / \</span><br><span class="line"> the cat      sat  P   NP</span><br><span class="line">               |    |   |</span><br><span class="line">               on   the mat</span><br></pre></td></tr></table></figure>

<p>在这个示例中，构成树以”Sentence”作为根节点，该句子被分解为名词短语（NP）”The cat” 和动词短语（VP）”sat on the mat”。名词短语”NP”由限定词（Det）”The”和名词（N）”cat”组成，动词短语”VP”由动词（V）”sat”和介词短语（PP）”on the mat”组成。</p>
<p>构成树提供了对句子结构的直观理解，有助于理解句子中不同部分之间的语法关系。它在自然语言处理中被广泛应用，例如句法分析、语言生成等任务中。</p>
<p><strong>情感分析（sentiment analysis）</strong></p>
<p>情感分析（Sentiment Analysis），也称为意见挖掘（Opinion Mining），是一种自然语言处理技术，旨在识别和提取文本中的情感和情绪。它通常用于分析文本的态度、情绪、观点和情感倾向，以了解作者对某个主题或实体的态度是正面的、负面的还是中性的。</p>
<p>情感分析可以应用于各种类型的文本数据，包括社交媒体帖子、产品评论、新闻文章、调查问卷等。它可以帮助企业了解客户对其产品和服务的感受，政府了解公众对政策和事件的反应，以及分析师了解市场情绪和趋势。</p>
<p>情感分析的任务通常包括以下几个方面：</p>
<ol>
<li><strong>情感极性分类</strong>：将文本分类为积极、消极或中性情感。例如，判断一篇产品评论是正面的、负面的还是中性的。</li>
<li><strong>情感强度分析</strong>：确定文本中情感的强度程度。例如，判断一篇评论中的情感是强烈的还是弱化的。</li>
<li><strong>主题情感分析</strong>：针对特定主题或实体分析情感。例如，针对某个产品或品牌进行情感分析，了解公众对其的态度。</li>
<li><strong>情感趋势分析</strong>：分析情感随时间的变化趋势。例如，追踪产品评论的情感趋势，了解产品在市场上的表现如何随着时间推移而变化。</li>
</ol>
<p>一个例子：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303163722854.png" alt="image-20240303163722854"></p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络是受人脑结构启发的计算机模型。神经网络通常简称为神经网络。 </p>
<p>人脑由大约 1000 亿个称为神经元的神经细胞组成，每个神经元都与其他神经元相连。</p>
<p><em>电信号沿着称为树突的</em>通道流入神经元，如果这些电输入的总和超过特定<em>阈值</em>，神经元就会通过称为<em>轴突</em>的通道发出电输出。  </p>
<p>为了在计算机内部构建<em>神经网络，我们只需将这些神经元的集合放在一起，这样一些神经元的输出就会馈送到其他神经元的输入。</em></p>
<p>典型的神经网络具有一个 输入层 （您正在使用的数据中的每个特征有一个输入）、一个 输出层 （每个目标变量有一个输出）以及 中间的一个或多个神经元隐藏层。</p>
<p>每个神经元通过权重连接到其他神经元，权重决定每个神经元对其连接的其他神经元的影响程度。</p>
<p>图示：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240303164126043.png" alt="image-20240303164126043"></p>
<p>在线调参地址：<a target="_blank" rel="noopener" href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.27180&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false">在线调参</a></p>
<p>一些其他的资源：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1mu411x7VD/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1mu411x7VD/?spm_id_from=333.337.search-card.all.click</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16x411V7Qg?p=2&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV16x411V7Qg?p=2&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
</ul>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><p><strong>Explain the difference between semantics and pragmatics and provide examples.</strong></p>
<table>
<thead>
<tr>
<th>概念</th>
<th>语义（semantics ）</th>
<th>语用学（pragmatics）</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>研究词语、短语和句子的字面意义</td>
<td>研究言语在实际交流中的使用情境和效果</td>
</tr>
<tr>
<td>焦点</td>
<td>关注语言单位与所表示概念之间的关系</td>
<td>关注言语行为的意图、社会和文化背景以及上下文对语言理解的影响</td>
</tr>
<tr>
<td>示例</td>
<td>“Cat”的语义是一种四肢动物，通常作为宠物饲养</td>
<td>“你好吗？”可能在实际交流中只是一种礼貌问候</td>
</tr>
<tr>
<td>稳定性</td>
<td>相对固定，通常与词语的定义和语法结构相关</td>
<td>更加灵活，受到言外之意、语境和交际目的等因素的影响</td>
</tr>
<tr>
<td>内部&#x2F;外部</td>
<td>语言的内部结构</td>
<td>语言的外部应用和效果</td>
</tr>
</tbody></table>
<p><strong>Identify all the various meanings of the following sentence: ‘one morning I shot an elephant in my pajamas.’</strong></p>
<p>这句话的多重含义可以理解为：</p>
<ol>
<li>“one morning”：这个短语指的是某个早晨，暗示了一个特定的时间点或者事件发生的背景。</li>
<li>“I shot an elephant”：这个短语表达了一个动作，即“我开枪射击了一头大象”。这句话的字面意思是“我在穿着睡衣的情况下开枪射击了一头大象”。</li>
<li>“in my pajamas”：这个短语描述了动作发生的环境，即“我穿着睡衣”。这个短语可以产生歧义，它可以解释为“我穿着睡衣时射击了大象”或者“我在大象穿着睡衣的情况下射击了它”。</li>
</ol>
<p>因此，这句话的多重含义可能是指在某个早晨，有人穿着睡衣时开枪射击了一头大象。这个句子之所以有趣，是因为它可以根据语境产生多种不同的理解。</p>
<p><strong>From the list of words, identify sub-words and morphemes.</strong></p>
<p>Sub-words是指一个词中可以被分解成的较小的语言单元，这些单元可能不是完整的词，但在某种程度上具有一定的语义或语法功能。</p>
<p>Morphemes是语言中的最小的具有意义的单位，它是词的构成部分，可以单独存在或者与其他morphemes组合形成词语。</p>
<p>一些例子：</p>
<ul>
<li>Sub-words：”un-“ 在 “undo” 中是一个sub-word，表示否定或相反的含义。”pre-“ 在 “preview” 中是一个sub-word，表示在前面或提前的含义。</li>
<li>Morphemes：”-ed” 在 “walked” 中是一个morpheme，表示过去时。”-s” 在 “cats” 中是一个morpheme，表示复数形式。</li>
</ul>
<p>**From the sentence explain its meaning in terms of semantics and pragmatics. **</p>
<p>这句话的语义是关于事件的描述，主要涉及到以下几个方面：</p>
<ol>
<li>“one morning” 表示一个具体的时间，即某个早晨。</li>
<li>“I shot an elephant” 描述了一个动作，即说话者开枪射击了一头大象。</li>
<li>“in my pajamas” 描述了动作发生的环境，即说话者穿着睡衣。</li>
</ol>
<p>在语义学的角度上，这句话是一个事件的简单描述，包括时间、动作和环境。</p>
<p>从语用学的角度来看，这句话可能有一些歧义，具体取决于上下文和说话者的意图：</p>
<ol>
<li>“in my pajamas” 这个短语可能引起歧义，因为它可以解释为说话者穿着睡衣时射击大象，也可以解释为说话者在大象穿着睡衣的情况下射击它。这种歧义可能引发听者的笑声，因为它打破了通常的语境预期。</li>
<li>语境的缺失可能导致对事件的理解产生不同的解释。例如，为什么说话者穿着睡衣？是偶然的还是有特殊原因？这些问题可能需要从语境中获取信息，而不仅仅依赖于字面意义。</li>
</ol>
<p>因此，语用学强调了言语行为的意图、言外之意和交际背景，这些因素在理解这句话时可能起到关键作用。</p>
<p><strong>From the sentence, identify parts of speech (POS)</strong>. </p>
<p>这句话中的各个词的词性（Parts of Speech, POS）如下：</p>
<ul>
<li>“one” - 数词（numeral）</li>
<li>“morning” - 名词（noun）</li>
<li>“I” - 代词（pronoun）</li>
<li>“shot” - 动词（verb）</li>
<li>“an” - 冠词（article）</li>
<li>“elephant” - 名词（noun）</li>
<li>“in” - 介词（preposition）</li>
<li>“my” - 代词（pronoun）</li>
<li>“pajamas” - 名词（noun）</li>
</ul>
<p>总体而言，这个句子包含了数词、名词、代词、动词和介词等不同的词性。</p>
<p>**From a list of business problems, identify which ones can be solved using Natural Language Processing. **</p>
<ul>
<li><p>客户支持和服务：通过NLP技术，可以建立智能客服系统来处理客户的问题和需求，例如自动回答常见问题、提供建议、处理投诉等。</p>
</li>
<li><p>市场调研和消费者洞察：NLP可以帮助分析社交媒体上的文本数据，了解消费者对产品和服务的看法，发现趋势和关键主题。</p>
</li>
<li><p>舆情分析和品牌管理：通过分析新闻报道、社交媒体内容和客户反馈等文本数据，可以了解公众对公司和产品的看法，及时发现并应对负面舆情。</p>
</li>
<li><p>市场营销和广告优化：NLP技术可以分析消费者的语言和行为模式，帮助企业定制个性化的营销策略和广告内容，提高营销效果。</p>
</li>
<li><p>文档管理和信息提取：利用NLP技术，可以自动处理和归档大量的文档、合同和报告，实现信息的快速检索和提取。</p>
</li>
<li><p>人才招聘和人力资源管理：NLP可以帮助企业自动筛选简历、分析候选人的语言和技能，提高招聘效率和质量。</p>
</li>
<li><p>知识管理和智能助手：企业可以利用NLP技术构建智能助手，帮助员工快速查找信息、解决问题，提高工作效率。</p>
</li>
<li><p>金融和投资分析：NLP可以分析新闻报道、财报和社交媒体上的信息，帮助投资者做出更准确的决策，发现投资机会和风险。</p>
</li>
</ul>
<p><strong>Evaluate whether the Machine Learning task can be supervised, unsupervised, or a semi-supervised learning problem.</strong></p>
<p>评估机器学习任务是监督学习、无监督学习还是半监督学习问题，需要考虑可用数据的性质以及任务的具体学习目标。</p>
<ol>
<li><strong>监督学习</strong>：<ul>
<li>在监督学习中，算法从带有标签的数据中学习，其中输入数据与相应的目标标签配对。</li>
<li>目标是学习从输入特征到目标标签的映射。</li>
<li>示例包括分类和回归任务。</li>
</ul>
</li>
<li><strong>无监督学习</strong>：<ul>
<li>无监督学习涉及从未标记的数据中学习模式或结构，而没有明确的目标标签。</li>
<li>算法发现数据内部的隐藏模式、群组或聚类。</li>
<li>示例包括聚类、降维和异常检测。</li>
</ul>
</li>
<li><strong>半监督学习</strong>：<ul>
<li>半监督学习介于监督学习和无监督学习之间，算法从标记和未标记数据的组合中学习。</li>
<li>通常，与总数据量相比，标记数据的数量较少。</li>
<li>算法利用额外的未标记数据来提高学习性能或泛化能力。</li>
</ul>
</li>
</ol>
<p>要评估机器学习任务属于哪个类别，我们需要检查可用的数据：</p>
<ul>
<li>如果数据包含输入特征以及相应的目标标签，那么这是一个监督学习问题。</li>
<li>如果数据没有目标标签，且目标是发现数据内部的模式或结构，那么这是一个无监督学习问题。</li>
<li>如果数据包含标记和未标记实例的组合，那么这是一个半监督学习问题。</li>
</ul>
<p>根据数据的特征和学习目标，我们可以确定机器学习任务是监督学习、无监督学习还是半监督学习。</p>
<h2 id="自然语言处理概念"><a href="#自然语言处理概念" class="headerlink" title="自然语言处理概念"></a>自然语言处理概念</h2><p>什么是自然语言处理：</p>
<blockquote>
<p>自然语言处理是一系列计算技术，用于分析和表示自然发生的文本，可以在一个或多个语言分析层面上进行，目的是实现类似人类语言处理的能力，以应用于各种特定任务或应用。 —— 来自 Liddy</p>
</blockquote>
<p>什么是语言：</p>
<blockquote>
<p>语言是人类用来沟通、表达思想和交流信息的工具。它是由一定的符号、词汇和规则组成的系统，可以通过口头、书写、手势等形式进行表达和传递。语言是人类文化和社会生活的核心组成部分，也是人类认知和思维的重要载体。</p>
<p>语言可以包括口头语言、书面语言以及其他形式的表达方式，如手语、图像符号等。每种语言都有自己的语法、词汇和语音体系，反映了所属社会文化的特点和历史发展的轨迹。</p>
<p>除了人类使用的自然语言，还有人工语言，如编程语言、数学语言等，它们是设计用来进行特定目的的交流和表达的工具。</p>
<p>总的来说，语言是人类沟通和交流的基础，是人类文化、社会和认知活动的重要载体和工具。</p>
</blockquote>
<p>人工语言（Artificial Language）是由人类创造出来的语言，其目的通常是为了特定的目标或应用。人工语言可以是计算机编程语言、数学符号系统、逻辑语言、国际辅助语言等。</p>
<p>以下是人工语言的一些常见类型：</p>
<ol>
<li><p><strong>编程语言：</strong> 用于编写计算机程序的语言，如Python、Java、C++等。这些语言具有严格的语法和语义规则，用于指导计算机执行特定的操作和任务。</p>
</li>
<li><p><strong>数学符号系统：</strong> 用于数学表达和推理的语言，如数学符号、公式、推理规则等。数学语言用于描述和解决数学问题，并在科学研究和工程领域中广泛应用。</p>
</li>
<li><p><strong>逻辑语言：</strong> 用于逻辑推理和论证的语言，如一阶逻辑、模态逻辑等。逻辑语言用于描述命题、论证结构和推理规则，被广泛应用于哲学、数学、计算机科学等领域。</p>
</li>
<li><p><strong>国际辅助语言：</strong> 也称为国际语言或人工国际语言，旨在作为跨国交流和交流的共同语言。例如，世界语（Esperanto）、国际音标（International Phonetic Alphabet）等。</p>
</li>
</ol>
<p>人工语言的特点是它们可以被设计和改变，以满足特定的需求和目标。这些语言通常具有较为简单的结构和规则，以便于学习和使用。通过人工语言，人类可以实现更有效的交流、思考和合作。</p>
<p>自然语言处理：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310180649774.png" alt="image-20240310180649774"></p>
<h2 id="为什么自然语言处理是困难的"><a href="#为什么自然语言处理是困难的" class="headerlink" title="为什么自然语言处理是困难的"></a>为什么自然语言处理是困难的</h2><p>自然语言之所以具有困难性主要是因为其高度的歧义性和复杂性。以下是几个导致自然语言困难的原因：</p>
<ol>
<li><p><strong>歧义性：</strong> 自然语言中存在许多语义上的歧义，即一个词语或句子可能有多个解释或意思。这种歧义可以是词义歧义（一个词有多个意思）、结构歧义（句子结构模糊不清）或语用歧义（句子在特定语境下有多种解释）等。</p>
</li>
<li><p><strong>上下文依赖性：</strong> 自然语言的意义通常依赖于其所处的上下文环境。相同的词语或句子在不同的上下文中可能具有不同的含义。因此，理解自然语言需要考虑到丰富的语境信息。</p>
</li>
<li><p><strong>语言变体和多样性：</strong> 自然语言在不同的地区、文化和社会群体中存在着许多变体和多样性。词汇、语法结构、语音特点等在不同的语言和方言中可能有很大的差异，这增加了理解自然语言的复杂性。</p>
</li>
<li><p><strong>非线性和模糊性：</strong> 自然语言通常是非线性的和模糊的，意味着其表达方式可能不是严格的逻辑结构，而是更加灵活和模糊的。这种模糊性使得理解自然语言变得更加困难。</p>
</li>
<li><p><strong>语言演变和创造性：</strong> 自然语言是活跃的、动态的系统，不断地发展和演变。新词汇的出现、语法结构的变化等使得自然语言的理解和处理需要不断地适应和更新。</p>
</li>
</ol>
<p>综上所述，自然语言之所以困难是因为它具有复杂的歧义性、上下文依赖性、多样性、模糊性和创造性等特点，这使得对自然语言进行准确理解和处理成为一项复杂而具有挑战性的任务。</p>
<p>以下是一些具体的例子，展示自然语言中的歧义性和上下文依赖性：</p>
<ol>
<li><p><strong>歧义性的例子：</strong></p>
<ul>
<li>“银行”这个词在不同的上下文中可以有不同的含义，比如可以指银行机构，也可以指河岸。</li>
<li>“我在公园看到了一个大熊”，这句话中的”大熊”可能是指一个巨大的熊，也可能是指一个年龄较大的熊。</li>
</ul>
</li>
<li><p><strong>上下文依赖性的例子：</strong></p>
<ul>
<li>“他走了进来，把门打开了”。在这个句子中，”打开了”的动作是由谁完成的，取决于上下文中的指代关系。</li>
<li>“我想吃鸡蛋”。在不同的上下文中，这句话可能是表达一个人想要吃鸡蛋，也可能是指某种食物的名称。</li>
</ul>
</li>
<li><p><strong>模糊性的例子：</strong></p>
<ul>
<li>“这个房间很大”。对于一个人来说，”很大”的定义可能是相对的，取决于他们之前所见过的房间大小。</li>
</ul>
</li>
<li><p><strong>语言演变和创造性的例子：</strong></p>
<ul>
<li>“自行车”这个词是近代才出现的，它是由”自己”和”车”两个词组合而成的，指代一种新型交通工具。</li>
</ul>
</li>
</ol>
<p>这些例子展示了自然语言中的复杂性和多样性，以及理解和处理自然语言所需考虑的歧义性、上下文依赖性、模糊性和创造性等因素。</p>
<h2 id="语言学"><a href="#语言学" class="headerlink" title="语言学"></a>语言学</h2><p>在语言学中，语言组成部分（Linguistic components）通常可以按照以下方式分类：</p>
<ol>
<li><p><strong>音素（Phoneme）：</strong> 语音学上的最小单位，是一个语言中能够区分词义的最小音位单位。</p>
</li>
<li><p><strong>形态素（Morpheme）：</strong> 语言学上的最小意义单位，是语言中能够独立存在或附着于词根上的最小的、具有意义的语言单位。</p>
</li>
<li><p><strong>词素（Word）：</strong> 指语言中的基本单词，是能够单独存在并有意义的最小语言单位。</p>
</li>
<li><p><strong>短语（Phrase）：</strong> 由一个或多个词组成的具有一定语法结构和意义的短语性单位。</p>
</li>
<li><p><strong>句子（Sentence）：</strong> 是由词语按照一定的语法规则组合而成的完整语言表达，具有完整的意义。</p>
</li>
<li><p><strong>段落（Paragraph）：</strong> 由一个或多个句子组成，是表达一个完整思想或主题的连续语言单位。</p>
</li>
<li><p><strong>文本（Text）：</strong> 由一个或多个段落组成的连续语言材料，具有一定的上下文结构和语言特点。</p>
</li>
<li><p><strong>语料库（Corpus）：</strong> 是收集和整理了大量文本或语言数据的库，用于语言学研究、自然语言处理等领域。</p>
</li>
</ol>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310181038872.png" alt="image-20240310181038872"></p>
<p>这些语言组成部分按照不同的层次和结构组织起来，构成了语言的基本单元和体系，用于描述和理解语言的结构、功能和使用规律。</p>
<p>以下是以上术语的中文解释和示例：</p>
<ul>
<li><strong>音素（Phoneme）：</strong> 语言中能够区分一个词与另一个词的最小音位单位。例如，单词’猫’有三个音素：&#x2F;m&#x2F; &#x2F;ā&#x2F; &#x2F;o&#x2F;。</li>
<li><strong>形态素（Morpheme）：</strong> 语言中最小的有意义的词汇单位。例如，单词”不可打破”中的”不-“，”打破”和”-able”都是形态素。</li>
<li><strong>子词（Sub-word）：</strong> 单词的部分，类似于形态素。例如，单词”猫”的”猫”就是一个子词。</li>
<li><strong>短语（Phrase）：</strong> 由一个或多个单词组成的语法单位。例如，形容词短语”非常开心”。</li>
<li><strong>语料库（Corpus）：</strong> 由大量结构化文本组成的语言资源，例如一组维基百科文章。</li>
<li><strong>语义学（Semantics）：</strong> 文字的字面意义。例如，”下午5点”在字面上表示一个时间点。</li>
<li><strong>语用学（Pragmatics）：</strong> 上下文中的含义。例如，”下午5点”可能表示该是回家的时间。</li>
</ul>
<p>这些术语和例子有助于理解语言学中的基本概念和术语，从而更好地理解和分析语言及其用法。</p>
<p>判断一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">“This box is unbreakable”</span><br><span class="line">“s”:</span><br><span class="line">phoneme, morpheme, phrase?</span><br><span class="line">“This box”:</span><br><span class="line">phoneme, morpheme, phrase?</span><br><span class="line">“un-”:</span><br><span class="line">phoneme, morpheme, phrase?</span><br></pre></td></tr></table></figure>

<p>首先是”s”：</p>
<ul>
<li>这是一个音素 &#x2F;s&#x2F;</li>
<li>这不是一个语素：因为没有表示复数或所有格的意义</li>
<li>这也不是一个短语</li>
</ul>
<p>然后是“this box”：</p>
<ul>
<li>音素：包含&#x2F;ðɪs bɒks&#x2F; 的音素</li>
<li>语速：“this”和“box”都是语速，语言中最小的有意义的词汇单位，this表示指向，而box表示一个容器</li>
<li>“这个盒子”形成一个名词短语，作为句子的主语。</li>
</ul>
<p>最后是“un-”：</p>
<ul>
<li>音素：&#x2F;ʌn&#x2F;</li>
<li>语素：“un-”是一个语素，是一个前缀，通常表示否定或逆转词根的含义。</li>
<li>短语：单独来看，“un-”并不构成短语。但它和“打破”的结合形成一个词语“不可打破”。</li>
</ul>
<p>So, to summarize:</p>
<ul>
<li>“s” is a phoneme.</li>
<li>“This box” contains both phonemes and morphemes and forms a phrase.</li>
<li>“un-“ is a morpheme but doesn’t form a phrase on its own. It combines with “breakable” to form a word.</li>
</ul>
<p>以下是将标记（Tags）及其描述整理成表格的示例：</p>
<table>
<thead>
<tr>
<th>Tag</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>CC</td>
<td>coordinating conjunction</td>
<td>and</td>
</tr>
<tr>
<td>CD</td>
<td>cardinal number</td>
<td>1, third</td>
</tr>
<tr>
<td>DT</td>
<td>determiner</td>
<td>the</td>
</tr>
<tr>
<td>IN</td>
<td>preposition, subordinating conjunction</td>
<td>in, of, like</td>
</tr>
<tr>
<td>JJ</td>
<td>adjective</td>
<td>green</td>
</tr>
<tr>
<td>NN</td>
<td>noun, singular or mass</td>
<td>table</td>
</tr>
<tr>
<td>NNS</td>
<td>noun plural</td>
<td>tables</td>
</tr>
<tr>
<td>NP</td>
<td>proper noun, singular</td>
<td>John</td>
</tr>
<tr>
<td>NPS</td>
<td>proper noun, plural</td>
<td>Vikings</td>
</tr>
<tr>
<td>PP</td>
<td>personal pronoun</td>
<td>I, he, it</td>
</tr>
<tr>
<td>VB</td>
<td>verb be, base form</td>
<td>be</td>
</tr>
</tbody></table>
<p>这个表格清晰地展示了每个标记的含义以及相应的示例。这种格式有助于更直观地了解不同标记的作用和用法。</p>
<h2 id="NLP，NLU和NLG"><a href="#NLP，NLU和NLG" class="headerlink" title="NLP，NLU和NLG"></a>NLP，NLU和NLG</h2><p>NLP，NLU和NLG是自然语言处理（Natural Language Processing）领域中的三个重要概念，它们分别代表了不同的任务和功能：</p>
<ol>
<li><p><strong>NLP（Natural Language Processing）：</strong> 自然语言处理是指利用计算机技术处理和理解人类语言的过程。它涵盖了对文本数据进行分析、处理、理解和生成的一系列技术和方法。NLP的任务包括词性标注、句法分析、命名实体识别、情感分析、文本分类、文本生成等。</p>
</li>
<li><p><strong>NLU（Natural Language Understanding）：</strong> 自然语言理解是NLP的一个子领域，专注于让计算机理解和解释人类语言的意义和语境。NLU的任务包括语义理解、指代消解、逻辑推理、语义角色标注等。NLU的目标是使计算机能够对人类语言进行深层次的理解，以便更好地处理和应用自然语言数据。</p>
</li>
<li><p><strong>NLG（Natural Language Generation）：</strong> 自然语言生成是NLP的另一个子领域，它涉及使用计算机生成自然语言文本的过程。NLG的任务包括根据给定的数据、模板或规则生成文本、摘要生成、问答系统中的回答生成等。NLG的目标是让计算机能够像人类一样生成自然流畅的语言文本。</p>
</li>
</ol>
<p>这三个概念在自然语言处理中密切相关，彼此之间有着密切的联系和互动：</p>
<ul>
<li><p><strong>NLU和NLP的关系：</strong> NLU是NLP的一个重要组成部分，它涉及对文本数据的深层次理解和解释，是NLP中一个重要的子任务。</p>
</li>
<li><p><strong>NLG和NLP的关系：</strong> NLG是NLP的另一个重要组成部分，它涉及根据给定的数据或语境生成自然语言文本。在NLP中，NLG用于生成回答、摘要、自动生成文本等任务。</p>
</li>
</ul>
<p>综上所述，NLP、NLU和NLG是自然语言处理领域中的重要概念，它们分别代表了不同的任务和功能，但彼此之间又存在着紧密的关联和互动。通过这些技术和方法，计算机可以更好地处理和理解人类语言，实现更多样化、智能化的应用。</p>
<h2 id="NLP的一些应用"><a href="#NLP的一些应用" class="headerlink" title="NLP的一些应用"></a>NLP的一些应用</h2><p>以下是关于NLP在每个领域的简要讨论：</p>
<ol>
<li><p><strong>Text Classification（文本分类）：</strong> NLP在文本分类领域广泛应用，例如垃圾邮件过滤、新闻分类、情感分析等。通过机器学习和深度学习技术，可以对文本进行自动分类，并应用于广告定向、推荐系统等。</p>
</li>
<li><p><strong>Sentiment Analysis（情感分析）：</strong> 情感分析是NLP的重要应用领域之一，用于识别和分析文本中的情感倾向。它可以应用于社交媒体监控、产品评论分析、舆情监测等方面，帮助企业了解用户对产品和服务的态度和情感倾向。</p>
</li>
<li><p><strong>Information Extraction（信息抽取）：</strong> 信息抽取是从文本中提取结构化信息的过程，包括实体识别、关系抽取、事件抽取等。NLP技术可以用于从大量文本中自动提取关键信息，用于知识图谱构建、信息检索等任务。</p>
</li>
<li><p><strong>Information Retrieval（信息检索）：</strong> 信息检索是通过搜索引擎等工具从文本数据中检索相关信息的过程。NLP技术可以应用于改进搜索引擎的算法和模型，提高搜索结果的准确性和相关性。</p>
</li>
<li><p><strong>Word Prediction（词语预测）：</strong> NLP技术可以应用于自然语言输入法中，预测用户输入的词语或短语，提高输入效率和准确性。词语预测在手机键盘、智能助理等应用中得到了广泛的应用。</p>
</li>
<li><p><strong>Question Answering（问答系统）：</strong> 问答系统利用NLP技术，使计算机能够理解用户提出的问题，并从大量文本中提取相关信息并生成答案。问答系统在搜索引擎、智能助理等领域有重要应用。</p>
</li>
<li><p><strong>Machine Translation（机器翻译）：</strong> 机器翻译是NLP的重要应用领域之一，利用计算机自动将一种语言的文本翻译成另一种语言。NLP技术在机器翻译领域取得了重大进展，例如神经机器翻译模型的出现使得翻译质量得到了显著提升。</p>
</li>
<li><p><strong>Spoken Dialog System（口语对话系统）：</strong> 口语对话系统是基于语音和自然语言处理技术的人机交互系统，可以实现自然、流畅的对话。NLP技术在口语对话系统中扮演着关键角色，用于语音识别、自然语言理解、对话管理等方面。</p>
</li>
<li><p><strong>Summarization（文本摘要）：</strong> 文本摘要是将文本内容压缩为简洁、精炼的摘要的过程。NLP技术可以应用于自动提取文本中的关键信息和主题，并生成概括性的摘要，用于新闻摘要、文档摘要等应用场景。</p>
</li>
</ol>
<p>总体来说，NLP在以上领域的应用不断发展和拓展，为各行各业带来了许多新的应用和技术创新，为提高人们的工作效率、信息获取能力和用户体验提供了重要支持。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型（Language Model）是一种用于描述自然语言序列的概率模型。它的主要任务是根据给定的前文序列，预测下一个单词出现的概率分布。</p>
<p>具体而言，语言模型试图估计在一个给定的自然语言文本序列中，下一个单词是什么的概率。通常情况下，语言模型会根据已经观察到的历史文本数据，学习单词之间的概率分布和语言规律，然后利用这些信息来预测下一个单词。</p>
<p>语言模型的输入是一个单词序列 $x(1), x(2), x(3), \ldots, x(t)$，其中 $x(t)$ 是当前位置的单词，而输出则是给定这个输入序列后，下一个单词 $x(t+1)$ 的概率分布：</p>
<p>$P(x(t+1) | x(t), x(t-1), \ldots, x(1))$</p>
<p>这里的 $x(t+1)$ 可以是词汇表 $v$ 中的任何一个单词，$v &#x3D; {w_1, w_2, \ldots, w_{|v|}}$。语言模型的目标是通过训练，学习如何最好地估计这个条件概率分布，使得给定的上下文序列后，能够准确地预测下一个单词的可能性。</p>
<p>语言模型在自然语言处理的许多任务中都发挥着重要作用，例如语音识别、机器翻译、自动摘要、拼写检查等。它们不仅能够帮助理解和生成自然语言文本，还可以作为许多自然语言处理任务的基础组件之一。</p>
<h2 id="单词表示和相似度"><a href="#单词表示和相似度" class="headerlink" title="单词表示和相似度"></a>单词表示和相似度</h2><p><strong>单词表示 Word Representation : TF-IDF</strong></p>
<p>TF是Term Frequency的缩写，中文为词频。TF用来衡量一个词在文档中出现的频率，如果一个词在文档中出现的越频繁，那么这个词的TF值也就越高。TF可以通过简单的计算某个词在文旦中出现的次数得到，或者采用归一化的形式，比如说计算某个词在文档中出现的次数除以文档的总词数。</p>
<p>以下是TF（词频，Term Frequency）的计算公式的 LaTeX 格式：</p>
<p>$\text{TFT}(t, d) &#x3D; \frac{\text{count}(t, d)}{\text{count}(d)}$</p>
<p>其中：</p>
<ul>
<li>$\text{TFT}(t, d)$ 是词汇 t 在文档 d中的词频；</li>
<li>$\text{count}(t, d)$ 是词汇 t 在文档d中出现的次数；</li>
<li>$\text{count}(d)$ 是文档 d中所有词汇的总数。</li>
</ul>
<p>这个公式用于计算一个词汇在给定文档中的相对频率，即该词汇在文档中出现的次数除以文档中所有词汇的总数。</p>
<p>假设我们有一个文档：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;这个文档是一个简单的例子，用于演示如何计算词频。&quot;</span><br></pre></td></tr></table></figure>

<p>我们要计算单词 “一个” 在这个文档中的词频。首先，我们需要将文档拆分成单词，并对单词进行标准化处理（例如，转换成小写）。</p>
<p>拆分后的单词序列为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;这个&quot;, &quot;文档&quot;, &quot;是&quot;, &quot;一个&quot;, &quot;简单&quot;, &quot;的&quot;, &quot;例子&quot;, &quot;用于&quot;, &quot;演示&quot;, &quot;如何&quot;, &quot;计算&quot;, &quot;词频&quot;]</span><br></pre></td></tr></table></figure>

<p>接着，我们统计单词 “一个” 在文档中出现的次数。在这个例子中，单词 “一个” 出现了 1 次。所以，单词 “一个” 在这个文档中的词频（TF）为 1。</p>
<p>TF（词频，Term Frequency）的值可以高于1。举个例子，如果在一个文档中单词 “apple” 出现了3次，那么 “apple” 这个词的 TF 值就是3。但需要注意的是，在某些情况下，TF 值可能会进行归一化处理，即除以文档中的总词数，以避免文档长度的差异导致的偏差。这种情况下，TF 的值会被限制在0到1之间。</p>
<p><strong>IDF则是Inverse Document Frequency的缩写，中文为逆文档频率。</strong>IDF用来衡量一个词的普遍重要性，如果一个词在很多文档中出现过，那么这个词语的IDF就会非常的低，因为这个词语对于区分不同文档的准确性就越低，这个词语的重要性也就越低。IDF的计算方式是对整个文档集合中某个词出现的文档频率取倒数，然后取对数。</p>
<p>以下是逆文档频率（IDF）的计算公式的 LaTeX 形式：</p>
<p>$\text{idft} &#x3D; \log\left(\frac{N+1}{dft+1}\right)$</p>
<p>其中：</p>
<ul>
<li>$\text{idft}$ 是词汇的逆文档频率；</li>
<li>$N$ 是文档集合中的文档总数；</li>
<li>$dft$ 是包含该词汇的文档数目。</li>
</ul>
<p>这个公式中的 “+1” 是为了进行平滑处理，以避免因为某个词汇在整个文档集合中没有出现或者只在少数几个文档中出现而导致的除数为零或者结果过于极端的情况。</p>
<p>假设我们有一个包含5个文档的文档集合，其中包含一些特定的词汇。我们想计算词汇 “apple” 的逆文档频率（IDF）。</p>
<p>假设这个词汇 “apple” 出现在文档集合中的情况如下：</p>
<ul>
<li>文档1: “I like apple and banana.”</li>
<li>文档2: “Apple is my favorite fruit.”</li>
<li>文档3: “I enjoy eating apples.”</li>
<li>文档4: “Oranges and apples are both fruits.”</li>
<li>文档5: “An apple a day keeps the doctor away.”</li>
</ul>
<p>在自然语言处理和文本处理中，词汇一般是区分大小写的，因此 “apple” 和 “Apple” 被视为两个不同的词汇。在构建语言模型、计算 TF-IDF 等任务中，系统会将它们视为不同的词汇进行处理，而不会将它们视为同一个词汇。</p>
<p>我们计算包含词汇 “apple” 的文档数目。在这个例子中，词汇 “apple” 出现在了4个文档中。然后，我们计算逆文档频率（IDF）。IDF 的计算公式是对于整个文档集合中包含某个词汇的文档数目取倒数，然后取对数。</p>
<p>在这个例子中，我们有5个文档，其中包含词汇 “apple” 的文档数目为4。因此，IDF 可以计算为：</p>
<p>$\text{IDF}(\text{“apple”}) &#x3D; \log \left( \frac{N}{\text{df}(\text{“apple”})} \right)$<br>$&#x3D; \log \left( \frac{5}{4} \right) \approx 0.223 $</p>
<p>这里的 N 是文档集合的总文档数目，而 $\text{df}(\text{“apple”})$ 是包含词汇 “apple” 的文档数目。</p>
<p>因此，词汇 “apple” 的逆文档频率（IDF）约为0.223。</p>
<p>逆文档频率（IDF）是用来衡量一个词汇的普遍重要性的指标，它是基于整个文档集合的。如果一个文档中出现了多个词汇 “apple”，我们仍然只将这个文档视为包含了词汇 “apple”，而不会多次计数。</p>
<p>举个例子，如果一个文档中出现了两次 “apple”，我们仍然将这个文档视为包含了词汇 “apple”，并将其计入词汇 “apple” 的文档频率（DF）中。</p>
<p>在计算逆文档频率（IDF）时，我们不会将同一文档中词汇的出现次数计算多次。因此，不论一个文档中出现了多少次 “apple”，我们只将这个文档计入词汇 “apple” 的文档频率（DF）中，以避免重复计数。</p>
<p>IDF 的计算是基于整个文档集合的，它是通过总文档数目除以包含词汇的文档数目的对数来得到的，与词汇在单个文档中的出现次数无关。</p>
<p><strong>TF-IDF（Term Frequency-Inverse Document Frequency）是一种常用于信息检索和文本挖掘中的统计方法，用于评估一个词对于一个文档集合或语料库中某个文档的重要程度。</strong></p>
<p>TF-IDF 综合了两个指标：词频（TF）和逆文档频率（IDF）。</p>
<ul>
<li><p>TF（词频，Term Frequency） 衡量一个词在文档中出现的频率。如果一个词在文档中出现得越频繁，那么它的重要性也就越高。</p>
</li>
<li><p>IDF（逆文档频率，Inverse Document Frequency） 衡量一个词的普遍重要性。如果一个词在很多文档中都出现过，那么它的 IDF 越低，表示它对于区分不同文档的能力越低，也就是说它的重要性越低。</p>
</li>
</ul>
<p>TF-IDF 是通过将词频和逆文档频率结合起来，来计算一个词对于某个文档的重要性。具体计算公式为：</p>
<p>$\text{TF-IDF}(t, d, D) &#x3D; \text{TF}(t, d) \times \text{IDF}(t, D)$</p>
<p>其中，</p>
<ul>
<li>$t $表示某个词（term）；</li>
<li>$d$ 表示某个文档（document）；</li>
<li>$D$ 表示整个文档集合（document collection）。</li>
</ul>
<p>TF-IDF 的值越高，表示词对于文档的重要性越高。TF-IDF 通常被用于信息检索、文本分类、文本相似度计算等任务中，帮助识别关键词并衡量它们的重要性。</p>
<h2 id="词语表示：PMI"><a href="#词语表示：PMI" class="headerlink" title="词语表示：PMI"></a>词语表示：PMI</h2><p>PMI 是 Pointwise Mutual Information 的缩写，意为逐点互信息。它是一种用于衡量两个事件之间相关性的统计指标，常用于自然语言处理中的词语表示和语义分析。</p>
<p>PMI 衡量的是两个事件之间的关联程度，如果两个事件经常一起出现，那么它们的 PMI 值将会较高，反之则较低。在自然语言处理中，通常用于衡量两个词语之间的相关性。</p>
<p><strong>PMI 的计算公式如下：</strong></p>
<p>$\text{PMI}(x, y) &#x3D; \log_2 \left( \frac{P(x, y)}{P(x)P(y)} \right)$</p>
<p>其中：</p>
<ul>
<li>$P(x, y)$ 是事件 $x$ 和事件 $y$ 同时发生的概率；</li>
<li>$P(x)$ 和 $P(y)$ 分别是事件 $x$和事件 $y$ 单独发生的概率。</li>
</ul>
<p>PMI 的值可以为正数、负数或零。如果两个事件的共同发生概率远大于它们单独发生的概率，那么 PMI 将为正数，表示它们之间存在正相关性；如果两个事件的共同发生概率接近或等于它们单独发生的概率，那么 PMI 将为零，表示它们之间不存在关联；如果两个事件的共同发生概率远小于它们单独发生的概率，那么 PMI 将为负数，表示它们之间存在负相关性。</p>
<p>在自然语言处理中，PMI 常用于词语共现矩阵的构建和文本语料的语义分析，以帮助理解词语之间的关联性和语义相关性。</p>
<p>假设我们有一个文本语料库，其中包含两个词语的共现情况如下：</p>
<ul>
<li>单词 “dog” 出现了 100 次；</li>
<li>单词 “bone” 出现了 50 次；</li>
<li>单词 “dog” 和 “bone” 同时出现了 20 次。</li>
</ul>
<p>我们可以使用 PMI 来衡量单词 “dog” 和 “bone” 之间的关联程度。</p>
<p>首先，我们计算每个词语的单独出现概率：</p>
<ul>
<li>$ P(\text{“dog”}) &#x3D; \frac{100}{1000} &#x3D; 0.1 $</li>
<li>$ P(\text{“bone”}) &#x3D; \frac{50}{1000} &#x3D; 0.05 $</li>
</ul>
<p>然后，我们计算两个词语同时出现的概率：</p>
<ul>
<li>$ P(\text{“dog”}, \text{“bone”}) &#x3D; \frac{20}{1000} &#x3D; 0.02 $</li>
</ul>
<p>接下来，我们使用 PMI 公式计算两个词语之间的 PMI 值：</p>
<p>$ \text{PMI}(\text{“dog”}, \text{“bone”}) &#x3D; \log_2 \left( \frac{0.02}{0.1 \times 0.05} \right) $</p>
<p>$ \text{PMI}(\text{“dog”}, \text{“bone”}) &#x3D; \log_2 \left( \frac{0.02}{0.005} \right) $</p>
<p>$ \text{PMI}(\text{“dog”}, \text{“bone”}) &#x3D; \log_2(4) \approx 2 $</p>
<p>因此，单词 “dog” 和 “bone” 之间的 PMI 值为 2。这表示这两个词语之间存在一定的关联性，因为其共现概率远大于它们单独出现的概率。</p>
<p><strong>如何解读：</strong></p>
<p>PMI 的数值可以在不同的范围内，具体需要做怎样的解释取决于其具体的数值大小。</p>
<ol>
<li><p><strong>PMI &gt; 0</strong>：</p>
<ul>
<li>正值的 PMI 表示两个词之间的共现概率大于它们各自单独出现的概率。这表明两个词之间存在一定程度的正相关性，即它们很可能彼此相关或者一起出现。</li>
<li>解释：两个词在语境中可能具有相关的含义或语义关联。</li>
</ul>
</li>
<li><p><strong>PMI &#x3D; 0</strong>：</p>
<ul>
<li>PMI 值为零表示两个词之间的共现概率与它们各自单独出现的概率相等。这表明两个词之间不存在明显的相关性。</li>
<li>解释：两个词之间可能是独立的，它们出现或不出现不会互相影响。</li>
</ul>
</li>
<li><p><strong>PMI &lt; 0</strong>：</p>
<ul>
<li>负值的 PMI 表示两个词之间的共现概率小于它们各自单独出现的概率。这表明两个词之间存在一定程度的负相关性，即它们很可能不会同时出现。</li>
<li>解释：两个词之间可能具有互斥的含义，或者在不同的语境下可能会出现。</li>
</ul>
</li>
</ol>
<p>需要注意的是，对于 PMI 的具体阈值并没有一个标准的界定，解释也会因具体情况而异。一般来说，需要根据具体的应用场景和语料库的特性来判断 PMI 值的意义和阈值。</p>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>词嵌入（Word Embedding）是一种将词语映射到实数向量空间的技术，它将高维的离散词语表示转换为低维的实数向量表示。这种表示方式能够捕捉词语之间的语义和语法关系，并且可以作为输入提供给各种自然语言处理模型。</p>
<p>词嵌入技术的核心思想是，将词语嵌入到一个连续向量空间中，使得具有相似含义或上下文关系的词语在向量空间中的表示更加接近，而具有不同含义或上下文关系的词语在向量空间中的表示相距较远。</p>
<p>词嵌入的主要特点包括：</p>
<ol>
<li><strong>语义信息</strong>：词嵌入向量能够捕捉词语的语义信息，使得具有相似含义的词语在向量空间中距离较近。</li>
<li><strong>语法信息</strong>：词嵌入向量还能够捕捉词语的语法信息，使得在语法结构中具有相似角色或功能的词语在向量空间中距离较近。</li>
<li><strong>低维表示</strong>：词嵌入将高维的离散词语表示转换为低维的实数向量表示，减少了特征空间的维度。</li>
<li><strong>密集表示</strong>：与传统的词袋模型等稀疏表示不同，词嵌入是一种密集的表示方式，每个词都对应着一个实数向量。</li>
</ol>
<p>词嵌入技术在自然语言处理中被广泛应用，包括文本分类、情感分析、语义理解、机器翻译等任务中。它为模型提供了更加丰富和有意义的输入表示，从而提升了模型的性能和效果。</p>
<p>词嵌入图示：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310235135975.png" alt="image-20240310235135975"></p>
<p><strong>一个第三方库：Word2Vec</strong> </p>
<p>Word2Vec 是由 Google 在 2013 年提出的一种用于生成词嵌入的算法，它可以将词语映射到实数向量空间中，并且能够捕捉词语之间的语义和语法关系。Word2Vec 是基于神经网络模型的，通过训练神经网络来学习词语的向量表示。</p>
<p>Word2Vec 主要有两种模型：Continuous Bag of Words (CBOW) 和 Skip-gram。</p>
<ol>
<li><p>**Continuous Bag of Words (CBOW)**：<br>CBOW 模型的目标是根据上下文词语来预测目标词语。具体来说，CBOW 模型输入是一个固定大小的上下文窗口内的词语的词向量的平均值，输出是目标词语的词向量。CBOW 模型的训练目标是最大化给定上下文的条件下，目标词语的概率。</p>
</li>
<li><p><strong>Skip-gram</strong>：<br>Skip-gram 模型与 CBOW 模型相反，它的目标是根据目标词语来预测上下文词语。具体来说，Skip-gram 模型输入是一个目标词语的词向量，输出是该词语上下文窗口内各个词语的词向量。Skip-gram 模型的训练目标是最大化给定目标词语的条件下，上下文词语的概率。</p>
</li>
</ol>
<p>无论是 CBOW 还是 Skip-gram，Word2Vec 都是通过训练神经网络来学习词向量，一般使用的神经网络是浅层的前馈神经网络。Word2Vec 的优化目标是通过学习词向量，使得在语料库中共现的词语的词向量在向量空间中彼此靠近，而不共现的词语的词向量在向量空间中彼此远离。</p>
<p><strong>CBOW的例子。</strong></p>
<p>以下是一个简单的 CBOW 模型的例子：</p>
<p>假设我们有一个句子：”the quick brown fox jumps over the lazy dog”。</p>
<p>我们想要使用 CBOW 模型来学习词嵌入。</p>
<ol>
<li><p><strong>数据预处理</strong>：</p>
<ul>
<li>首先，我们需要将文本分解成词语，并创建一个词汇表。</li>
<li>然后，我们需要定义一个上下文窗口的大小（例如，窗口大小为 2）。</li>
</ul>
</li>
<li><p><strong>创建训练数据</strong>：</p>
<ul>
<li>对于每个词语，我们要从文本中选择该词语的上下文词语作为训练样本。</li>
<li>对于词语 “brown”，其上下文词语可以是 “the” 和 “quick”，也可以是 “quick” 和 “fox”，以此类推。</li>
</ul>
</li>
<li><p><strong>创建输入输出对</strong>：</p>
<ul>
<li>对于每个训练样本，我们将上下文词语的词向量作为输入，目标词语的词向量作为输出。</li>
<li>例如，对于上下文词语 “the” 和 “quick”，我们的输入是这两个词语的词向量的平均值，目标是词语 “brown” 的词向量。</li>
</ul>
</li>
<li><p><strong>训练 CBOW 模型</strong>：</p>
<ul>
<li>我们使用输入-输出对来训练神经网络模型。</li>
<li>神经网络模型的架构可以是一个简单的前馈神经网络，其中输入层是上下文词语的词向量的平均值，输出层是目标词语的词向量。</li>
<li>训练目标是最小化模型预测和实际目标之间的误差。</li>
</ul>
</li>
</ol>
<p>通过重复这个过程，我们可以训练出一个 CBOW 模型，并得到每个词语的词向量。这些词向量可以捕捉词语之间的语义和语法关系，并且可以用于各种自然语言处理任务，例如文本分类、情感分析等。</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000144503.png" alt="image-20240311000144503"></p>
<p><strong>Skip-gram的例子。</strong></p>
<p>假设我们有一个句子：”the quick brown fox jumps over the lazy dog”。</p>
<p>我们想要使用 Skip-gram 模型来学习词嵌入。</p>
<ol>
<li><p><strong>数据预处理</strong>：</p>
<ul>
<li>首先，我们将文本分解成词语，并创建一个词汇表。</li>
<li>然后，我们需要定义一个上下文窗口的大小（例如，窗口大小为 2）。</li>
</ul>
</li>
<li><p><strong>创建训练数据</strong>：</p>
<ul>
<li>对于每个词语，我们要创建训练样本，目标是该词语的上下文词语。</li>
<li>对于词语 “brown”，我们可以选择其上下文词语为 “quick” 和 “fox”，也可以选择为 “the” 和 “quick”，以此类推。</li>
</ul>
</li>
<li><p><strong>创建输入输出对</strong>：</p>
<ul>
<li>对于每个训练样本，我们将目标词语的词向量作为输入，上下文词语的词向量作为输出。</li>
<li>例如，对于目标词语 “brown”，我们的输入是其词向量，输出是上下文词语 “quick” 和 “fox” 的词向量。</li>
</ul>
</li>
<li><p><strong>训练 Skip-gram 模型</strong>：</p>
<ul>
<li>我们使用输入-输出对来训练神经网络模型。</li>
<li>神经网络模型的架构可以是一个简单的前馈神经网络，其中输入层是目标词语的词向量，输出层是上下文词语的词向量。</li>
<li>训练目标是最小化模型预测和实际上下文词语之间的误差。</li>
</ul>
</li>
</ol>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000200054.png" alt="image-20240311000200054"></p>
<p>通过重复这个过程，我们可以训练出一个 Skip-gram 模型，并得到每个词语的词向量。这些词向量可以捕捉词语之间的语义和语法关系，并且可以用于各种自然语言处理任务，例如文本分类、情感分析等。</p>
<p><strong>词比较</strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000339387.png" alt="image-20240311000339387"></p>
<p>三种计算方法：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240311000402355.png" alt="image-20240311000402355"></p>
<h2 id="一些问题-1"><a href="#一些问题-1" class="headerlink" title="一些问题"></a>一些问题</h2><p><strong>What languages do you speak other than English? How they are different from English?</strong></p>
<p>中文</p>
<p>中文和英文是两种不同的语言，它们在语言结构、语法、词汇、书写方式以及文化背景等方面存在着显著的差异。以下是它们之间的一些主要差异：</p>
<ol>
<li><p><strong>语言结构和语法：</strong></p>
<ul>
<li>中文是一种主谓宾的语言，句子的基本结构通常是主语 + 谓语 + 宾语。而英文则是一种主谓宾语的语言，但也会有其他句型。</li>
<li>中文没有词性变化，而英文则根据语法需要会有名词、动词、形容词等的变化，如单复数、时态、语态等。</li>
</ul>
</li>
<li><p><strong>词汇和表达方式：</strong></p>
<ul>
<li>中文的词汇较英文更加象形，一些词汇的构造常常可以反映出具体的意象，而英文则更加倾向于抽象和形式化的表达。</li>
<li>中文中有很多成语、俗语等文化内涵丰富的表达方式，而英文也有类似的表达，但文化背景不同，所用的成语、俗语也会有所不同。</li>
</ul>
</li>
<li><p><strong>书写方式：</strong></p>
<ul>
<li>中文是以汉字为基础的文字系统，每个汉字代表一个音节或一个词，每个汉字都有独立的书写形式。而英文则是以拉丁字母为基础，单词是由字母组成的，字母组合的顺序和方式决定了单词的意义。</li>
</ul>
</li>
<li><p><strong>语音和发音：</strong></p>
<ul>
<li>中文是以声调为特征的语言，同一个音节在不同的声调下会产生不同的意义。而英文则是以音节和重音为基础的，重音的位置对单词的理解和发音有着重要影响。</li>
</ul>
</li>
<li><p><strong>文化和思维方式：</strong></p>
<ul>
<li>中文和英文所承载的文化背景不同，因此在表达方式、文学风格、逻辑思维等方面也有所差异。中文更注重含蓄、间接的表达方式，而英文则更加直接和实用。</li>
</ul>
</li>
</ol>
<p><strong>What are the similarities and differences between logistic regression and neural networks?</strong></p>
<table>
<thead>
<tr>
<th>特征</th>
<th>逻辑回归</th>
<th>神经网络</th>
</tr>
</thead>
<tbody><tr>
<td>用途</td>
<td>二元或多元分类</td>
<td>二元或多元分类</td>
</tr>
<tr>
<td>参数化形式</td>
<td>单层线性模型</td>
<td>多层神经网络</td>
</tr>
<tr>
<td>损失函数</td>
<td>交叉熵损失函数</td>
<td>交叉熵损失函数</td>
</tr>
<tr>
<td>模型复杂度</td>
<td>简单，较低的复杂度</td>
<td>复杂，较高的复杂度</td>
</tr>
<tr>
<td>非线性能力</td>
<td>仅能处理线性可分数据</td>
<td>具有强大的非线性拟合能力</td>
</tr>
<tr>
<td>特征学习</td>
<td>需要手动选择或提取特征</td>
<td>可以自动学习特征表示</td>
</tr>
<tr>
<td>计算复杂度</td>
<td>较低，训练速度较快</td>
<td>较高，训练速度较慢</td>
</tr>
</tbody></table>
<p><strong>Given the function f(x) &#x3D; x^3, what is the value of the slope at x &#x3D; 1?</strong></p>
<p>To find the slope of the function $f(x) &#x3D; x^3$ at $x &#x3D; 1$, we need to find the derivative of the function $f(x)$ with respect to $x$, and then evaluate it at $x &#x3D; 1$.</p>
<p>The derivative of  $f(x) &#x3D; x^3$ with respect to $x$ is given by:</p>
<p>$f’(x) &#x3D; \frac{d}{dx}(x^3) &#x3D; 3x^2$</p>
<p>Now, to find the slope at $x &#x3D; 1$, we substitute $x &#x3D; 1$ into the derivative:</p>
<p>$f’(1) &#x3D; 3 \times (1)^2 &#x3D; 3$</p>
<p>Therefore, the slope of the function $f(x) &#x3D; x^3$ at $x &#x3D; 1$ is $3$.</p>
<p><strong>What is Stochastic Gradient Descent, and how is it functionally different to Batch Gradient Descent?</strong></p>
<p>随机梯度下降（Stochastic Gradient Descent，SGD）和批量梯度下降（Batch Gradient Descent）都是优化算法，用于训练机器学习模型，特别是在深度学习中应用广泛。它们之间的功能区别在于优化过程中处理数据的方式。</p>
<p>随机梯度下降（SGD）：</p>
<ul>
<li>SGD是一种迭代优化算法，每次迭代时只使用训练集中的一个样本来更新模型的参数。</li>
<li>在每次迭代中，随机选择一个样本，计算该样本的梯度，然后使用该梯度来更新模型的参数。</li>
<li>SGD的优点是计算开销小，尤其适用于大规模数据集和大型模型，因为每次迭代只需处理一个样本。</li>
<li>SGD的缺点是由于随机选择样本，更新方向不一定是最优的，导致参数更新的方差较大，收敛速度相对较慢，甚至会在局部最优点附近震荡。</li>
</ul>
<p>批量梯度下降（Batch Gradient Descent）：</p>
<ul>
<li>批量梯度下降是指在每次迭代中使用整个训练集来计算梯度并更新模型参数。</li>
<li>对于每个迭代，计算所有训练样本的梯度，然后使用平均梯度来更新模型的参数。</li>
<li>批量梯度下降的优点是更新方向更稳定，更有可能接近全局最优点，收敛速度相对较快。</li>
<li>缺点是计算开销大，特别是对于大规模数据集和大型模型，每次迭代都需要处理整个数据集。</li>
</ul>
<p>功能上的区别：</p>
<ul>
<li>SGD每次迭代只使用一个样本，计算速度快，但更新方向不稳定，容易陷入局部最优解。</li>
<li>批量梯度下降使用整个训练集来计算梯度，更新方向更稳定，但计算开销大，特别是在大规模数据集上。</li>
</ul>
<p>综上所述，SGD和批量梯度下降是常用的优化算法，在训练机器学习模型时可以根据数据集的大小和模型的复杂度选择合适的算法。</p>
<p>假设我们有一个简单的线性回归问题，我们要拟合一个线性模型：$y &#x3D; mx + b$，其中 m 是斜率，b 是截距。</p>
<p>我们有以下数据集：</p>
<table>
<thead>
<tr>
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
</tr>
<tr>
<td>4</td>
<td>9</td>
</tr>
</tbody></table>
<p>我们的目标是使用梯度下降算法来找到最佳的斜率 $m$ 和截距 $b$，使得模型拟合数据集最好。</p>
<p>随机梯度下降（SGD）：</p>
<ol>
<li>随机选择一个样本，例如选择 $ (x&#x3D;1, y&#x3D;3) $。</li>
<li>计算该样本的梯度并更新参数 $m$ 和 $b$。</li>
<li>重复以上步骤直到达到收敛条件。</li>
</ol>
<p>批量梯度下降（Batch GD）：</p>
<ol>
<li>使用整个数据集计算梯度，并更新参数 $m$ 和 $b$。</li>
<li>重复以上步骤直到达到收敛条件。</li>
</ol>
<p>让我们假设初始参数 $m &#x3D; 0$ 和 $b &#x3D; 0$。我们使用以下的损失函数（均方误差）来更新参数：</p>
<p>$\text{Loss} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} (y_i - (mx_i + b))^2$</p>
<p>在实际应用中，我们需要设置学习率 $\alpha$ 和迭代次数。</p>
<p>接下来，我们可以使用具体的算法来实现随机梯度下降和批量梯度下降，并观察它们的参数更新过程和收敛情况。</p>
<p>首先使用Python语言生成随机数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_gradient_descent</span>(<span class="params">X, y, learning_rate=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">    m = <span class="built_in">len</span>(X)</span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># 初始化参数</span></span><br><span class="line">    theta_history = []</span><br><span class="line">    loss_history = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        gradients = <span class="number">2</span>/m * X.T.dot(X.dot(theta) - y)</span><br><span class="line">        theta = theta - learning_rate * gradients</span><br><span class="line">        loss = np.mean((X.dot(theta) - y) ** <span class="number">2</span>)</span><br><span class="line">        loss_history.append(loss)</span><br><span class="line">        theta_history.append(theta)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> theta, theta_history, loss_history</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加偏置项</span></span><br><span class="line">X_b = np.c_[np.ones((<span class="number">1000</span>, <span class="number">1</span>)), X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行批量梯度下降</span></span><br><span class="line">theta_batch, theta_history_batch, loss_history_batch = batch_gradient_descent(X_b, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失函数随迭代次数的变化</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(loss_history_batch)), loss_history_batch, label=<span class="string">&#x27;Batch GD&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Loss over Epochs (Batch GD)&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制参数随迭代次数的变化</span></span><br><span class="line">theta_history_batch = np.array(theta_history_batch)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(theta_history_batch)), theta_history_batch[:, <span class="number">0</span>], label=<span class="string">&#x27;Intercept&#x27;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(theta_history_batch)), theta_history_batch[:, <span class="number">1</span>], label=<span class="string">&#x27;Slope&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Parameter Value&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Parameter Values over Epochs (Batch GD)&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>可视化的结果为：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310180013521.png" alt="image-20240310180013521"></p>
<p><strong>How does regularisation reduce overfitting? Explain this given the cost function with regularisation.</strong></p>
<p>正则化是机器学习中常用的一种技术，旨在帮助减少模型的过拟合，并提高模型的泛化能力。正则化通过在模型的损失函数中添加额外的惩罚项来实现。</p>
<p>在正则化中，通常会将原始的损失函数与正则化项相结合，形成新的带有惩罚项的损失函数。这样的损失函数更倾向于选择简单的模型，即在模型参数较多的情况下，尽量让参数的取值趋于较小的范围，从而减少模型的复杂度。</p>
<p>常用的两种正则化方法是L1正则化（Lasso）和L2正则化（Ridge）：</p>
<ol>
<li><p><strong>L1正则化（Lasso）：</strong> 在损失函数中添加模型参数的绝对值之和，即 $ \lambda \sum_{i&#x3D;1}^{n} |w_i| $。L1正则化倾向于产生稀疏权重，即使得一些特征的权重为0，从而达到特征选择（Feature Selection）的效果。</p>
</li>
<li><p><strong>L2正则化（Ridge）：</strong> 在损失函数中添加模型参数的平方和，即 $ \lambda \sum_{i&#x3D;1}^{n} w_i^2 $。L2正则化倾向于使得权重分布较为平滑，避免出现特别大的权重值，有助于防止模型过度拟合。</p>
</li>
</ol>
<p>在实际应用中，正则化参数 $ \lambda $ 是需要手动设定的超参数，通常通过交叉验证来选择最佳的值。</p>
<p>正则化可以帮助减少过度拟合的主要原因如下：</p>
<ol>
<li><p><strong>惩罚复杂模型：</strong> 正则化在损失函数中引入了惩罚项，惩罚复杂模型，使得模型更倾向于选择简单的参数值。这样可以防止模型过度拟合训练数据，即模型过分适应训练数据中的噪声和异常值。</p>
</li>
<li><p><strong>控制模型的参数大小：</strong> 正则化通过对模型参数施加惩罚，促使模型参数的值保持在一个较小的范围内。这有助于避免模型参数过大，减少了对训练数据的敏感度，从而降低了模型的方差。</p>
</li>
<li><p><strong>特征选择（Feature Selection）：</strong> 在某些情况下，L1正则化可以使得一些特征的权重变为0，即稀疏化。这意味着模型只依赖于少数几个最重要的特征，忽略了对预测目标无关或相关性较弱的特征，从而简化了模型，减少了过拟合的可能性。</p>
</li>
<li><p><strong>泛化能力提升：</strong> 通过控制模型的复杂度，正则化可以提高模型的泛化能力，使得模型在面对新的未见数据时，能够更好地进行预测。</p>
</li>
</ol>
<p>总的来说，正则化通过惩罚复杂模型，控制模型的参数大小，并进行特征选择，帮助模型更好地适应于未知的数据，从而减少过度拟合的风险。</p>
<p>损失函数（Loss Function）是机器学习模型中的一个重要组成部分，用于衡量模型预测结果与实际观测值之间的差异或误差程度。损失函数通常在训练过程中被优化，以便模型能够更准确地预测目标变量。</p>
<p>在监督学习中，损失函数通常定义为模型预测值和真实标签之间的差异，它是一个关于模型参数的函数。通过调整模型参数，优化算法试图最小化损失函数，从而使模型能够更好地拟合训练数据，同时提高在未见数据上的泛化能力。</p>
<p>不同的机器学习任务和模型类型可能会使用不同的损失函数。以下是一些常见的损失函数：</p>
<ol>
<li><p><strong>均方误差（Mean Squared Error，MSE）：</strong> 用于回归问题，计算预测值与真实值之间的平方差的均值。</p>
</li>
<li><p><strong>交叉熵损失（Cross-Entropy Loss）：</strong> 用于分类问题，衡量模型预测结果与真实标签之间的差异，常用于逻辑回归、softmax分类器等模型。</p>
</li>
<li><p><strong>对数损失（Log Loss）：</strong> 用于二元分类问题，是交叉熵损失的特例。</p>
</li>
<li><p><strong>Hinge Loss：</strong> 用于支持向量机（SVM）等模型中的分类问题，适用于二元分类任务。</p>
</li>
<li><p><strong>绝对误差（Absolute Error）：</strong> 也称为 L1 损失，用于回归问题，计算预测值与真实值之间的绝对差的均值。</p>
</li>
</ol>
<p>损失函数的选择取决于具体的问题和模型，需要根据任务的性质和数据的特点来合理选取，以达到更好的模型训练效果。</p>
<p>成本函数（Cost Function）通常用于描述机器学习模型的整体性能，它是模型参数的函数，表示模型在训练集上的整体误差或损失。成本函数是损失函数的扩展，损失函数通常用于衡量单个样本或样本集的误差，而成本函数则用于衡量整个训练集上的误差。</p>
<p>具有正则化的成本函数是指在模型的损失函数中添加了正则化项，用于惩罚模型的复杂度。正则化项通常是模型参数的范数（L1或L2范数），乘以一个正则化参数 $\lambda$，这个参数控制了正则化的强度。</p>
<p>正则化的成本函数通常表示为：</p>
<p> $J(\theta) &#x3D; \text{损失函数} + \lambda \times \text{正则化项}$</p>
<p>其中，$J(\theta)$ 是带有正则化的成本函数，$\theta$ 是模型的参数，损失函数表示模型的预测误差，正则化项用于惩罚模型的复杂度， $ \lambda$是正则化参数，控制了正则化的强度。</p>
<p>正则化的成本函数的引入可以实现以下目的：</p>
<ol>
<li><p><strong>控制模型的复杂度：</strong> 正则化项通过惩罚模型的复杂度，鼓励模型选择较简单的参数值。这有助于防止模型过度拟合训练数据，提高模型的泛化能力。</p>
</li>
<li><p><strong>避免过拟合：</strong> 过度拟合是指模型在训练数据上表现良好，但在测试数据上表现较差的现象。通过引入正则化项，可以有效减少过拟合的风险，使模型更好地适应新的数据。</p>
</li>
<li><p><strong>权衡预测精度和模型复杂度：</strong> 正则化参数 $\lambda$ 控制了正则化的强度。较大的 $\lambda$ 值会增加正则化的力度，从而降低模型的复杂度，但可能会损失一定的预测精度；较小的 $\lambda$ 值则更侧重于提高预测精度，但可能导致模型过度拟合。</p>
</li>
</ol>
<p>综上所述，具有正则化的成本函数可以帮助平衡模型的预测精度和复杂度，降低过拟合的风险，提高模型的泛化能力。</p>
<p><strong>Share your impressions of using the tensorflow playground. What have you learned, what was interesting?</strong></p>
<p>TensorFlow Playground 是一个基于网页的交互式可视化工具，用于探索神经网络的基本原理和参数调整对模型的影响。在使用 TensorFlow Playground 进行探索时，我获得了一些有趣的体验和启发：</p>
<ol>
<li><p><strong>直观理解神经网络结构：</strong> TensorFlow Playground 提供了直观的界面，可以通过简单的拖动滑块和选择不同的参数来构建和调整神经网络的结构。这帮助我更好地理解神经网络的层数、节点数和激活函数等参数对模型性能的影响。</p>
</li>
<li><p><strong>探索不同激活函数的效果：</strong> 通过在 TensorFlow Playground 中尝试不同的激活函数（如ReLU、Sigmoid、Tanh等），我能够直观地观察到它们对模型的影响。例如，ReLU 激活函数可以有效缓解梯度消失问题，而 Sigmoid 和 Tanh 函数则可能导致梯度消失和梯度爆炸问题。</p>
</li>
<li><p><strong>观察模型的决策边界：</strong> TensorFlow Playground 提供了一个可视化的界面，可以实时显示模型在输入空间中的决策边界。这使我能够直观地理解神经网络是如何在输入空间中进行分类和决策的。</p>
</li>
<li><p><strong>调整学习率和正则化参数：</strong> 在 TensorFlow Playground 中，我可以调整学习率和正则化参数，观察模型训练过程中损失函数的变化。这帮助我理解了学习率对收敛速度和模型性能的影响，以及正则化参数对模型的泛化能力的影响。</p>
</li>
</ol>
<p>总的来说，使用 TensorFlow Playground 是一个富有启发性的体验，它使我能够通过直观的方式探索神经网络的基本原理和参数调整对模型的影响。这有助于加深对神经网络工作原理的理解，并且为进一步探索深度学习提供了良好的起点。</p>
<p><strong>Using pretrained word vectors to find similar words (online interface) <a target="_blank" rel="noopener" href="http://vectors.nlpl.eu/explore/embeddings/en/">http://vectors.nlpl.eu/explore/embeddings/en/#
  </a></strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-01%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D/image-20240310174834750.png" alt="image-20240310174834750"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"># 自然语言处理</a>
              <a href="/tags/NPL/" rel="tag"># NPL</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-00%EF%BC%9A%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E7%82%B9/" rel="prev" title="自然语言处理 00：前置知识点">
                  <i class="fa fa-chevron-left"></i> 自然语言处理 00：前置知识点
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/03/07/Linux%E7%9F%A5%E8%AF%86%E7%82%B9/" rel="next" title="Linux知识点">
                  Linux知识点 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Jiu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
