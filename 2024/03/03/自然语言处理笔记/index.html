<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="学校的自然语言处理课程的笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理笔记">
<meta property="og:url" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="夜久">
<meta property="og:description" content="学校的自然语言处理课程的笔记">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303153844956.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303192849562.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303154755596.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303162518422.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303162702975.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303163722854.png">
<meta property="og:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303164126043.png">
<meta property="article:published_time" content="2024-03-03T04:48:00.000Z">
<meta property="article:modified_time" content="2024-03-03T09:19:13.358Z">
<meta property="article:author" content="Ye Jiu">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="自然语言处理">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303153844956.png">


<link rel="canonical" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/","path":"2024/03/03/自然语言处理笔记/","title":"自然语言处理笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>自然语言处理笔记 | 夜久</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">夜久</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/848368468&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/terence-vassallo-415912750" title="Ocelotter" target="_blank" style="color: #cccccc; text-decoration: none;">Ocelotter</a> · <a href="https://soundcloud.com/terence-vassallo-415912750/euphoria" title="Euphoria - 楽園の扉 中日字幕" target="_blank" style="color: #cccccc; text-decoration: none;">Euphoria - 楽園の扉 中日字幕</a></div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=535990&auto=1&height=66"></iframe>

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">1.</span> <span class="nav-text">课程前置知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E9%9C%80%E6%B1%82"><span class="nav-number">1.1.</span> <span class="nav-text">知识需求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%90%E4%BE%9B%E8%B5%84%E6%BA%90"><span class="nav-number">1.2.</span> <span class="nav-text">提供资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E5%90%88%E3%80%81%E5%87%BD%E6%95%B0%E3%80%81%E6%B1%82%E5%92%8C"><span class="nav-number">1.3.</span> <span class="nav-text">集合、函数、求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-number">1.4.</span> <span class="nav-text">描述性统计知识点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.5.</span> <span class="nav-text">朴素贝叶斯与分类器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-number">2.</span> <span class="nav-text">第一周知识点</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E5%91%A8%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.1.</span> <span class="nav-text">本周任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">2.2.</span> <span class="nav-text">词向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7%E4%B8%8E%E7%AE%97%E6%B3%95"><span class="nav-number">2.3.</span> <span class="nav-text">工具与算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%85%B7%E4%BD%93%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">2.4.</span> <span class="nav-text">一些具体的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.5.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">2.6.</span> <span class="nav-text">一些问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-number">2.7.</span> <span class="nav-text">主题知识点</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Jiu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Jiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夜久">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="自然语言处理笔记 | 夜久">
      <meta itemprop="description" content="学校的自然语言处理课程的笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          自然语言处理笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-03-03 15:18:00 / Modified: 19:49:13" itemprop="dateCreated datePublished" datetime="2024-03-03T15:18:00+10:30">2024-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">学校的自然语言处理课程的笔记</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="课程前置知识"><a href="#课程前置知识" class="headerlink" title="课程前置知识"></a>课程前置知识</h1><h2 id="知识需求"><a href="#知识需求" class="headerlink" title="知识需求"></a>知识需求</h2><p>编程语言需求：Python语言。</p>
<p>数学知识：数据科学的数学基础。</p>
<ul>
<li><input disabled type="checkbox"> 基础知识：集合、函数、求和</li>
<li><input disabled type="checkbox"> 概率</li>
<li><input disabled type="checkbox"> 朴素贝叶斯定理和分类器</li>
<li><input disabled type="checkbox"> 用矩阵表示数据</li>
<li><input disabled type="checkbox"> 解线性方程</li>
<li><input disabled type="checkbox"> 降维：主成分分析（PCA）</li>
<li><input disabled type="checkbox"> 微积分和梯度下降</li>
</ul>
<p>机器学习与人工智能知识。</p>
<ul>
<li><input disabled type="checkbox"> 线性回归，成本&#x2F;损失函数，均方误差。 </li>
<li><input disabled type="checkbox"> 分类和交叉验证：贝叶斯分类、k-NN（k近邻）、决策树、感知器训练、逻辑回归、支持向量机（SVM）。 </li>
<li><input disabled type="checkbox"> 聚类：k均值。</li>
</ul>
<h2 id="提供资源"><a href="#提供资源" class="headerlink" title="提供资源"></a>提供资源</h2><p><strong>Python和R</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.w3schools.com/python/">W3 Python 教程到外部站点的链接。</a>是刷新 Python 知识的好资源。本教程包含一组页面，涉及 Python 编程中的许多主题，并提供了一个简单的环境来试验 Python 代码。</p>
<p><a target="_blank" rel="noopener" href="https://www.w3schools.com/r/">W3 R教程到外部站点的链接。</a>或者Katya Ognyanova对<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/77838/files/11103244/download">R 基础知识和可视化工具</a>的精彩介绍。还有一个附带的<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/77838/files/11103243/download">R 代码</a>。</p>
<p>以下资源将帮助建立&#x2F;修改统计知识。这些是课程中许多主题中使用的重要概念。</p>
<p><strong>统计</strong></p>
<p>总结<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540179?wrap=1">_</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540179/download?download_frd=1">下载摘要</a>的描述性统计。</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303153844956.png" alt="image-20240303153844956"></p>
<p>DS 摩尔和 GP 麦凯布 (1989)。<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2246282641530001811">统计实践简介到外部站点的链接。</a>。WH 弗里曼&#x2F;时代图书&#x2F;亨利·霍尔特公司</p>
<p>有关统计的其他资源：</p>
<p>其他资源：</p>
<ul>
<li>描述性统计： <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Five-number_summary">https://en.wikipedia.org/wiki/Five-number_summary到外部站点的链接。</a></li>
<li>汇总数据<a target="_blank" rel="noopener" href="https://www.youtube.com/channel/UCxdR3vGHDUYIOm6RqUJxPPg/videos">https://www.youtube.com/channel/UCxdR3vGHDUYIOm6RqUJxPPg/videos到外部站点的链接。</a></li>
<li>形状、分布、异常值<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Q8b1_dOM8LU">https://www.youtube.com/watch?v=Q8b1_dOM8LU到外部站点的链接。</a></li>
<li>箱形图<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bhCGIUmZeDE">https://www.youtube.com/watch?v=bhCGIUmZeDE到外部站点的链接。</a></li>
</ul>
<p><strong>概率</strong></p>
<p>以下资源将帮助建立&#x2F;修改有关概率的知识。这些是贝叶斯方法中的重要概念，例如朴素贝叶斯算法。</p>
<p><a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2246282644410001811">机器学习数学到外部站点的链接。</a>，第 1.2 章。剑桥大学出版社。</p>
<p><strong>线性代数和微积分</strong></p>
<p>对于本模块，需要熟悉几个数学主题，其中包括：</p>
<ul>
<li>矩阵求逆和转置</li>
<li>矩阵-矩阵运算</li>
<li>矩阵向量运算</li>
<li>向量点积</li>
<li>偏导数</li>
</ul>
<p>以下<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540175?wrap=1"><strong>幻灯片</strong></a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540175/download?download_frd=1"> 下载幻灯片</a>总结这些基本的数学概念。</p>
<p>这是一个关于矩阵向量乘法的视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aV-P3mDgK_E">https://www.youtube.com/watch?v=aV-P3mDgK_E到外部站点的链接。</a></p>
<p> 可以在这里找到很好的数学参考：<a target="_blank" rel="noopener" href="http://cs229.stanford.edu/summer2020/cs229-linalg.pdf">http://cs229.stanford.edu/summer2020/cs229-linalg.pdf到外部站点的链接。</a></p>
<p>以下书籍是深入了解数据科学中使用的数学方法的良好学习来源： Sheldon Axler，<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2242923306420001811">线性代数做得正确到外部站点的链接。</a>，第三版，施普林格。</p>
<p><strong>机器学习和神经网络简介</strong></p>
<p>以下书籍是一本很好的机器学习入门读物，可能有助于复习 ML 概念。</p>
<p>Rebala, G.、Ravi, A. 和 Churiwala, S. (2019)。<a target="_blank" rel="noopener" href="https://adelaide.leganto.exlibrisgroup.com/lti/launch?institute=61ADELAIDE_INST&tool=CANVAS_LTI_v1_1&citation_id=2242923306410001811">机器学习简介到外部站点的链接。</a>。施普林格。</p>
<p>如果您只喜欢摘要，这里有关于经典机器学习各种主题的幻灯片：</p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540176?wrap=1">机器学习方法论</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540176/download?download_frd=1">下载机器学习方法</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540180?wrap=1">k-最近邻</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540180/download?download_frd=1">下载 k 最近邻</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540191?wrap=1">朴素贝叶斯方法</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540191/download?download_frd=1">下载朴素贝叶斯方法</a></p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540184?wrap=1"><strong>幻灯片</strong></a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540184/download?download_frd=1"> 下载幻灯片</a>总结神经网络的基础知识。</p>
<p><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178?wrap=1"><strong>另外，Python代码</strong></a>的示例<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178?wrap=1"> </a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540178/download?download_frd=1"> 下载代码</a>使用这个**<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540204?wrap=1">数据集</a><a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/files/14540204/download?download_frd=1"> 下载数据集</a>**用于测试各种机器学习方法。</p>
<p>使用不同的机器学习方法来进行训练范例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_fscore_support, accuracy_score,classification_report</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> BernoulliNB</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line">Data=pd.read_csv(<span class="string">&#x27;titanic.csv&#x27;</span>)</span><br><span class="line">Data1 = Data[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>,<span class="string">&quot;Survived&quot;</span>]]</span><br><span class="line"><span class="keyword">if</span> Data1.Age <span class="keyword">is</span> <span class="literal">None</span>: Data1[<span class="string">&quot;Age&quot;</span>] = Data1.Age.mean()</span><br><span class="line">Data1 = Data1.dropna()</span><br><span class="line">Data2 = Data1.copy()</span><br><span class="line">Data2[<span class="string">&#x27;Sex&#x27;</span>]=Data1[<span class="string">&#x27;Sex&#x27;</span>].replace([<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;female&#x27;</span>],[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">Y=Data2.Survived</span><br><span class="line">X=Data2</span><br><span class="line">X.drop([<span class="string">&#x27;Survived&#x27;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=<span class="number">0.2</span>, random_state=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Nauve Bayes</span></span><br><span class="line">classifier=BernoulliNB(fit_prior=<span class="literal">True</span>, alpha=<span class="number">1.0</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BernoulliNB\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Decision Tree</span></span><br><span class="line">classifier = tree.DecisionTreeClassifier(min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">    min_samples_split=<span class="number">2</span>, max_depth=<span class="literal">None</span>,criterion=<span class="string">&#x27;entropy&#x27;</span>,random_state=<span class="number">0</span>)</span><br><span class="line">model = classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;DecisionTreeClassifier \n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; sklearn NN &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># p is the parameter for Minkowski distance</span></span><br><span class="line"><span class="comment"># weights can be also &#x27;distance&#x27;</span></span><br><span class="line">classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, p=<span class="number">2</span>, weights=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KNeighborsClassifier\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">classifier = MLPClassifier(solver=<span class="string">&#x27;lbfgs&#x27;</span>, alpha=<span class="number">1e-5</span>, max_iter=<span class="number">300</span>,</span><br><span class="line">                    hidden_layer_sizes=(<span class="number">5</span>, <span class="number">2</span>), random_state=<span class="number">1</span>,</span><br><span class="line">                    validation_fraction=<span class="number">0.2</span>)</span><br><span class="line">classifier.fit(X_train, Y_train)</span><br><span class="line">predicted_y=classifier.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MLPClassifier\n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; keras NN &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="comment"># keras.layers.Dense(units, activation=None, use_bias=True, </span></span><br><span class="line"><span class="comment"># kernel_initializer=&#x27;glorot_uniform&#x27;, </span></span><br><span class="line"><span class="comment"># bias_initializer=&#x27;zeros&#x27;, kernel_regularizer=None, </span></span><br><span class="line"><span class="comment"># bias_regularizer=None, activity_regularizer=None, </span></span><br><span class="line"><span class="comment"># kernel_constraint=None, bias_constraint=None)</span></span><br><span class="line"></span><br><span class="line">classifier = Sequential()</span><br><span class="line"><span class="comment"># Input layer with 5 inputs neurons</span></span><br><span class="line">classifier.add(Dense(<span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>, input_dim = <span class="number">5</span>))</span><br><span class="line"><span class="comment">#Hidden layer</span></span><br><span class="line">classifier.add(Dense(<span class="number">2</span>, activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#output layer with 1 output neuron which will predict 1 or 0</span></span><br><span class="line"><span class="comment"># this is why we use sigmoid</span></span><br><span class="line">classifier.add(Dense(<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">classifier.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;adam&#x27;</span>, \</span><br><span class="line">    loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">classifier.fit(X_train, Y_train, batch_size = <span class="number">10</span>, epochs = <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#getting predictions of test data</span></span><br><span class="line">predicted_y_cont = classifier.predict(X_test).tolist()</span><br><span class="line">predicted_y = [<span class="built_in">round</span>(x[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> predicted_y_cont]</span><br><span class="line"><span class="comment"># print(predicted_y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Keras classifier \n&quot;</span>, classification_report(Y_test, predicted_y,zero_division=<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这段代码是一个完整的机器学习项目示例，展示了如何使用不同的分类算法对数据进行训练和测试，以解决泰坦尼克号乘客生存预测问题。</p>
<p><strong>注意</strong></p>
<p>上述内容我已经下载到传到了Github上面。</p>
<h2 id="集合、函数、求和"><a href="#集合、函数、求和" class="headerlink" title="集合、函数、求和"></a>集合、函数、求和</h2><p>当涉及到集合、函数和求和时，我们可以通过一个具体的例子来解释它们的概念和关系。</p>
<p>假设我们有一个集合 $A$，包含了一些整数：</p>
<p>$$<br>A &#x3D; {1, 2, 3, 4, 5}<br>$$</p>
<p>现在，我们定义一个函数 $f$，这个函数将集合 $A$ 中的每个元素映射到它的平方。也就是说，如果 $x$ 是集合 $A$ 中的一个元素，则 $f(x)$ 是 $x$ 的平方。我们可以写成如下的函数表达式：</p>
<p>$$<br>f(x) &#x3D; x^2<br>$$</p>
<p>现在，我们来计算函数 $f$ 应用在集合 $A$ 中每个元素上的结果。</p>
<ul>
<li><p>对于 $x &#x3D; 1$，我们有 $f(1) &#x3D; 1^2 &#x3D; 1$。</p>
</li>
<li><p>对于 $x &#x3D; 2$，我们有 $f(2) &#x3D; 2^2 &#x3D; 4$。</p>
</li>
<li><p>对于 $x &#x3D; 3$，我们有 $f(3) &#x3D; 3^2 &#x3D; 9$。</p>
</li>
<li><p>对于 $x &#x3D; 4$，我们有 $f(4) &#x3D; 4^2 &#x3D; 16$。</p>
</li>
<li><p>对于 $x &#x3D; 5$，我们有 $f(5) &#x3D; 5^2 &#x3D; 25$。</p>
</li>
</ul>
<p>现在，我们可以将这些结果加起来得到一个总和。这就是求和的过程。我们将函数 $f$ 应用到集合 $A$ 中的每个元素上，然后将这些结果相加。</p>
<p>$$<br>\text{总和} &#x3D; f(1) + f(2) + f(3) + f(4) + f(5) &#x3D; 1 + 4 + 9 + 16 + 25 &#x3D; 55<br>$$</p>
<p>所以，这里的总和是 $55$。</p>
<p>在这个例子中，我们看到了集合 $A$，函数 $f$，以及将函数应用于集合元素并对结果求和的过程。这展示了集合、函数和求和之间的关系。</p>
<p>求和符号通常用来表示对一组数值进行求和的过程。在数学中，求和符号通常用希腊字母 sigma（Σ）来表示。具体而言，如果我们有一个序列或集合 $a_1, a_2, …, a_n$，求和符号的使用方式如下：<br>$$<br>\sum_{i&#x3D;1}^{n} a_i<br>$$</p>
<p>这个符号的意思是对 $i$ 从 $1$ 到 $n$ 的每个 $a_i$ 求和。换句话说，它表示对序列中的每个元素求和。</p>
<p>现在让我们通过一个具体的例子来解释：</p>
<p>假设我们有一个序列 $1, 2, 3, 4, 5$，我们想要对这个序列中的所有数进行求和。那么我们可以写成：<br>$$<br>\sum_{i&#x3D;1}^{5} i<br>$$</p>
<p>这里 $i$ 是从 $1$ 到 $5$ 的整数。因此，这个求和符号表示对 $i$ 从 $1$ 到 $5$ 的每个 $i$ 求和。</p>
<p>计算过程如下：</p>
<p>$$<br>\sum_{i&#x3D;1}^{5} i &#x3D; 1 + 2 + 3 + 4 + 5 &#x3D; 15<br>$$</p>
<p>因此，序列 $1, 2, 3, 4, 5$ 的总和是 $15$。</p>
<p>现在，让我们看一个具有多个求和符号的示例。假设我们有一个二维数组 $a_{ij}$，其中 $i$ 表示行，$j$ 表示列。我们想要计算所有元素的总和。</p>
<p>$$<br>\sum_{i&#x3D;1}^{m} \sum_{j&#x3D;1}^{n} a_{ij}<br>$$</p>
<p>这个符号的意思是对所有 $i$ 从 $1$ 到 $m$，以及对所有 $j$ 从 $1$ 到 $n$ 的 $a_{ij}$ 求和。</p>
<p>举例来说，如果我们有一个 $2 \times 2$ 的矩阵：</p>
<p>$$<br>\begin{pmatrix}<br>1 &amp; 2 \<br>3 &amp; 4 \<br>\end{pmatrix}<br>$$</p>
<p>那么总和可以计算为：</p>
<p>$$<br>\sum_{i&#x3D;1}^{2} \sum_{j&#x3D;1}^{2} a_{ij} &#x3D; (1 + 2) + (3 + 4) &#x3D; 10<br>$$</p>
<p>这里，我们首先对每一行的元素进行求和，然后再对行求和的结果进行总和。</p>
<h2 id="描述性统计知识点"><a href="#描述性统计知识点" class="headerlink" title="描述性统计知识点"></a>描述性统计知识点</h2><p>当涉及描述性统计时，”平均数”，”中位数”和”众数”是常用的统计指标，用于了解数据集的集中趋势。以下是它们的定义和用途：</p>
<ol>
<li><p><strong>平均数（算术平均数）</strong>：</p>
<ul>
<li>定义：平均数是一组数据之和除以数据的个数。它是最常用的描述数据集中心位置的指标。</li>
<li>公式：对于数据集 $x_1, x_2, …, x_n$，平均数 $\bar{x}$ 计算公式为：$\bar{x} &#x3D; \frac{x_1 + x_2 + \ldots + x_n}{n}$</li>
<li>用途：平均数可以提供数据集的集中趋势，但在数据存在极端值（异常值）时，可能会受到影响。</li>
</ul>
</li>
<li><p><strong>中位数</strong>：</p>
<ul>
<li>定义：中位数是将数据集按顺序排列后位于中间位置的数值。如果数据集包含偶数个数据，则中位数是中间两个数的平均值。</li>
<li>计算方法：将数据集从小到大排列，中间位置的数即为中位数。</li>
<li>用途：中位数不受极端值的影响，更能反映数据集的典型值。</li>
</ul>
</li>
<li><p><strong>众数</strong>：</p>
<ul>
<li>定义：众数是数据集中出现频率最高的数值，即数据集中的模式值。</li>
<li>用途：众数可以帮助识别数据集中最常见的值，尤其适用于分类数据。</li>
</ul>
</li>
</ol>
<p>在实际应用中，这三种指标通常会一起使用，以便全面了解数据的分布情况和集中趋势。根据数据的特点和分布，选择合适的指标进行分析和解释。</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>平均数</th>
<th>中位数</th>
<th>众数</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>数据的总和除以个数</td>
<td>排序后位于中间的值</td>
<td>出现频率最高的值</td>
</tr>
<tr>
<td>优点</td>
<td>- 能够反映数据的集中趋势；<br>- 使用范围广泛。</td>
<td>- 不受极端值影响；<br>- 对偏态数据有良好的表示。</td>
<td>- 简单易懂；<br>- 对分类数据适用。</td>
</tr>
<tr>
<td>缺点</td>
<td>- 受极端值影响；<br>- 不能反映数据的分布情况。</td>
<td>- 不利于计算；<br>- 无法反映数据的具体取值。</td>
<td>- 可能存在多个众数；<br>- 对连续型数据不够精确。</td>
</tr>
<tr>
<td>使用的场合</td>
<td>- 数据分布近似对称；<br>- 数据没有极端值。</td>
<td>- 数据存在极端值；<br>- 数据分布不对称。</td>
<td>- 对分类数据分析；<br>- 识别数据集中的模式。</td>
</tr>
</tbody></table>
<p>下面是关于变异程度的常见统计指标的具体讲解和例子：</p>
<ol>
<li><p><strong>标准差</strong>：</p>
<ul>
<li>定义：标准差是一组数据与其平均数之间差异的平方的平均值的平方根。它衡量了数据集的离散程度。</li>
<li>计算方法：标准差的计算公式为数据与平均数的差的平方和的平均值的平方根。<br>$\text{标准差} &#x3D; \sqrt{\frac{\sum_{i&#x3D;1}^{n}(x_i - \bar{x})^2}{n - 1}}$</li>
<li>例子：假设有一组数据集 {3, 6, 9, 12, 15}，平均数 $\bar{x} &#x3D; \frac{3 + 6 + 9 + 12 + 15}{5} &#x3D; 9$。计算标准差，即$\sqrt{\frac{(3-9)^2 + (6-9)^2 + (9-9)^2 + (12-9)^2 + (15-9)^2}{5}}$，结果为标准差为 4.24。</li>
</ul>
</li>
<li><p><strong>方差</strong>：</p>
<ul>
<li>定义：方差是一组数据与其平均数之间差异的平方的平均值，是标准差的平方。</li>
<li>计算方法：方差的计算公式与标准差类似，只是不取平方根。<br>$方差 &#x3D; \frac{\sum_{i&#x3D;1}^{n}(x_i - \bar{x})^2}{n}$</li>
<li>例子：在上面的例子中，标准差的平方即为方差，所以方差为 $4.24^2 &#x3D; 18$。</li>
</ul>
</li>
<li><p><strong>范围</strong>：</p>
<ul>
<li>定义：范围是数据集中最大值与最小值之间的差异，用来衡量数据的波动范围。</li>
<li>计算方法：最大值减去最小值即可得到范围。</li>
<li>例子：对于数据集 {3, 6, 9, 12, 15}，范围为 ( 15 - 3 &#x3D; 12 )。</li>
</ul>
</li>
<li><p><strong>四分位数和四分位差</strong>：</p>
<ul>
<li>定义：四分位数将数据集分成四等份，分别是最小值、下四分位数、中位数、上四分位数和最大值。四分位差是上四分位数和下四分位数之间的差异。</li>
<li>计算方法：四分位数可通过将数据集按大小排序后找到对应位置的值得到。四分位差即上四分位数减去下四分位数。</li>
<li>例子：对于数据集 {3, 6, 9, 12, 15}，上四分位数为 9，下四分位数为 6，四分位差为 ( 9 - 6 &#x3D; 3 )。</li>
</ul>
</li>
<li><p><strong>百分位数</strong>：</p>
<ul>
<li>定义：百分位数是数据集中所有观测值中对应百分比的值。例如，中位数就是50th百分位数。</li>
<li>计算方法：将数据按大小排序后，找到相应百分比位置的值即可。</li>
<li>例子：中位数是50th百分位数。对于数据集 {3, 6, 9, 12, 15}，中位数是 9。</li>
</ul>
</li>
</ol>
<p>这些指标提供了关于数据集变异程度的不同方面的信息，有助于更全面地了解数据的分布和离散程度。</p>
<p>为什么方差要n-1：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102043269">https://zhuanlan.zhihu.com/p/102043269</a></p>
<p>数据分布是描述数据集中数据分布情况的统计概念，正态分布参数、偏度和峰度是常用于描述数据分布形态的指标。</p>
<ol>
<li><p><strong>正态分布参数（均值、标准差）</strong>：</p>
<ul>
<li>正态分布是统计学中最常见的分布形式，也称为钟形曲线。其特点是具有对称的形态，均值和标准差决定了分布的中心位置和数据的离散程度。</li>
<li>均值：正态分布的均值代表数据集的中心位置，位于分布的中心点。</li>
<li>标准差：正态分布的标准差决定了数据点相对于均值的分布集中程度，标准差越大，数据分布越分散。</li>
</ul>
</li>
<li><p><strong>偏度（Skewness）</strong>：</p>
<ul>
<li>偏度是描述数据分布偏斜程度的统计量。当数据集的分布向左偏或向右偏时，偏度值会偏离零。</li>
<li>正偏态（右偏）数据分布的尾部向右延伸，均值位于中位数右侧。</li>
<li>负偏态（左偏）数据分布的尾部向左延伸，均值位于中位数左侧。</li>
<li>偏度为0表示数据分布对称。</li>
</ul>
</li>
<li><p><strong>峰度（Kurtosis）</strong>：</p>
<ul>
<li>峰度用于描述数据集中数据分布形态的尖峰程度。它与正态分布进行比较。</li>
<li>正态分布的峰度为3，称为Mesokurtic，表示与正态分布的峰度相同。</li>
<li>高峰度（Leptokurtic）数据分布具有比正态分布更尖锐的峰度，峰度大于3。</li>
<li>低峰度（Platykurtic）数据分布具有比正态分布更平坦的峰度，峰度小于3。</li>
</ul>
</li>
</ol>
<p>这些统计指标帮助我们了解数据的分布情况、偏斜程度以及尖峰程度，从而更好地进行数据分析和解释。在应用中，可以使用这些指标来评估数据集的形态并做出相应的推断。</p>
<p>频率分布是描述数据集中各个数值或类别出现次数的统计概念。在频率分布中，常见的指标包括频数、相对频率和累积频率。</p>
<ol>
<li><p><strong>频数</strong>：</p>
<ul>
<li>频数是指某个数值或类别在数据集中出现的次数。它表示了数据集中各个数值或类别的出现情况。</li>
<li>例如，对于一个骰子投掷的结果，1至6的频数分别表示投掷到1、2、3、4、5、6的次数。</li>
</ul>
</li>
<li><p><strong>相对频率</strong>：</p>
<ul>
<li>相对频率是指某个数值或类别在数据集中出现的频数与总样本量之比。它表示了每个数值或类别在整体中的占比情况。</li>
<li>相对频率可以通过将频数除以样本量得到。</li>
<li>例如，如果一个数值的频数为10，总样本量为100，则该数值的相对频率为10%。</li>
</ul>
</li>
<li><p><strong>累积频率</strong>：</p>
<ul>
<li>累积频率是指累计到某个数值或类别的频数之和。它表示了数据集中小于或等于某个数值或类别的所有数值的总频数。</li>
<li>例如，如果数据集中小于或等于3的频数为20，小于或等于4的频数为40，则小于或等于4的累积频率为40。</li>
</ul>
</li>
</ol>
<p>频率分布可以通过这些指标来分析数据集中各个数值或类别的分布情况，以及它们在整体中的相对重要性。通过频率分布，可以更好地理解数据集的特征和结构，从而进行更深入的数据分析和解释。</p>
<p>集中度与离散度是描述数据分布或变异程度的统计概念。常见的指标包括变异系数、相对离散度和相对标准差。</p>
<ol>
<li><p><strong>变异系数</strong>：</p>
<ul>
<li>变异系数是数据标准差与数据平均值的比值，通常用来比较不同数据集之间的离散程度，尤其是当数据集的均值大小差异较大时。</li>
<li>公式：变异系数（CV）等于标准差（SD）除以均值（Mean），再乘以100%。</li>
<li>通常以百分比的形式表示，可以用来比较不同单位或量级的数据集的离散程度。</li>
</ul>
</li>
<li><p><strong>相对离散度</strong>：</p>
<ul>
<li>相对离散度是指标准差与均值的比值，用来衡量数据集中的离散程度。</li>
<li>公式：相对离散度等于标准差（SD）除以均值（Mean）。</li>
<li>相对离散度的值越大，表示数据集中的波动程度越大，相对离散度的值越小，表示数据集中的波动程度越小。</li>
</ul>
</li>
<li><p><strong>相对标准差</strong>：</p>
<ul>
<li>相对标准差是标准差与均值的比值，类似于相对离散度。</li>
<li>公式：相对标准差等于标准差（SD）除以均值（Mean）。</li>
<li>相对标准差的计算方式与相对离散度相同，但是它不以百分比形式表示。</li>
</ul>
</li>
</ol>
<p>这些指标帮助我们了解数据集的离散程度和集中趋势，以及不同数据集之间的比较。通过这些指标，可以更好地理解数据集的特征和结构，从而进行更深入的数据分析和解释。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个示例数据集</span></span><br><span class="line">data = np.array([<span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的均值和标准差</span></span><br><span class="line">mean_value = np.mean(data)</span><br><span class="line">std_deviation = np.std(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算变异系数</span></span><br><span class="line">coefficient_of_variation = (std_deviation / mean_value) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相对离散度</span></span><br><span class="line">relative_dispersion = std_deviation / mean_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相对标准差</span></span><br><span class="line">relative_standard_deviation = std_deviation / mean_value</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集的均值：&quot;</span>, mean_value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集的标准差：&quot;</span>, std_deviation)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;变异系数：&quot;</span>, coefficient_of_variation)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相对离散度：&quot;</span>, relative_dispersion)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相对标准差：&quot;</span>, relative_standard_deviation)</span><br></pre></td></tr></table></figure>

<p>计算结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据集的均值： <span class="number">25.0</span></span><br><span class="line">数据集的标准差： <span class="number">7.0710678118654755</span></span><br><span class="line">变异系数： <span class="number">28.284271247461902</span></span><br><span class="line">相对离散度： <span class="number">0.282842712474619</span></span><br><span class="line">相对标准差： <span class="number">0.282842712474619</span></span><br></pre></td></tr></table></figure>

<p>如何使用变异系数来比较两个数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个示例数据集</span></span><br><span class="line">data1 = np.array([<span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>])</span><br><span class="line">data2 = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的均值和标准差</span></span><br><span class="line">mean_value1 = np.mean(data1)</span><br><span class="line">mean_value2 = np.mean(data2)</span><br><span class="line">std_deviation1 = np.std(data1)</span><br><span class="line">std_deviation2 = np.std(data2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算数据集的变异系数</span></span><br><span class="line">coefficient_of_variation1 = (std_deviation1 / mean_value1) * <span class="number">100</span></span><br><span class="line">coefficient_of_variation2 = (std_deviation2 / mean_value2) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集1的变异系数：&quot;</span>, coefficient_of_variation1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集2的变异系数：&quot;</span>, coefficient_of_variation2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个数据集的变异系数</span></span><br><span class="line"><span class="keyword">if</span> coefficient_of_variation1 &lt; coefficient_of_variation2:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据集1的变异程度较小&quot;</span>)</span><br><span class="line"><span class="keyword">elif</span> coefficient_of_variation1 &gt; coefficient_of_variation2:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;数据集2的变异程度较小&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;两个数据集的变异程度相等&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>得到的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据集<span class="number">1</span>的变异系数： <span class="number">28.284271247461902</span></span><br><span class="line">数据集<span class="number">2</span>的变异系数： <span class="number">47.14045207910317</span></span><br><span class="line">数据集<span class="number">1</span>的变异程度较小</span><br></pre></td></tr></table></figure>

<p>当说数据集1的变异程度较小时，意味着数据集1中的数据点相对于其平均值的分布更为集中，波动性较小。换句话说，数据集1中的数据点更趋向于围绕着其均值附近波动，而不是偏离均值太远。</p>
<p>这对于数据分析和解释来说是有意义的，因为一个变异程度较小的数据集通常意味着数据点之间的差异性较小，数据集的稳定性较高。这样的数据集更容易预测和理解，因为数据点的波动不会太大，更接近数据集的中心趋势。</p>
<p>以下是直方图、箱线图、散点图、折线图和饼图的对比表格：</p>
<table>
<thead>
<tr>
<th>图形类型</th>
<th>特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>直方图</td>
<td>- 显示数据分布的频率或频数；<br>- x轴表示数据范围，y轴表示频数或频率。</td>
<td>- 描述数据分布的形态；<br>- 比较不同组之间的差异。</td>
</tr>
<tr>
<td>箱线图</td>
<td>- 显示数据的分散情况和离群值；<br>- 展示了数据的中位数、四分位数和异常值。</td>
<td>- 比较数据集的分布；<br>- 发现异常值和离群点。</td>
</tr>
<tr>
<td>散点图</td>
<td>- 显示两个变量之间的关系；<br>- 每个数据点表示为平面上的一个点。</td>
<td>- 描述变量之间的相关性；<br>- 观察数据的分布规律。</td>
</tr>
<tr>
<td>折线图</td>
<td>- 通过连线表示数据随时间或有序类别的变化趋势；<br>- x轴表示时间或类别，y轴表示数值。</td>
<td>- 描述数据随时间的变化；<br>- 比较不同时间点的趋势。</td>
</tr>
<tr>
<td>饼图</td>
<td>- 用圆形将数据分成各个部分；<br>- 表示各个部分占总体的比例。</td>
<td>- 展示数据组成的比例；<br>- 显示分类数据的比例关系。</td>
</tr>
</tbody></table>
<p>这些图形类型各有特点，适用于不同类型的数据分析和展示。根据数据的特点和分析目的，可以选择合适的图形类型来呈现数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 生成示例数据</span><br><span class="line">x = np.linspace(0, 10, 100)</span><br><span class="line">y1 = np.random.normal(0, 1, 100)</span><br><span class="line">y2 = np.random.normal(2, 1, 100)</span><br><span class="line">y3 = np.random.normal(5, 2, 100)</span><br><span class="line"></span><br><span class="line"># 创建画布和子图</span><br><span class="line">fig, axs = plt.subplots(2, 2, figsize=(12, 8))</span><br><span class="line"></span><br><span class="line"># 绘制直方图</span><br><span class="line">axs[0, 0].hist(y1, bins=20, color=&#x27;blue&#x27;, alpha=0.7, label=&#x27;Histogram&#x27;)</span><br><span class="line">axs[0, 0].set_title(&#x27;Histogram&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制箱线图</span><br><span class="line">axs[0, 1].boxplot([y1, y2, y3], labels=[&#x27;Dataset 1&#x27;, &#x27;Dataset 2&#x27;, &#x27;Dataset 3&#x27;])</span><br><span class="line">axs[0, 1].set_title(&#x27;Boxplot&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制散点图</span><br><span class="line">axs[1, 0].scatter(x, y1, color=&#x27;red&#x27;, label=&#x27;Scatter&#x27;)</span><br><span class="line">axs[1, 0].set_title(&#x27;Scatter Plot&#x27;)</span><br><span class="line"></span><br><span class="line"># 绘制折线图</span><br><span class="line">axs[1, 1].plot(x, y1, color=&#x27;green&#x27;, label=&#x27;Line&#x27;)</span><br><span class="line">axs[1, 1].set_title(&#x27;Line Plot&#x27;)</span><br><span class="line"></span><br><span class="line"># 添加图例</span><br><span class="line">axs[0, 0].legend()</span><br><span class="line">axs[1, 0].legend()</span><br><span class="line">axs[1, 1].legend()</span><br><span class="line"></span><br><span class="line"># 调整布局</span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line"># 显示图形</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303192849562.png" alt="image-20240303192849562"></p>
<h2 id="朴素贝叶斯与分类器"><a href="#朴素贝叶斯与分类器" class="headerlink" title="朴素贝叶斯与分类器"></a>朴素贝叶斯与分类器</h2><p><strong>贝叶斯定理</strong></p>
<p>贝叶斯定理是概率论中的一个基本定理，它描述了在已知先验信息的情况下，如何通过新的观察数据来更新我们对事件的概率估计。贝叶斯定理在统计学、机器学习和人工智能等领域中都有广泛的应用。</p>
<p>贝叶斯定理的数学表达如下：</p>
<p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<p>其中：</p>
<ul>
<li>$P(A|B)$ 表示在给定B发生的条件下A发生的概率，也称为后验概率。</li>
<li>$P(B|A)$ 表示在给定A发生的条件下B发生的概率，也称为似然度。</li>
<li>$P(A)$ 和 $P(B)$ 分别是A和B的边缘概率，也称为先验概率和边缘似然度。</li>
</ul>
<p>拓展视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1R7411a76r/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1R7411a76r/?spm_id_from=333.337.search-card.all.click</a></p>
<p>下面是一个简单的例子，展示了如何使用贝叶斯定理进行推断：</p>
<p>假设某地区有一种罕见疾病，该疾病的患病率很低，只有1%的人口患病。现在，我们有一种测试方法，该测试在患病人群中能够正确诊断出99%的患者，但在健康人群中也有1%的误诊率。</p>
<p>现在有一个人进行了测试，测试结果为阳性。我们想知道这个人实际上患病的概率是多少。</p>
<p>我们用事件A表示这个人患病，事件B表示测试结果为阳性。</p>
<p>根据题设可知：</p>
<ul>
<li>$P(A) &#x3D; 0.01$（先验概率，即患病率）</li>
<li>$P(B|A) &#x3D; 0.99$ （在患病的情况下，测试结果为阳性的概率）</li>
<li>$P(B|\neg A) &#x3D; 0.01$ （在健康的情况下，测试结果为阳性的概率）</li>
</ul>
<p>我们需要计算后验概率$P(A|B)$，即在测试结果为阳性的情况下，这个人实际上患病的概率。</p>
<p>根据贝叶斯定理：</p>
<p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<p>其中，$P(B) &#x3D; P(B|A)P(A) + P(B|\neg A)P(\neg A)$</p>
<p>$P(B) &#x3D; 0.99 \times 0.01 + 0.01 \times 0.99 &#x3D; 0.0099 + 0.0099 &#x3D; 0.0198$</p>
<p>因此</p>
<p>$P(A|B) &#x3D; \frac{0.99 \times 0.01}{0.0198} \approx \frac{0.0099}{0.0198} \approx 0.5$</p>
<p>即在测试结果为阳性的情况下，这个人实际上患病的概率约为50%。这个例子展示了贝叶斯定理在推断中的应用。</p>
<p><strong>朴素贝叶斯与朴素贝叶斯分类器</strong></p>
<p>朴素贝叶斯（Naive Bayes）是基于贝叶斯定理和特征条件独立假设的一种简单且高效的分类算法。朴素贝叶斯分类器是基于朴素贝叶斯算法构建的分类模型。</p>
<p>朴素贝叶斯算法的关键假设是，给定类别，每个特征之间是条件独立的，也就是说，特征之间的存在概率不受其他特征的影响。尽管这个假设在实际情况中很少成立，但在实践中，朴素贝叶斯仍然表现出令人满意的性能，特别是在文本分类和垃圾邮件过滤等任务中。</p>
<p>朴素贝叶斯分类器的工作原理如下：</p>
<ol>
<li><p><strong>训练阶段</strong>：通过已知分类标签的数据集，计算每个特征在每个类别下的概率。</p>
</li>
<li><p><strong>预测阶段</strong>：对于新的数据点，计算在每个类别下的后验概率，然后选择具有最高后验概率的类别作为预测结果。</p>
</li>
</ol>
<p>在文本分类任务中，朴素贝叶斯常常被使用。例如，给定一些文档，我们想要将它们归类到不同的类别中，比如“体育”、“政治”、“科技”等等。我们可以将每个文档表示为特征向量，其中特征是词汇表中的词，特征的值可以是词出现的频率或者TF-IDF等。然后，利用朴素贝叶斯分类器来对这些特征向量进行分类。</p>
<p>下面是一个简单的朴素贝叶斯分类器的示例：</p>
<p>假设我们有一个数据集，包含两个类别（”是”和”否”），每个样本有两个特征（特征1和特征2）。</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>特征1</th>
<th>特征2</th>
<th>类别</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>高</td>
<td>高</td>
<td>是</td>
</tr>
<tr>
<td>2</td>
<td>高</td>
<td>低</td>
<td>是</td>
</tr>
<tr>
<td>3</td>
<td>低</td>
<td>高</td>
<td>否</td>
</tr>
<tr>
<td>4</td>
<td>低</td>
<td>低</td>
<td>否</td>
</tr>
</tbody></table>
<p>我们想要对一个新样本（特征1&#x3D;高，特征2&#x3D;低）进行分类。</p>
<p>首先，计算类别的先验概率：</p>
<ul>
<li>$P(\text{“是”}) &#x3D; \frac{2}{4} &#x3D; 0.5$</li>
<li>$P(\text{“否”}) &#x3D; \frac{2}{4} &#x3D; 0.5$</li>
</ul>
<p>然后，计算条件概率：</p>
<ul>
<li>$P(\text{“高”}|\text{“是”}) &#x3D; \frac{2}{2} &#x3D; 1$</li>
<li>$P(\text{“低”}|\text{“是”}) &#x3D; \frac{0}{2} &#x3D; 0$</li>
<li>$P(\text{“高”}|\text{“否”}) &#x3D; \frac{1}{2} &#x3D; 0.5$</li>
<li>$P(\text{“低”}|\text{“否”}) &#x3D; \frac{1}{2} &#x3D; 0.5$</li>
</ul>
<p>计算后验概率：</p>
<p>对于新样本（特征1&#x3D;高，特征2&#x3D;低），我们计算在每个类别下的后验概率：</p>
<ul>
<li>对于类别”是”： $P(\text{“是”}|\text{“高”}, \text{“低”}) &#x3D; P(\text{“高”}|\text{“是”}) \times P(\text{“低”}|\text{“是”}) \times P(\text{“是”}) &#x3D; 1 \times 0 \times 0.5 &#x3D; 0$</li>
<li>对于类别”否”： $P(\text{“否”}|\text{“高”}, \text{“低”}) &#x3D; P(\text{“高”}|\text{“否”}) \times P(\text{“低”}|\text{“否”}) \times P(\text{“否”}) &#x3D; 0.5 \times 0.5 \times 0.5 &#x3D; 0.125$</li>
</ul>
<p>因此，我们预测新样本属于类别”否”。</p>
<p>Python语言代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NaiveBayesClassifier</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.classes = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        self.class_feature_counts = defaultdict(<span class="keyword">lambda</span>: defaultdict(<span class="built_in">int</span>))</span><br><span class="line">        self.class_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        self.total_samples = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="keyword">for</span> features, label <span class="keyword">in</span> <span class="built_in">zip</span>(X, y):</span><br><span class="line">            self.classes[label] += <span class="number">1</span></span><br><span class="line">            self.class_counts[label] += <span class="built_in">sum</span>(features)</span><br><span class="line">            <span class="keyword">for</span> idx, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">                self.class_feature_counts[label][idx] += feature</span><br><span class="line">            self.total_samples += <span class="built_in">sum</span>(features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        predictions = []</span><br><span class="line">        <span class="keyword">for</span> features <span class="keyword">in</span> X:</span><br><span class="line">            max_prob, max_class = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>), <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> label <span class="keyword">in</span> self.classes:</span><br><span class="line">                log_prob = <span class="number">0</span></span><br><span class="line">                class_count = self.classes[label]</span><br><span class="line">                class_total = self.class_counts[label]</span><br><span class="line">                <span class="keyword">for</span> idx, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">                    <span class="comment"># Laplace smoothing for unseen features</span></span><br><span class="line">                    feature_count = self.class_feature_counts[label][idx] + <span class="number">1</span></span><br><span class="line">                    log_prob += feature * (class_count / class_total)</span><br><span class="line">                <span class="keyword">if</span> log_prob &gt; max_prob:</span><br><span class="line">                    max_prob, max_class = log_prob, label</span><br><span class="line">            predictions.append(max_class)</span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X_train = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">]</span><br><span class="line">y_train = [<span class="string">&quot;yes&quot;</span>, <span class="string">&quot;yes&quot;</span>, <span class="string">&quot;no&quot;</span>, <span class="string">&quot;no&quot;</span>]</span><br><span class="line"></span><br><span class="line">X_test = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并训练朴素贝叶斯分类器</span></span><br><span class="line">clf = NaiveBayesClassifier()</span><br><span class="line">clf.train(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">predictions = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predictions:&quot;</span>, predictions)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Predictions: [<span class="string">&#x27;no&#x27;</span>, <span class="string">&#x27;no&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h1 id="第一周知识点"><a href="#第一周知识点" class="headerlink" title="第一周知识点"></a>第一周知识点</h1><h2 id="本周任务"><a href="#本周任务" class="headerlink" title="本周任务"></a>本周任务</h2><p>到本周末，将能够：</p>
<ul>
<li>解释基本语言术语及其之间的关系</li>
<li>解释如何创建词向量模型 </li>
<li>讨论词向量的可能应用 </li>
<li>使用预先训练的词向量来查找语义相关的词。</li>
</ul>
<p>下面列出了需要完成的任务。</p>
<ol>
<li><strong>完成<a target="_blank" rel="noopener" href="https://myuni.adelaide.edu.au/courses/95210/assignments/382141">介绍性测验</a></strong></li>
<li>在参加讲座之前，请查看本模块中的在线学习内容并按照指示完成活动。</li>
<li>阅读评估要求。</li>
<li>将的任何问题发布到 Piazza 讨论区。</li>
<li>参加互动讲座。</li>
<li>完成每周测验</li>
</ol>
<p>测试结果：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303154755596.png" alt="image-20240303154755596"></p>
<p>要理解本模块中的概念，需要应用以下主题的知识。</p>
<p><strong>机器学习：</strong></p>
<ul>
<li><input disabled type="checkbox"> 分类：分类是一种机器学习任务，旨在将数据实例划分为预定义的类别或标签。分类算法根据已知的特征将数据分配到离散的类别中。</li>
<li><input disabled type="checkbox"> 回归（逻辑回归）：回归是一种机器学习任务，旨在预测连续型变量的输出。逻辑回归是一种用于二分类问题的回归算法，通过将输入特征的线性组合传递给一个逻辑函数来预测输出的概率。</li>
<li><input disabled type="checkbox"> 聚类：聚类是一种无监督学习任务，旨在将数据集中的数据分成不同的组（或簇），使得同一组内的数据之间的相似度高于不同组之间的相似度。</li>
<li><input disabled type="checkbox"> 监督机器学习：监督机器学习是一种机器学习范式，其中模型从带有标签的训练数据中学习，并尝试预测未标记数据的标签。监督学习的目标是根据输入特征预测输出标签或值。</li>
<li><input disabled type="checkbox"> 无监督机器学习：无监督机器学习是一种机器学习范式，其中模型从不带标签的数据中学习，并试图发现数据中的结构或模式。无监督学习的目标是发现数据中的隐藏关系或结构，而不是预测标签或值。</li>
<li><input disabled type="checkbox"> 成本函数：成本函数是用来衡量机器学习模型预测与实际目标之间差异的函数。在训练过程中，优化算法尝试最小化成本函数，以使模型的预测结果与真实值尽可能接近。</li>
</ul>
<p><strong>机器学习方法论：</strong></p>
<ul>
<li><input disabled type="checkbox"> 训练、验证和测试：训练、验证和测试是机器学习模型开发过程中的三个关键阶段。训练阶段用于训练模型参数，验证阶段用于调整模型超参数和评估模型性能，测试阶段用于评估模型的泛化性能。</li>
<li><input disabled type="checkbox"> 交叉验证：交叉验证是一种评估模型性能和选择模型超参数的技术。它将数据集分成 k 个子集，每次使用其中的一个子集作为验证集，其余的作为训练集，重复 k 次后取平均值作为最终性能评估指标。</li>
<li><input disabled type="checkbox"> 精确率、召回率和 F1 指标：精确率衡量的是模型预测为正类别的样本中实际为正类别的比例，召回率衡量的是实际为正类别的样本中被模型正确预测为正类别的比例，F1 指标是精确率和召回率的调和平均数，综合评估了模型的预测性能。</li>
<li><input disabled type="checkbox"> 模型欠拟合和过拟合：模型欠拟合指模型对训练数据和测试数据都表现不佳，未能捕获数据中的趋势和模式；模型过拟合指模型在训练数据上表现很好，但在测试数据上表现较差，过度拟合了训练数据中的噪声或特定特征。</li>
<li><input disabled type="checkbox"> 模型评估中的偏差和方差：在模型评估中，偏差是指模型的预测值与真实值之间的差异，反映了模型的拟合能力；方差是指模型在不同数据集上预测结果的变化程度，反映了模型的稳定性。偏差-方差权衡是指在模型选择和优化中需要平衡偏差和方差之间的关系，以获得更好的泛化性能。</li>
</ul>
<p>交叉验证是在机器学习中评估模型性能和选择模型超参数的常用技术。以下是使用交叉验证的情况和原因：</p>
<ol>
<li><strong>评估模型性能</strong>：交叉验证可用于评估模型在未见过的数据上的性能。通过将数据集划分为训练集和验证集，可以多次训练模型并评估其性能，从而得到更稳健和可靠的性能评估。</li>
<li><strong>选择模型超参数</strong>：交叉验证可用于选择模型的超参数。通过尝试不同的超参数组合并使用交叉验证评估每个组合的性能，可以找到最佳的超参数设置，以提高模型的泛化能力。</li>
<li><strong>防止过拟合</strong>：交叉验证可以帮助检测和减轻过拟合问题。通过在不同的数据子集上训练和验证模型，可以更好地了解模型对数据的泛化能力，避免模型过度拟合训练数据。</li>
<li><strong>数据利用率高</strong>：交叉验证充分利用了数据集中的所有样本，尤其在数据量较小的情况下，可以提供更可靠的模型评估结果。</li>
</ol>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>词向量是预先训练的模型，可根据词在向量空间中的距离查找语义相关的词。训练结果是一组向量。每个向量都有 N 个数字，它们是 N 维空间中的坐标。语义相似的向量在这个空间中很接近。</p>
<p>一个演示网站：<a target="_blank" rel="noopener" href="http://vectors.nlpl.eu/explore/embeddings/en/#%E3%80%82">http://vectors.nlpl.eu/explore/embeddings/en/#。</a></p>
<p>另外一个有趣的工具：<a target="_blank" rel="noopener" href="https://ronxin.github.io/wevi/">https://ronxin.github.io/wevi/</a></p>
<p>使用最广泛的 WV 模型，特别注意它们的训练方式：Skip-Gram 和 GloVe。</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>Skip-Gram</th>
<th>GloVe</th>
</tr>
</thead>
<tbody><tr>
<td>训练方式</td>
<td>基于中心词预测上下文词</td>
<td>基于全局词共现矩阵</td>
</tr>
<tr>
<td>模型原理</td>
<td>基于神经网络</td>
<td>基于全局词共现统计信息</td>
</tr>
<tr>
<td>计算效率</td>
<td>训练过程可能较耗时</td>
<td>可能更高效，基于矩阵分解</td>
</tr>
<tr>
<td>数据需求</td>
<td>需要大量数据训练良好的词向量</td>
<td>对全局统计信息依赖较大，但对小数据集也适用</td>
</tr>
<tr>
<td>适用场景</td>
<td>大型语料库，复杂语义表示</td>
<td>语料库规模不大，需求较简单的语义表示</td>
</tr>
</tbody></table>
<p>词向量是词的表示。每个单词都是 N 维向量空间中的一个点。最重要的是，意义相近的单词彼此之间的距离也很近。当我们想要找到相似的单词时，单词向量的这个属性非常有用，例如与未知单词相似的单词，我们将在下一个模块中探索这些单词。</p>
<p>关于词向量：</p>
<blockquote>
<p>词向量是一种将单词映射到高维向量空间中的表示方法。在这个向量空间中，每个单词都被表示为一个向量，而这些向量的维度通常由预先定义的模型决定，比如100维或300维等。</p>
<p>这种表示方法的核心思想是利用单词在上下文中的分布来确定其向量表示，即假设在语言中，具有相似上下文的单词也会有相似的向量表示。</p>
</blockquote>
<p>举例来说，假设我们有一个简单的语料库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;I love natural language processing&quot;</span><br></pre></td></tr></table></figure>

<p>我们可以将每个单词表示为一个向量，例如：</p>
<ul>
<li>“I” -&gt; [0.2, 0.3, 0.5]</li>
<li>“love” -&gt; [0.1, 0.4, 0.8]</li>
<li>“natural” -&gt; [0.7, 0.9, 0.2]</li>
<li>“language” -&gt; [0.6, 0.3, 0.1]</li>
<li>“processing” -&gt; [0.5, 0.7, 0.6]</li>
</ul>
<p>在这个例子中，每个单词都被表示为一个3维向量。这些向量可以捕捉到单词之间的语义和语法关系，比如”love”和”language”可能在向量空间中更接近，而”natural”和”processing”可能也会有一定的相似性。</p>
<p>通过将单词表示为向量，我们可以在向量空间中进行各种操作，比如计算词语之间的相似度、查找与给定词语最相似的词语等。这种表示方法在自然语言处理中被广泛应用，尤其是在词嵌入（Word Embedding）技术中，如Word2Vec、GloVe等。</p>
<p><strong>如何确定单词对应的向量：</strong></p>
<blockquote>
<p>词向量中每个数字的确定通常是通过训练模型来实现的，其中最常见的模型包括Word2Vec、GloVe、FastText等。这些模型使用了大量的文本数据，并利用神经网络或其他统计方法来学习单词的向量表示。</p>
<p>具体地说，这些模型会考虑单词在上下文中的分布情况，通过优化某种损失函数，将单词的向量表示调整到使得在语料库中频繁共现的单词在向量空间中距离更接近的情况。换句话说，如果两个单词在语料库中经常出现在相似的上下文中，那么它们在向量空间中的表示也应该更加接近。</p>
<p>训练过程中，模型会不断调整单词向量中每个数字的值，直到达到最优的表示。这些值可能没有特定的物理含义，但它们能够捕捉到单词之间的语义和语法关系，从而在许多自然语言处理任务中都能取得良好的效果。</p>
</blockquote>
<p>在完成在线学习部分的阅读和观看视频时，请注意下面列出的概念，因为它们对于成功完成在线测验和作业非常重要。</p>
<p>下面嵌入的视频提供了 NLP 和语言学的有用介绍：<a target="_blank" rel="noopener" href="https://youtu.be/MPOVoIB4EGw">https://youtu.be/MPOVoIB4EGw</a></p>
<p><strong>NPL金字塔的层次通常如下所示（自下而上）：</strong></p>
<ol>
<li><strong>语言学基础（Linguistic Foundations）</strong>：这是金字塔的基础层。它涵盖了语言学的基本原理，如词法学（词汇分析）、句法学（语法分析）、语音学（声音学）等。</li>
<li><strong>词法分析（Morphological Analysis）</strong>：这一层包括词法分析，即将词汇分解成词干和词缀等基本单位。</li>
<li><strong>句法分析（Syntactic Analysis）</strong>：句法分析涉及对句子结构的分析，包括词语之间的语法关系和句子的句法结构。</li>
<li><strong>语义分析（Semantic Analysis）</strong>：在这一层，系统试图理解句子的意义，包括词语之间的语义关系以及句子的整体含义。</li>
<li><strong>语用分析（Pragmatic Analysis）</strong>：语用分析涉及到语言使用的背景和语境，以及言语行为的目的和意图。</li>
</ol>
<p>在NPL金字塔的顶部，还可以包括更高级的任务，如对话系统、情感分析、文本生成等。</p>
<h2 id="工具与算法"><a href="#工具与算法" class="headerlink" title="工具与算法"></a>工具与算法</h2><p>自然语言处理（NLP）领域有许多库和工具可供使用，涵盖了各种不同的任务和技术。以下是一些常用的NLP库和工具，包括但不限于：</p>
<ol>
<li>**NLTK (Natural Language Toolkit)**：NLTK是Python中最常用的NLP库之一，提供了丰富的工具和资源，用于词性标注、句法分析、语义分析等任务。</li>
<li><strong>spaCy</strong>：spaCy是另一个流行的Python NLP库，提供了快速的词法分析和句法分析等功能，支持多种自然语言处理任务。</li>
<li><strong>Gensim</strong>：Gensim是用于文本处理和主题建模的Python库，支持词向量表示、文档相似性计算等功能。</li>
<li><strong>Stanford NLP</strong>：Stanford NLP是斯坦福大学开发的一组NLP工具，包括分词器、词性标注器、句法分析器等，提供了Java和Python的接口。</li>
<li><strong>OpenNLP</strong>：OpenNLP是Apache基金会的一个项目，提供了一系列NLP工具，包括分词器、词性标注器、命名实体识别器等。</li>
<li><strong>CoreNLP</strong>：CoreNLP是斯坦福大学的一个NLP工具包，提供了一系列NLP任务的功能，包括句法分析、语义分析、情感分析等。</li>
<li><strong>Spacy</strong>：Spacy是一个用于自然语言处理任务的Python库，具有高效的句法分析和实体识别功能。</li>
<li><strong>TextBlob</strong>：TextBlob是一个简单易用的Python库，提供了文本处理和情感分析等功能。</li>
<li><strong>Word2Vec</strong>：Word2Vec是Google开发的一个词向量表示工具，用于将词语转换为向量表示，常用于词语相似性计算和文本表示。</li>
<li><strong>FastText</strong>：FastText是Facebook开发的一个词向量表示工具，支持快速训练和使用大规模词向量模型。</li>
</ol>
<p><strong>以下是一些常见的NLP算法：</strong></p>
<ol>
<li><strong>维特比算法（Viterbi Algorithm）</strong>：维特比算法是一种动态规划算法，用于在给定隐马尔可夫模型的情况下，寻找最有可能产生观测序列的隐藏状态序列。这个算法通过在状态空间中动态地计算每个时刻的最佳路径来实现。维特比算法通常用于诸如词性标注、语音识别等序列标注任务中。</li>
<li><strong>BOUND-WELCH算法（Baum-Welch Algorithm）</strong>：BOUND-WELCH算法是一种用于隐马尔可夫模型参数估计的期望最大化（EM）算法的特例。它用于通过观测序列学习隐马尔可夫模型的参数，包括状态转移概率和观测概率。BOUND-WELCH算法通过迭代地调整模型参数，使得观测序列出现的概率最大化。这个算法常用于无监督学习的问题，其中隐藏状态是未知的，但可以通过观测序列进行估计。</li>
<li><strong>词袋模型（Bag of Words，BoW）</strong>：将文本表示为单词的集合，忽略单词的顺序和语法结构，常用于文本分类和情感分析等任务。</li>
<li><strong>TF-IDF（Term Frequency-Inverse Document Frequency）</strong>：用于衡量单词在文档中的重要性，通过词频和逆文档频率来计算单词的权重，常用于文本分类和信息检索等任务。</li>
<li><strong>词嵌入（Word Embedding）</strong>：将单词映射到低维向量空间中，以捕捉单词之间的语义关系，常用的词嵌入模型包括Word2Vec、GloVe、FastText等。</li>
<li><strong>序列标注算法（Sequence Labeling）</strong>：用于给定输入序列中的每个单词标注一个标签，常见的序列标注任务包括词性标注、命名实体识别等。</li>
<li><strong>机器翻译算法（Machine Translation）</strong>：将一种自然语言翻译成另一种自然语言的算法，常用的机器翻译模型包括统计机器翻译（SMT）和神经机器翻译（NMT）等。</li>
<li><strong>情感分析算法（Sentiment Analysis）</strong>：用于分析文本中的情感倾向，常见的情感分析技术包括基于词典的方法、机器学习方法和深度学习方法等。</li>
<li><strong>命名实体识别算法（Named Entity Recognition，NER）</strong>：用于识别文本中具有特定意义的命名实体，如人名、地名、组织机构名等。</li>
<li><strong>文本生成算法（Text Generation）</strong>：用于生成符合语法和语义规则的文本，常见的文本生成技术包括基于规则的方法、统计语言模型和深度学习模型等。</li>
<li><strong>关键词提取算法（Keyword Extraction）</strong>：从文本中自动提取关键词或关键短语，常用于文本摘要、信息检索等应用。</li>
<li><strong>情境理解（Discourse Analysis）</strong>：用于理解文本中句子之间的逻辑关系和语义关联，常见的技术包括共指消解、指代消解等。</li>
</ol>
<p>这些算法和技术在NLP领域的不同任务和应用中起着重要作用，可以根据具体的需求和场景选择合适的算法来解决问题。</p>
<h2 id="一些具体的例子"><a href="#一些具体的例子" class="headerlink" title="一些具体的例子"></a>一些具体的例子</h2><p><strong>一个语言学+深度学习的例子：</strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303162518422.png" alt="image-20240303162518422"></p>
<p><strong>一个依存树的例子：</strong></p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303162702975.png" alt="image-20240303162702975"></p>
<p>更加详细的笔记查看：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/mantch/p/12327735.html">https://www.cnblogs.com/mantch/p/12327735.html</a></p>
<p><strong>另外一个是Constituency trees（构成树）。</strong></p>
<p>Constituency trees（构成树）是自然语言处理中一种常用的句法结构表示方法，用于描述句子的成分结构。构成树将句子分解为不同的成分（或短语），并显示它们之间的层次关系。在构成树中，句子被分解为若干个成分，每个成分可以是单词或者更大的短语。这些成分通过树形结构相互连接，其中树的叶子节点对应于句子中的单词，而内部节点表示短语结构。</p>
<p>例如，考虑以下句子：”The cat sat on the mat.”（猫坐在垫子上。）</p>
<p>构成树的示例可能如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">        Sentence</span><br><span class="line">           |</span><br><span class="line">     ______|______</span><br><span class="line">    |             |</span><br><span class="line">  NP (The)       VP (sat)</span><br><span class="line">    |             |</span><br><span class="line">   / \           / \</span><br><span class="line">Det  N         V   PP</span><br><span class="line"> |   |         |   / \</span><br><span class="line"> the cat      sat  P   NP</span><br><span class="line">               |    |   |</span><br><span class="line">               on   the mat</span><br></pre></td></tr></table></figure>

<p>在这个示例中，构成树以”Sentence”作为根节点，该句子被分解为名词短语（NP）”The cat” 和动词短语（VP）”sat on the mat”。名词短语”NP”由限定词（Det）”The”和名词（N）”cat”组成，动词短语”VP”由动词（V）”sat”和介词短语（PP）”on the mat”组成。</p>
<p>构成树提供了对句子结构的直观理解，有助于理解句子中不同部分之间的语法关系。它在自然语言处理中被广泛应用，例如句法分析、语言生成等任务中。</p>
<p><strong>情感分析（sentiment analysis）</strong></p>
<p>情感分析（Sentiment Analysis），也称为意见挖掘（Opinion Mining），是一种自然语言处理技术，旨在识别和提取文本中的情感和情绪。它通常用于分析文本的态度、情绪、观点和情感倾向，以了解作者对某个主题或实体的态度是正面的、负面的还是中性的。</p>
<p>情感分析可以应用于各种类型的文本数据，包括社交媒体帖子、产品评论、新闻文章、调查问卷等。它可以帮助企业了解客户对其产品和服务的感受，政府了解公众对政策和事件的反应，以及分析师了解市场情绪和趋势。</p>
<p>情感分析的任务通常包括以下几个方面：</p>
<ol>
<li><strong>情感极性分类</strong>：将文本分类为积极、消极或中性情感。例如，判断一篇产品评论是正面的、负面的还是中性的。</li>
<li><strong>情感强度分析</strong>：确定文本中情感的强度程度。例如，判断一篇评论中的情感是强烈的还是弱化的。</li>
<li><strong>主题情感分析</strong>：针对特定主题或实体分析情感。例如，针对某个产品或品牌进行情感分析，了解公众对其的态度。</li>
<li><strong>情感趋势分析</strong>：分析情感随时间的变化趋势。例如，追踪产品评论的情感趋势，了解产品在市场上的表现如何随着时间推移而变化。</li>
</ol>
<p>一个例子：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303163722854.png" alt="image-20240303163722854"></p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络是受人脑结构启发的计算机模型。神经网络通常简称为神经网络。 </p>
<p>人脑由大约 1000 亿个称为<strong>神经元</strong>的神经细胞组成，每个神经元都与其他神经元相连。</p>
<p><em>电信号沿着称为树突的</em>通道流入神经元，如果这些电输入的总和超过特定<em>阈值</em>，神经元就会通过称为<em>轴突</em>的通道发出电输出。  </p>
<p>为了在计算机内部构建<em>神经网络，我们只需将这些神经元的集合放在一起，这样一些神经元的输出就会馈送到其他神经元的输入。</em></p>
<p>典型的神经网络具有一个 <strong>输入层</strong> （您正在使用的数据中的每个特征有一个输入）、一个 <strong>输出层</strong> （每个目标变量有一个输出）以及 中间的一个或多个神经元<strong>隐藏层。</strong></p>
<p>每个神经元通过权重连接到其他神经元，权重决定每个神经元对其连接的其他神经元的影响程度。</p>
<p>图示：</p>
<p><img src="/2024/03/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/image-20240303164126043.png" alt="image-20240303164126043"></p>
<p>在线调参地址：<a target="_blank" rel="noopener" href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.27180&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false">在线调参</a></p>
<p>一些其他的资源：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1mu411x7VD/?spm_id_from=333.337.search-card.all.click">https://www.bilibili.com/video/BV1mu411x7VD/?spm_id_from=333.337.search-card.all.click</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.999.0.0&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.999.0.0&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16x411V7Qg?p=2&vd_source=70cc82c6f851aaa826e5c863112d2113">https://www.bilibili.com/video/BV16x411V7Qg?p=2&amp;vd_source=70cc82c6f851aaa826e5c863112d2113</a></li>
</ul>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><p><strong>Explain the difference between semantics and pragmatics and provide examples.</strong></p>
<table>
<thead>
<tr>
<th>概念</th>
<th>语义（semantics ）</th>
<th>语用学（pragmatics）</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>研究词语、短语和句子的字面意义</td>
<td>研究言语在实际交流中的使用情境和效果</td>
</tr>
<tr>
<td>焦点</td>
<td>关注语言单位与所表示概念之间的关系</td>
<td>关注言语行为的意图、社会和文化背景以及上下文对语言理解的影响</td>
</tr>
<tr>
<td>示例</td>
<td>“Cat”的语义是一种四肢动物，通常作为宠物饲养</td>
<td>“你好吗？”可能在实际交流中只是一种礼貌问候</td>
</tr>
<tr>
<td>稳定性</td>
<td>相对固定，通常与词语的定义和语法结构相关</td>
<td>更加灵活，受到言外之意、语境和交际目的等因素的影响</td>
</tr>
<tr>
<td>内部&#x2F;外部</td>
<td>语言的内部结构</td>
<td>语言的外部应用和效果</td>
</tr>
</tbody></table>
<p><strong>Identify all the various meanings of the following sentence: ‘one morning I shot an elephant in my pajamas.’</strong></p>
<p>这句话的多重含义可以理解为：</p>
<ol>
<li>“one morning”：这个短语指的是某个早晨，暗示了一个特定的时间点或者事件发生的背景。</li>
<li>“I shot an elephant”：这个短语表达了一个动作，即“我开枪射击了一头大象”。这句话的字面意思是“我在穿着睡衣的情况下开枪射击了一头大象”。</li>
<li>“in my pajamas”：这个短语描述了动作发生的环境，即“我穿着睡衣”。这个短语可以产生歧义，它可以解释为“我穿着睡衣时射击了大象”或者“我在大象穿着睡衣的情况下射击了它”。</li>
</ol>
<p>因此，这句话的多重含义可能是指在某个早晨，有人穿着睡衣时开枪射击了一头大象。这个句子之所以有趣，是因为它可以根据语境产生多种不同的理解。</p>
<p><strong>From the list of words, identify sub-words and morphemes.</strong></p>
<p>Sub-words是指一个词中可以被分解成的较小的语言单元，这些单元可能不是完整的词，但在某种程度上具有一定的语义或语法功能。</p>
<p>Morphemes是语言中的最小的具有意义的单位，它是词的构成部分，可以单独存在或者与其他morphemes组合形成词语。</p>
<p>一些例子：</p>
<ul>
<li>Sub-words：”un-“ 在 “undo” 中是一个sub-word，表示否定或相反的含义。”pre-“ 在 “preview” 中是一个sub-word，表示在前面或提前的含义。</li>
<li>Morphemes：”-ed” 在 “walked” 中是一个morpheme，表示过去时。”-s” 在 “cats” 中是一个morpheme，表示复数形式。</li>
</ul>
<p>**From the sentence explain its meaning in terms of semantics and pragmatics. **</p>
<p>这句话的语义是关于事件的描述，主要涉及到以下几个方面：</p>
<ol>
<li>“one morning” 表示一个具体的时间，即某个早晨。</li>
<li>“I shot an elephant” 描述了一个动作，即说话者开枪射击了一头大象。</li>
<li>“in my pajamas” 描述了动作发生的环境，即说话者穿着睡衣。</li>
</ol>
<p>在语义学的角度上，这句话是一个事件的简单描述，包括时间、动作和环境。</p>
<p>从语用学的角度来看，这句话可能有一些歧义，具体取决于上下文和说话者的意图：</p>
<ol>
<li>“in my pajamas” 这个短语可能引起歧义，因为它可以解释为说话者穿着睡衣时射击大象，也可以解释为说话者在大象穿着睡衣的情况下射击它。这种歧义可能引发听者的笑声，因为它打破了通常的语境预期。</li>
<li>语境的缺失可能导致对事件的理解产生不同的解释。例如，为什么说话者穿着睡衣？是偶然的还是有特殊原因？这些问题可能需要从语境中获取信息，而不仅仅依赖于字面意义。</li>
</ol>
<p>因此，语用学强调了言语行为的意图、言外之意和交际背景，这些因素在理解这句话时可能起到关键作用。</p>
<p><strong>From the sentence, identify parts of speech (POS)</strong>. </p>
<p>这句话中的各个词的词性（Parts of Speech, POS）如下：</p>
<ul>
<li>“one” - 数词（numeral）</li>
<li>“morning” - 名词（noun）</li>
<li>“I” - 代词（pronoun）</li>
<li>“shot” - 动词（verb）</li>
<li>“an” - 冠词（article）</li>
<li>“elephant” - 名词（noun）</li>
<li>“in” - 介词（preposition）</li>
<li>“my” - 代词（pronoun）</li>
<li>“pajamas” - 名词（noun）</li>
</ul>
<p>总体而言，这个句子包含了数词、名词、代词、动词和介词等不同的词性。</p>
<p>**From a list of business problems, identify which ones can be solved using Natural Language Processing. **</p>
<ul>
<li><p>客户支持和服务：通过NLP技术，可以建立智能客服系统来处理客户的问题和需求，例如自动回答常见问题、提供建议、处理投诉等。</p>
</li>
<li><p>市场调研和消费者洞察：NLP可以帮助分析社交媒体上的文本数据，了解消费者对产品和服务的看法，发现趋势和关键主题。</p>
</li>
<li><p>舆情分析和品牌管理：通过分析新闻报道、社交媒体内容和客户反馈等文本数据，可以了解公众对公司和产品的看法，及时发现并应对负面舆情。</p>
</li>
<li><p>市场营销和广告优化：NLP技术可以分析消费者的语言和行为模式，帮助企业定制个性化的营销策略和广告内容，提高营销效果。</p>
</li>
<li><p>文档管理和信息提取：利用NLP技术，可以自动处理和归档大量的文档、合同和报告，实现信息的快速检索和提取。</p>
</li>
<li><p>人才招聘和人力资源管理：NLP可以帮助企业自动筛选简历、分析候选人的语言和技能，提高招聘效率和质量。</p>
</li>
<li><p>知识管理和智能助手：企业可以利用NLP技术构建智能助手，帮助员工快速查找信息、解决问题，提高工作效率。</p>
</li>
<li><p>金融和投资分析：NLP可以分析新闻报道、财报和社交媒体上的信息，帮助投资者做出更准确的决策，发现投资机会和风险。</p>
</li>
</ul>
<p><strong>Evaluate whether the Machine Learning task can be supervised, unsupervised, or a semi-supervised learning problem.</strong></p>
<p>评估机器学习任务是监督学习、无监督学习还是半监督学习问题，需要考虑可用数据的性质以及任务的具体学习目标。</p>
<ol>
<li><strong>监督学习</strong>：<ul>
<li>在监督学习中，算法从带有标签的数据中学习，其中输入数据与相应的目标标签配对。</li>
<li>目标是学习从输入特征到目标标签的映射。</li>
<li>示例包括分类和回归任务。</li>
</ul>
</li>
<li><strong>无监督学习</strong>：<ul>
<li>无监督学习涉及从未标记的数据中学习模式或结构，而没有明确的目标标签。</li>
<li>算法发现数据内部的隐藏模式、群组或聚类。</li>
<li>示例包括聚类、降维和异常检测。</li>
</ul>
</li>
<li><strong>半监督学习</strong>：<ul>
<li>半监督学习介于监督学习和无监督学习之间，算法从标记和未标记数据的组合中学习。</li>
<li>通常，与总数据量相比，标记数据的数量较少。</li>
<li>算法利用额外的未标记数据来提高学习性能或泛化能力。</li>
</ul>
</li>
</ol>
<p>要评估机器学习任务属于哪个类别，我们需要检查可用的数据：</p>
<ul>
<li>如果数据包含输入特征以及相应的目标标签，那么这是一个监督学习问题。</li>
<li>如果数据没有目标标签，且目标是发现数据内部的模式或结构，那么这是一个无监督学习问题。</li>
<li>如果数据包含标记和未标记实例的组合，那么这是一个半监督学习问题。</li>
</ul>
<p>根据数据的特征和学习目标，我们可以确定机器学习任务是监督学习、无监督学习还是半监督学习。</p>
<h2 id="主题知识点"><a href="#主题知识点" class="headerlink" title="主题知识点"></a>主题知识点</h2>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"># 自然语言处理</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/02/15/Learning-how-to-learn/" rel="prev" title="Learning how to learn">
                  <i class="fa fa-chevron-left"></i> Learning how to learn
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/26/%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%94%9F%E4%B8%83%E5%B9%B4%E8%AE%A1%E5%88%92/" rel="next" title="第一个人生七年计划">
                  第一个人生七年计划 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Jiu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
